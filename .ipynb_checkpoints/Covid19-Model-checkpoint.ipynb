{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of the dataset CORD-19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the parser and parsing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Parser and specifying what kind of dataset we want to parse\n",
    "parser = Parser([Dataset.BIORXIV])\n",
    "parser.parse(indexByFile = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of accesing the paper by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "Deep Learning-based Detection for COVID-19 from Chest CT using Weak Label\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "Accurate and rapid diagnosis of COVID-19 suspected cases plays a crucial role in timely quarantine and medical treatment. Developing a deep learning-based model for automatic COVID-19 detection on chest CT is helpful to counter the outbreak of SARS-CoV-2. A weakly-supervised deep learning-based software system was developed using 3D CT volumes to detect COVID-19. For each patient, the lung region was segmented using a pre-trained UNet; then the segmented 3D lung region was fed into a 3D deep neural network to predict the probability of COVID-19 infectious. 499 CT volumes collected from Dec. 13, 2019, to Jan. 23, 2020, were used for training and 131 CT volumes collected from Jan 24, 2020, to Feb 6, 2020, were used for testing. The deep learning algorithm obtained 0.959 ROC AUC and 0.976 PR AUC. There was an operating point with 0.907 sensitivity and 0.911 specificity in the ROC curve. When using a probability threshold of 0.5 to classify COVID-positive and COVID-negative, the algorithm obtained an accuracy of 0.901, a positive predictive value of 0.840 and a very high negative predictive value of 0.982. The algorithm took only 1.93 seconds to process a single patient's CT volume using a dedicated GPU. Our weaklysupervised deep learning model can accurately predict the COVID-19 infectious probability in chest CT volumes without the need for annotating the lesions for training. The easily-trained and highperformance deep learning algorithm provides a fast way to identify COVID-19 patients, which is beneficial to control the outbreak of SARS-CoV-2. The developed deep learning software is available at https://github.com/sydney0zq/covid-19-detection.\n",
      "\n",
      "\u001b[1mBody\u001b[0m\n",
      "huge amount of efforts for radiologists, which is not acceptable when COVID-19 is spreading fastly and there are great shortages for radiologists. Thus, performing COVID-19 detection in a weakly-supervised manner is of great importance. One of the simplest labels for COVID-19 detection is the patient-level, i.e., indicating the patient is COVID-19 positive or negative. Therefore, aim of current study was to investigate the potential of a deep learning-based model for automatic COVID-19 detection on chest CT volumes using the weak patient-level label, for the sake of rapid diagnosis of COVID-19 at this critical situation to help to counter this outbreak, especially within Wuhan, Hubei province, China.Patients This retrospective study was approved by Huazhong University of Science and Technology ethics committee, patient consent was waived due to the retrospective nature of this study.Between Dec. 13, 2019 to Feb. 6, 2020, we searched unenhanced chest CT scans of patients with suspected COVID-19 from the picture archiving and communication system (PACS) of radiology department (Union Hospital, Tongji Medical College, Huazhong University of Science and Technology). Finally, 540 patients (mean age, 42.5±16.1 years; range, 3-81 years, male 226, female 314) were enrolled into this study, including 313 patients (mean age, 50.7±14.7 years; range, 8-81 years; male 138, female 175) with clinical diagnosed COVID-19 (COVID-positive group) and 229 patients (mean age, 31.2±10.0 years; range, 3-69 years; male 88, female 141) without COVID-19 (COVID-negative group). There was no significant difference in sex between the two groups (χ 2 =1.744; P=0.187), age in COVIDpositive group significantly higher than that of COVID-negative group (t=17.09; P<0.001). The main clinical symptoms for these patients were fever, cough, fatigue, and diarrhea. Of all the patients, two were included by both groups due to the first and second follow-up CT scans. The first case (female, year 66) was diagnosed as COVID-19 negative on Jan 24, 2020, then changed into COVID-positive on Feb 6, 2020; the second case (female, year 23) was diagnosed as COVID-19 positive on Jan 24, 2020, then changed into COVID-negative on Feb 3, 2020. All the CT volumes scanned on and before Jan 23, 2020, were assigned for deep learning training, and all the CT volumes scanned after Jan 23, 2020, were assigned for deep learning testing.The CT scanning of all the enrolled patients was performed on a gemstone CT scanner (GE Discovery CT750HD; GE Healthcare, Milwaukee, WI), and were positioned in a headfirst supine position, with their bilateral arms raised and placed beside bilateral ears. All the patients underwent CT scans during the end-inspiration without the administration of contrast material. Related parameters for chest CT scanning were listed as follows: field of view (FOV), 36 cm; tube voltage, 100 kV; tube current, 350 mA; noise index, 13; helical mode; section thickness, 5 mm; slice interval, 5 mm; pitch, 1.375; collimation 64×0.625 mm; gantry rotation speed, 0.7 s; matrix, 512×512; the reconstruction slice thickness 1 mm with an interval of 0.8 mm; scan rage from apex to lung base; the mediastinal window: window width of 200 HU with a window level of 35 HU, and the lung window: window width of 1500 HU with a window level of -700 HU.Ground-truth Label In the latest diagnosis and treatment protocols of pneumonia caused by a novel coronavirus (trial version 5) [18] which was released by National Health Commission of the People's Republic of China on Feb 4, 2020, suspected cases with characteristic radiological manifestations of 3 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint .COVID-19 has been regarded as current standard for clinical diagnostic cases in severely affected areas only in Hubei Province, indicating that chest CT is fundamental for COVID-19 identification of clinically diagnosed cases.Typical CT findings for COVID-19 are also listed: multifocal small patchy shadowing and interstitial abnormalities in the early stage, especially for the peripheral area of the bilateral lungs. In the progressive period, the lesions could increase in range and in number; it could develop into multiple ground glass opacity (GGO) with further infiltration into the bilateral lungs. In severe cases, pulmonary diffuse consolidation may occur and pleural effusion is rarely shown.The combination of epidemiologic features (travel or contact history), clinical signs and symptoms, chest CT, laboratory findings and real-time RT-PCR (if available) for SARS-CoV-2 nucleic acid testing is used for the final identification of COVID-19. The medical CT reports were acquired via the electronic medical record of Union Hospital, Tongji Medical College, Huazhong University of Science and Technology. According to the CT reports, if a CT scan was COVID-positive, its ground-truth label was 1; otherwise, the label was 0. Figure 1 : Architecture of the proposed DeCoVNet. The network took a CT volume with its 3D lung mask as input and directly output the probabilities of COVID-positive and COVID-negative.The Proposed DeCoVNet We proposed a 3D deep convolutional neural Network to Detect COVID-19 (DeCoVNet) from CT volumes. As shown in Fig. 1 , DeCoVNet took a CT volume and its 3D lung mask as input. The 3D lung mask was generated by a pre-trained UNet [19] . DeCoVNet was divided into three stages for a clear illustration in Table. 1. The first stage was the network stem, which consisted of a vanilla 3D convolution with a kernel size of 5 × 7 × 7, a batchnorm layer and a pooling layer. The second stage was composed of two 3D residual blocks (ResBlocks). In each ResBlock, a 3D feature map was passed into both a 3D convolution with a batchnorm layer and a shortcut connection containing a 3D convolution that was omitted in Fig. 1 for dimension alignment. The resulted feature maps were added in an element-wise manner. The third stage was a progressive classifier (ProClf), which mainly contained three 3D convolution layers and a fully-connected (FC) layer with the softmax activation function. ProClf progressively abstracts the information in the CT volumes by 3D max-pooling and finally directly output the probabilities of being COVID-positive and COVID-negative. The 3D lung mask of an input chest CT volume helped to reduce background information and better detect COVID-19. Detecting the 3D lung mask was a well-studied issue. In this study, we trained a simple 2D UNet using the CT images in our training set. To obtain the ground-truth lung masks, we 4 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . Table 1 : Detailed structure of the proposed DeCovNet. The number after the symbol \"@\", e.g., 5×7×7, denotes the kernel size of the convolution layer or the residual block. \"&\" means that there are two types of kernel size in the residual block. \"T\" denotes the length of the input CT volume. The number in \"Output size\" is in the order of \"channel, length, height, width\". The input size is 2 × T × 192 × 288. segmented the lung regions using an unsupervised learning method [20] , removed the failure cases manually, and the rest segmentation results were taken as ground-truth masks. The 3D lung mask of each CT volume was obtained by testing the trained 2D UNet frame-by-frame without using any temporal information. The overall training and testing procedures of UNet and DeCoVNet for COVID-19 detection were illustrated in Fig. 2 .Preprocessing of 2D UNet All the CT volumes were preprocessed in a unified manner before training the 2D UNet for lung segmentation. First, the unit of measurement was converted to the Hounsfield Unit (HU) and the value was linearly normalized from 16-bit to 8-bit (i.e., 0-255) after determining the threshold of a HU window (e.g., -1 200-600 HU). After that, all the CT volumes were resampled into a same spatial resolution (e.g., 368×368), by which the CT volumes could be aligned without the influence of the cylindrical scanning bounds of CT scanners. This step was applied to the obtained ground-truth lung masks as well.Preprocessing of DeCoVNet For each CT volume, the lung masks produced by the trained UNet formed a mask volume, then the CT volume was concatenated with the mask volume to obtain a CT-Mask volume. Finally, the CT-Mask volume was resampled into a fixed spatial resolution (e.g., 224×336) without changing the number of slices for DeCoVNet training and testing. The number of slices in the whole dataset was 141±16 ranging from 73 to 250.Data Augmentation To avoid the overfitting problem since the number of training CT volumes was limited, online data augmentation strategies were applied including random affine transformation and color jittering. The affine transformation was composed of rotation (0 • ±10 • ), horizontal and vertical 5 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint translations (0%±10%), scaling (0%±20%) and shearing in the width dimension (0 • ±10 • ). The color jittering adjusted brightness (0%±50%) and contrast (0%±30%). For each training sample, the parameters were randomly generated and the augmentation was identically applied for each slice in the sampled CT volume.Training and Testing Procedures The DeCoVNet software was developed based on the PyTorch framework [21] . Our proposed DeCoVNet was trained in an end-to-end manner, which meant that the CT volumes were provided as input and only the final output was supervised without any manual intervention. The network was trained for 100 epochs using Adam optimizer [22] with a constant learning rate of 1e-5. Because the length of CT volume of each patient was not fixed, the batch size was set to 1. The binary cross-entropy loss function was used to calculate the loss between predictions and ground-truth labels.During the procedure of testing, data augmentation strategies were not applied. The trained DeCoV-Net took the preprocessed CT-Mask volume of each patient and output the COVID-positive probability as well as COVID-negative probability. Then the predicted probabilities of all patients and their corresponding ground-truth labels were collected for statistical analysis.The cohort for studying the COVID-19 detection contained 630 CT scans collected from Dec 13, 2019 to Feb 6, 2020. To simulate the process of applying the proposed DeCoVNet for clinical computeraided diagnosis (i.e., prospective clinical trials), we used the 499 CT scans collected from Dec 13, 2019 to Jan 23, 2020 for training and used the rest 131 CT volumes collected from Jan 24, 2020 to Feb. 06, 6 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.12.20027185 doi: medRxiv preprint 2020 for testing. Of the training volumes, 15% were randomly selected for hyperparameter tuning during the training stage.Statistical Analysis COVID-19 detection results were reported and analyzed using receiver operating characteristic (ROC) and precision-recall (PR) curves. The area under the ROC curve (ROC AUC) and the area under the precision-recall curve (PR AUC) were calculated. Besides, multiple operating points were chosen on the ROC curve, e.g., the points with approximately 0.95 sensitivity (high sensitivity point) and with approximately 0.95 specificity (high specificity point). ROC AUC, PR AUC, and some key operating points were used to assess the deep learning algorithm.This was a retrospective case series study and no patients were involved in the study design, setting the research questions, or the outcome measures directly. No patients were asked to advise on interpretation or writing up of results. The software for COVID-19 detection with the pre-trained model as well as the results was available at https://github.com/sydney0zq/covid-19-detection, which will be made publicly available on the publication of this paper. Training DeCoVNet on the training set which consisted of 499 CT volumes took about 20 hours (11 hours for UNet and 9 hours for DeCoVNet) and testing a CT volume costed an average of 1.93 seconds (1.80 seconds for UNet and 0.13 seconds for DeCoVNet) on an NVIDIA Titan Xp GPU.. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. For every testing CT scan, we used the trained DeCoVNet to predict its probability of COVID-19. By comparing with their binary ground-truth labels, we plotted ROC and PR curves as shown in Fig. 3 and Fig. 4 respectively. In the ROC, we obtained a ROC AUC value of 0.959. When true positive rate (TPR, i.e., sensitivity) was approximately 0.95, our model obtained a true negative rate (TNR, i.e., specificity) of 0.786; when TNR was approximately 0.95, our model obtained a TPR of 0.880; there was another operating showed that our algorithm obtained both TPR and FPR larger than 0.9, i.e., sensitivity=0.907 and specificity=0.911. On the PR curve, our model obtained a PR AUC of 0.975.When using the threshold of 0.5 to make COVID-19 detection prediction (i.e., if the probability of COVID-19 was larger than 0.5, the patient was classified as COVID-positive, and vice versa), the algorithm obtained an accuracy of 0.901 with a positive predictive value (PPV) of 0.840 and a negative predictive value (NPV) of 0.982. By varying the probability threshold, we obtained a series of COVID-19 detection accuracy, PPV and NPV in Table 2 . Our data showed that the COVID-19 prediction accuracy obtained by the DeCoVNet algorithm was higher than 0.9 when the threshold ranged from 0.2 to 0.5. At the threshold setting of 0.5, there were 12 false positive predictions in total and only one false positive prediction by the algorithm in our study, indicating that the algorithm to have a very high negative predictive value.The accurate predictions (a true positive and a true negative) were presented in Fig. 5 (A-B) , and erroneous predictions in Fig. 5 (C-F) . In images corresponding to the true positive and the false negative, the lesions of COVID-19 were annotated by red arrows. As shown in Fig. 5 (C, D, E) , the false negative predictions were made by the algorithm, and Fig. 5 (F) showed the only false positive prediction, in which the respiratory artifact had been mistaken as a COVID-19 lesion by the DeCoVNet algorithm.. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . Figure 5 : Some accurate and erroneous predictions of the proposed DeCoVNet.. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint .To our knowledge, this is the first study to perform weakly-supervised computer-aided COVID-19 detection with a large number of CT volumes from the frontline hospital at the present epidemic period. By designing an effective weakly-supervised deep learning-based algorithm and training it on CT volumes collected before Jan 23, 2020 with only patient-level labels, the testing results on 131 CT scans collected from Jan 24, 2020, to Feb 6, 2020, were very impressive, e.g., the PR AUC value was 0.975. On the ROC curve, the algorithm obtained sensitivity and specificity values larger than 0.9, which were both clinically applicable.The motivation of this study was to utilize AI to alleviate the problem of shortage of professional interpretations for CT images when the epidemic is still fast spreading. Though there were many effective applications of medical AI in previous studies [13, 23] , developing AI for automatic COVID-19 detection was still a challenging task. Firstly, in the current emergency situation, the number of enrolled patients is relatively smaller compared with that used in previous studies [13, 23] ; and patients enrolled in our study were clinically diagnosed cases with COVID-19, because the majority of them did not undergo the nucleic acid testing due to the sudden outbreak and limited medical resource in such a short time period. Secondly, the lesions of COVID-19 in CT volumes were not labeled by radiologists and only patientlevel labels (i.e., COVID-positive or COVID-negative) were utilized for training the AI algorithm in our study. Thirdly, some small infected areas of COVID-19 have the potential to be missed even by professional radiologists, and whether it is feasible to be detected by deep learning-based 3D DCNN model remains unclear. We hypothesized to solve these problems by proposing a delicate 3D DCNN, i.e., DeCoVNet. It solved the first problem by applying extensive data augmentation on training CT volumes to obtain more training examples. The second problem was solved by regarding the COVID-19 detection problem as a weakly-supervised learning problem [24] , i.e., detecting COVID-19 without annotating the regions of COVID-19 lesions. In the designed DeCoVNet, we used the spatially global pooling layer and the temporally global pooling layer to technically handle the weakly-supervised COVID-19 detection problem. The third problem was addressed by taking the advantages of deep learning and utilizing a pre-trained UNet for providing the lung masks to guide the learning of DeCoVNet.The deep learning-based COVID-19 diagnostic algorithm used in our study is effective compared to recent deep learning-based computer-aided diagnosis methods. On the task of predicting the risk of lung cancer [13] , the deep learning model was trained on 42290 CT cases from 14851 patients and obtained 0.944 ROC AUC. On the task of critical findings from head CT [23] , the deep learning model was trained on 310055 head CT scans and obtained ROC AUC of 0.920. In our study, only 499 scans were used for training, but the obtained ROC AUC was 0.959. By comparing the data between them, it was able to find that the task of COVID-19 detection may be easier and the proposed deep learning algorithm was very powerful. As for the erroneous 12 false negative predictions in our results, the most possible explanations after we rechecked the original CT images were listed as follows: those lesions were slightly increased in CT densities, and images of those ground-glass opacities were very faint without consolidation.Our study provided a typical and successful solution for developing medical AI for emerging diseases, such as COVID-19. While we were developing this AI, doctors in Wuhan were still extremely busy with treating a huge number of COVID-19 patients and it may be impossible for them to annotate the lesions in CT volumes in the current austere fight against this epidemic. Thanks to the weaklysupervised algorithm in this study, locations of pulmonary lesions in CT volumes are not necessary to be annotated, and radiologists' annotating efforts can be minimized, i.e., only providing patient-level labels. Therefore, developing a helpful AI tool swiftly has become possible and available in the clinical application. In the future, the burden of AI experts could be lifted significantly by automatic machine learning (AutoML) [25] .Limitations of this study There are still several limitations in this study. First, network design and training may be further improved. For example, the UNet model for lung segmentation did not utilize temporal information and it was trained using imperfect ground-truth masks, which could be improved by using 3D segmentation networks and adopting precise ground-truth annotated by experts. Second, the data used in this study came from a single hospital and cross-center validations were not performed. Third, when diagnosing COVID-19, the algorithm worked in a black-box manner, since the algorithm was based on deep learning and its explainability was still at an early stage. Related work of all limitations mentioned above will be addressed in our further studies.In conclusion, without the need for annotating the COVID-19 lesions in CT volumes for training, our weakly-supervised deep learning algorithm obtained strong COVID-19 detection performance. Therefore, our algorithm has great potential to be applied in clinical application for accurate and rapid COVID-19 diagnosis, which is of great help for the frontline medical staff and is also vital to control this epidemic worldwide.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can access the date by index or by file name, but we have to change in the parse function\n",
    "#what kind of invoke we want\n",
    "print(parser.data_dicts[Dataset.BIORXIV][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesing certain elements of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "p53 is not necessary for DUX4 pathology\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Potentially highly potent drugs for 2019-nCoV\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Deep Learning-based Detection for COVID-19 from Chest CT using Weak Label\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "The Viral Protein Corona Directs Viral Pathogenesis and Amyloid Aggregation\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Significance of hydrophobic and charged sequence similarities in sodium-bile acid cotransporter and vitamin D-binding protein macrophage activating factor\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Dark Proteome of Newly Emerged SARS-CoV-2 in Comparison with Human and Bat Coronaviruses\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Title: Genome Detective Coronavirus Typing Tool for rapid identification and characterization of novel coronavirus genomes Short title: Automated tool for phylogenetic and mutational analysis of coronaviruses genomes\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Effect of SARS-CoV-2 infection upon male gonadal function: A single center- based study\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "SARS-CoV-2 infection does not significantly cause acute renal injury: an analysis of 116 hospitalized patients with COVID-19 in a single hospital, Wuhan, China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "2\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Quantifying the roles of vomiting, diarrhea, and residents vs. staff in\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Full Title: Comparisons of a Novel Air Sampling Filter Material, Wash Buffers and 2 Extraction Methods in the Detection and Quantification of Influenza Virus 3 Short title: Comparisons of Air Sampling Filter Materials for the Detection Quantification 4 of Influenza Virus CPUB-Occup & Environ Hea\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A simple model to assess Wuhan lock-down effect and region efforts during COVID-19 epidemic in China Mainland\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Estimating Spot Prevalence of COVID-19 from Daily Death Data in Italy\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Heat inactivation of serum interferes with the immunoanalysis of antibodies to SARS-CoV-2\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "68 Consecutive patients assessed for COVID-19 infection; experience from a UK regional infectious disease unit\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Animal virus ecology and evolution are shaped by the virus host-body\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A comprehensive annotation and differential expression analysis of short and long non-coding RNAs in 16 bat genomes\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Precautions are Needed for COVID-19 Patients with Coinfection of Common Respiratory Pathogens\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Artesunate interacts with Vitamin D receptor to reverse mouse model of sepsis-induced immunosuppression via enhancing autophagy Short running title: Artesunate reverses sepsis induced immunosuppression\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Characterizing the transmission and identifying the control strategy for COVID-19 through epidemiological modeling\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Epidemiology of seasonal coronaviruses: Establishing the context for COVID-19 emergence\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Ultra-Low-Cost Integrated Silicon-based Transducer for On-Site, Genetic Detection of Pathogens\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "50-valent inactivated rhinovirus vaccine is broadly immunogenic in rhesus macaques\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Multiplex logic processing isothermal diagnostic assays for an evolving virus\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Therapeutic effects of dipyridamole on COVID-19 patients with 1 coagulation dysfunction\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Study of the mental health status of medical personnel dealing with new coronavirus pneumonia\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Effect of non-pharmaceutical interventions for containing the COVID-19 outbreak in China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Differential Antibody Recognition by Novel SARS-CoV-2 and SARS-CoV Spike Protein Receptor Binding Domains: Mechanistic Insights and Implications for the Design of Diagnostics and\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "SARS-CoV-2 specific antibody responses in COVID-19 patients\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Enabling large-scale genome editing by reducing DNA nicking\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A B\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Reconciling early-outbreak estimates of the basic reproductive number and its uncertainty: framework and applications to the novel coronavirus (SARS-CoV-2) outbreak\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Title: Feasibility of controlling 2019-nCoV outbreaks by isolation of cases and contacts\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A data-driven assessment of early travel restrictions related to the spreading of the novel COVID-19 within mainland China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Structure-Guided Mutagenesis Alters Deubiquitinating Activity 2 and Attenuates Pathogenesis of a Murine Coronavirus\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Impact of the contact and exclusion rates on the spread of COVID-19 pandemic\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "The reproductive number R 0 of COVID-19 based on estimate of a statistical time delay dynamical system\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Combination attenuation offers strategy for live-attenuated coronavirus vaccines 1 2\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Real-time, MinION-based, amplicon sequencing for lineage typing of infectious bronchitis virus from upper respiratory samples\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Estimation of local novel coronavirus (COVID-19) cases in Wuhan, China from off-site reported cases and population flow data from different sources\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "How domain growth is implemented determines the long term behaviour of a cell population through its effect on spatial correlations\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Substantial undocumented infection facilitates the rapid dissemination of novel coronavirus (COVID-19)\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "More than efficacy revealed by single-cell analysis of antiviral therapeutics\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A transmissible RNA pathway in honey bees\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Mouse hepatitis virus nsp14 exoribonuclease activity is required for\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Highly ACE2 Expression in Pancreas May Cause Pancreas Damage After SARS-CoV-2 Infection\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Title: Viruses are a dominant driver of protein adaptation in mammals\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Transmission and clinical characteristics of coronavirus disease 2019 in 104 outside-Wuhan patients, China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "DEN-IM: Dengue Virus identification from shotgun and targeted metagenomics\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "National and Regional Influenza-Like-Illness Forecasts for the USA\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Robitaille et al. 1 DUSP1 regulates apoptosis and cell migration, but not the JIP1-protected cytokine response, during Respiratory Syncytial Virus and Sendai Virus infection\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A comparative multicenter study on the clinical and imaging features of confirmed and unconfirmed patients with COVID19 Authors' names\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Population movement, city closure and spatial transmission of the 2019-nCoV 1 infection in China 2\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Using heterogeneity in the population structure of U.S. swine farms to compare transmission models for porcine epidemic diarrhoea\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Clinical characteristics of 25 death cases with COVID-19: a retrospective review of medical records in a single medical center\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Mutations, Recombination and Insertion in the Evolution of 2019-nCoV\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Sex differences in clinical findings among patients with coronavirus disease 2019 (COVID-19) and severe condition\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "MATHEMATICAL PREDICTIONS FOR COVID-19 AS A GLOBAL PANDEMIC\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "SKEMPI 2.0: An updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Identification of a Nidovirales Orf1a N7-guanine cap Methyltransferase signature- sequence as a genetic marker of large genome Tobaniviridae Running title: RNA cap N7-guanine Methyltransferase in Tobaniviridae Orf1a\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Article: Discoveries On the causes of evolutionary transition:transversion bias\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "The molecular basis for Pompe disease 3 revealed by the structure of human acid α-glucosidase 4 5\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Myocardial injury is associated with in-hospital mortality of confirmed or suspected COVID-19 in Wuhan, China: A single center retrospective cohort study\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Epidemiology and Transmission of COVID-19 in Shenzhen China: Analysis of 391 cases and 1,286 of their close contacts\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Transferrin binding protein B and Transferrin binding protein A 2 expand the transferrin 1 recognition range of Histophilus somni 2 3 Running title: TbpB and TbpA2 broaden specificity of\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Critical Nodes of Virus-Host Interaction Revealed Through an Integrated Network Analysis\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Speed and strength of an epidemic intervention\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "The effects of modified sialic acids on mucus and erythrocytes on influenza A virus HA and NA functions\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A high efficient hospital emergency responsive mode is the key of successful treatment of 100 COVID-19 patients in Zhuhai\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "D R A F T Inferring transmission trees to guide targeting of interventions against visceral leishmaniasis and post-kala-azar dermal leishmaniasis\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "A hidden gene in astroviruses encodes a cell-permeabilizing protein involved in virus release\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Facemask shortage and the coronavirus disease (COVID-19) outbreak: Reflection on public health measures\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Multivariate Analyses of Codon Usage of SARS-CoV-2 and other 1 betacoronaviruses\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Metagenomic Nanopore sequencing of influenza virus 1 direct from clinical respiratory samples 2 3\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Age profile of susceptibility, mixing, and social distancing shape the dynamics of the novel coronavirus disease 2019 outbreak in China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Rapid metagenomic characterization of a case of imported COVID-19 in Cambodia\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Potential Neutralizing Antibodies Discovered for Novel Corona Virus Using Machine Learning\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Outbound traffic from Wuhan and COVID-19 incidence Temporal relationship between outbound traffic from Wuhan and the 2019 coronavirus disease (COVID-19) incidence in China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Adaptive Estimation for Epidemic Renewal and Phylogenetic Skyline Models\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "TWIRLS, an automated topic-wise inference method based on massive literature, suggests a possible mechanism via ACE2 for the pathological changes in the human host after coronavirus infection\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Epidemiological parameters of coronavirus disease 2019: a pooled analysis of publicly reported individual data of 1155 cases from seven countries Summary Background\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Demographic science aids in understanding the spread and fatality rates of COVID-19\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "In trans variant calling reveals enrichment for compound heterozygous variants in genes involved in neuronal development and growth\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Prevalence and clinical features of 2019 novel coronavirus disease (COVID-19) in the Fever Clinic of a teaching hospital in Beijing: a single-center, retrospective study\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Caution: The clinical characteristics of COVID-19 patients at admission are changing\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Rapid Molecular Detection of SARS-CoV-2 (COVID-19) Virus RNA Using Colorimetric LAMP\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Characteristics of patients with COVID-19 during epidemic ongoing outbreak in Wuhan, China\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Emergence of evidence during disease outbreaks: lessons learnt from the Zika virus outbreak\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Modification of primary amines to higher order amines reduces in vivo hematological and immunotoxicity of cationic nanocarriers through TLR4 and complement pathways\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Evaluation of the potential incidence of COVID-19 and effectiveness of contention measures in Spain: a data-driven approach\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Structural and functional conservation of the programmed -1 ribosomal frameshift signal of SARS-CoV-2\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Structure of the chromatin remodelling enzyme Chd1 bound to a ubiquitinylated nucleosome\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "High incidence of asymptomatic SARS-CoV-2 infection, Chongqing, China\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "Strategies for vaccine design for corona virus using Immunoinformatics techniques\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#By method titles(), abstracts() and bodies() you can access to certain elements of the paper\n",
    "paper_abstracts = parser.titles()\n",
    "for abstract in paper_abstracts[Dataset.BIORXIV].values():\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User manual\n",
    "#----------------------------------\n",
    "#Install --> pip3 install gensim (apart from gensim, you will need numpy)\n",
    "#Download word2vec file -->  https://code.google.com/archive/p/word2vec/\n",
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we initialize word2vec with already pretrained vectors\n",
    "word2vec = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corona_virus', 0.7276226282119751),\n",
       " ('coronaviruses', 0.7216538190841675),\n",
       " ('paramyxovirus', 0.7113003730773926),\n",
       " ('SARS_coronavirus', 0.6601907014846802),\n",
       " ('arenavirus', 0.6494410037994385),\n",
       " ('influenza_virus', 0.6449826955795288),\n",
       " ('H#N#_subtype', 0.6360139846801758),\n",
       " ('H#N#_strain', 0.6324741840362549),\n",
       " ('H7_virus', 0.6261191964149475),\n",
       " ('flu_virus', 0.6249204874038696)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As you can see, coronavirus is extremely similar with other virus terms \n",
    "word2vec.most_similar(\"coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11914062,  0.0189209 , -0.02648926,  0.22460938, -0.17089844,\n",
       "        0.4609375 ,  0.38867188, -0.19921875,  0.15429688, -0.00180054,\n",
       "       -0.10693359, -0.26757812, -0.17089844,  0.17382812, -0.06982422,\n",
       "        0.38085938,  0.10253906,  0.1171875 ,  0.14453125,  0.01409912,\n",
       "        0.32226562,  0.45703125,  0.25976562,  0.06738281, -0.28515625,\n",
       "        0.21289062, -0.20996094,  0.04418945, -0.14746094,  0.04296875,\n",
       "       -0.22167969, -0.24707031, -0.24121094, -0.13574219, -0.15234375,\n",
       "        0.02502441, -0.08203125, -0.328125  ,  0.44921875, -0.12988281,\n",
       "        0.24414062,  0.01489258, -0.33203125, -0.14453125, -0.24023438,\n",
       "       -0.11035156, -0.0300293 ,  0.06152344, -0.15917969, -0.12890625,\n",
       "        0.02832031,  0.40039062, -0.046875  , -0.3203125 ,  0.09765625,\n",
       "       -0.0859375 , -0.1171875 , -0.32421875, -0.0390625 , -0.09814453,\n",
       "        0.41210938,  0.09765625,  0.19042969,  0.0859375 , -0.03710938,\n",
       "        0.05688477,  0.05883789,  0.06640625,  0.0703125 ,  0.20214844,\n",
       "        0.08007812,  0.05932617,  0.09228516,  0.00405884, -0.01660156,\n",
       "       -0.09033203, -0.06445312, -0.13574219, -0.07373047,  0.17480469,\n",
       "       -0.06933594, -0.16015625, -0.00622559, -0.13183594,  0.08203125,\n",
       "        0.24023438, -0.03637695,  0.23242188,  0.11230469, -0.140625  ,\n",
       "        0.19335938, -0.11962891,  0.25      , -0.19824219,  0.04858398,\n",
       "       -0.08886719,  0.3046875 , -0.05932617,  0.10009766,  0.19433594,\n",
       "       -0.125     ,  0.07275391,  0.2421875 , -0.16308594,  0.02709961,\n",
       "       -0.18652344, -0.1484375 , -0.28320312,  0.40429688, -0.21679688,\n",
       "        0.00173187, -0.07324219, -0.01794434,  0.25976562,  0.19824219,\n",
       "       -0.02722168, -0.15917969, -0.24609375, -0.23730469,  0.16601562,\n",
       "       -0.12158203,  0.13085938, -0.19628906, -0.5078125 , -0.01190186,\n",
       "       -0.27148438,  0.2734375 ,  0.22265625,  0.13476562,  0.08349609,\n",
       "        0.12792969, -0.26757812,  0.0378418 , -0.04418945, -0.15527344,\n",
       "       -0.15136719,  0.09179688,  0.06103516,  0.49023438,  0.4140625 ,\n",
       "        0.06982422, -0.28125   ,  0.05126953,  0.20507812,  0.06933594,\n",
       "       -0.234375  , -0.43945312, -0.27734375, -0.1484375 , -0.15625   ,\n",
       "        0.06884766, -0.31835938, -0.24902344,  0.12451172,  0.11767578,\n",
       "        0.15722656, -0.17871094, -0.18652344, -0.33203125, -0.18066406,\n",
       "        0.41210938, -0.01165771,  0.00708008, -0.07226562,  0.02685547,\n",
       "       -0.47265625, -0.26171875, -0.04125977, -0.06396484,  0.05200195,\n",
       "        0.26171875,  0.21972656, -0.28320312, -0.15722656,  0.16699219,\n",
       "       -0.12451172,  0.0378418 , -0.19335938,  0.21191406, -0.00540161,\n",
       "       -0.01153564, -0.296875  ,  0.32617188,  0.00415039, -0.04931641,\n",
       "       -0.08984375,  0.14453125, -0.08984375,  0.1953125 ,  0.05712891,\n",
       "       -0.20019531,  0.23925781, -0.05639648,  0.09472656, -0.35546875,\n",
       "        0.43164062,  0.02929688, -0.1328125 , -0.16503906, -0.23925781,\n",
       "       -0.16113281,  0.10742188, -0.08935547,  0.22753906,  0.08642578,\n",
       "        0.08007812, -0.27539062,  0.09423828,  0.10644531, -0.140625  ,\n",
       "       -0.31054688, -0.03564453, -0.24414062, -0.12255859,  0.06079102,\n",
       "       -0.10546875,  0.21679688, -0.2578125 , -0.06445312,  0.07519531,\n",
       "       -0.21289062,  0.07666016,  0.39453125,  0.203125  ,  0.0480957 ,\n",
       "        0.3828125 , -0.08105469, -0.34960938, -0.265625  ,  0.328125  ,\n",
       "        0.12597656,  0.18066406, -0.00714111,  0.06640625, -0.08837891,\n",
       "        0.15234375, -0.27148438, -0.04077148,  0.27734375,  0.06445312,\n",
       "        0.24023438, -0.140625  , -0.07763672,  0.04541016,  0.16894531,\n",
       "        0.23925781,  0.01324463,  0.03149414,  0.01721191, -0.171875  ,\n",
       "        0.13085938, -0.06591797,  0.265625  ,  0.03833008,  0.07470703,\n",
       "       -0.09667969, -0.13085938, -0.05273438,  0.04150391, -0.15234375,\n",
       "        0.04077148, -0.02697754, -0.06835938, -0.10644531,  0.06445312,\n",
       "       -0.12109375,  0.15039062, -0.20019531, -0.14746094, -0.18945312,\n",
       "        0.1015625 ,  0.10058594,  0.2421875 , -0.2109375 ,  0.11035156,\n",
       "        0.11865234, -0.34570312,  0.02258301,  0.03125   , -0.21386719,\n",
       "        0.17382812, -0.07470703,  0.12890625,  0.01538086,  0.00793457,\n",
       "       -0.08398438,  0.04174805, -0.04956055,  0.1484375 , -0.06176758,\n",
       "        0.08398438, -0.0859375 , -0.26171875,  0.30273438,  0.12304688,\n",
       "        0.30859375, -0.27929688,  0.17285156,  0.19433594,  0.11132812],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So word2vec is basically a dict, where for word it returns us a 300 dimensional vector. The more the words are similiar\n",
    "#so are the vectors going to be similar (talking here about cosine similarity!).\n",
    "word2vec[\"cure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now we are heading into the big guns! Doc2Vec is basically word2vec, but only for words that are appearing in our dataset. Meaning that words like Coronavirus, Covid19, Wuhan and other important phrases will be recognized here by our model. In contrast, word2vec couldn't recognize covid19, because that's new term for this disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are basically making our own dataset. We are taking our own papers ( parser.toList() will return all papers in dataset) and tagging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(parser.toList())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the training part. Here we are making our own word embeddings. That means we are basically going to make our own word2vec. In other words, for every word from our dataset our model will make a vector in 20 dimensional space. Furthermore, every vectors will be similar if the words they are representing are similar. E.g. vectors for word coronavirus and covid19 will be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 120\n",
    "vec_size = 20 #word2vec has 300, but I left 100 here\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are testing our word embeddings with some query. Our query will be \"Coronavirus transmission\" and we are hoping to  find all the documents that are talking about coronavirus transmission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = word_tokenize(\"Coronavirus transmission\".lower()) #change this query to test different things \n",
    "v1 = model.infer_vector(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are finding the embeddings that will correnspond with our query. Function most_similar() will return us id and percentage of similarity with corrensponding query. E.g. ('43', 0.834 ) means that document with id 43 is 83% similar with query. ( although this isn't really percentage, this is similarity, but thats the gist :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('57', 0.5976769924163818), ('82', 0.5365582704544067), ('13', 0.5226129293441772), ('44', 0.476043164730072), ('60', 0.46594515442848206), ('58', 0.4617353081703186), ('46', 0.44316622614860535), ('76', 0.41599059104919434), ('37', 0.4150567650794983), ('80', 0.4132328927516937)]\n"
     ]
    }
   ],
   "source": [
    "#finding the most similar doc\n",
    "similar_doc = model.docvecs.most_similar([v1])\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the most similar document with our query within our dataset of 100 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population movement, city closure and spatial transmission of the 2019-nCoV 1 infection in China 2The outbreak of pneumonia caused by a novel coronavirus (2019-nCoV) in Wuhan 17City of China obtained global concern, the population outflow from Wuhan has 18 contributed to spatial expansion in other parts of China. We examined the effects of 19 population outflow from Wuhan on the 2019-nCoV transmission in other provinces 20 and cities of China, as well as the impacts of the city closure in Wuhan. We observed 21 a significantly positive association between population movement and the number of 22 cases. Further analysis revealed that if the city closure policy was implemented two 23 days earlier, 1420 (95% CI: 1059, 1833) cases could be prevented, and if two days 24 later, 1462 (95% CI: 1090, 1886) more cases would be possible. Our findings suggest 25 that population movement might be one important trigger of the 2019-nCoV infection 26 transmission in China, and the policy of city closure is effective to prevent the 27 epidemic. 28In December 2019, a series of pneumonia cases caused by a novel coronavirus, 33 namely 2019-nCoV, emerged in Wuhan, the capital city of Hubei Province in China 34(1), Similar with severe acute respiratory syndrome (SARS), the Wuhan pneumonia 35 outbreak was highly suspected to be linked to the wild animals in the seafood market, 36 although the definitive source of this virus was not clear yet (2). 37 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.04.20020339 doi: medRxiv preprint As of Jan 31, 2020, the infection has been transmitted to all the provinces in China 38 and a few other countries. Epidemiology evidence shown that most of the cases had a 39 history of living or travelling to Wuhan, the household cluster cases and cases of 40 health-care workers indicated the human-to-human transmission route (3), which 41 might be the reason for a rapid increasing rate of infection across the country and 42 globally (4). 43Considering the person-to-person transmission and the large travel volume during 44 the traditional Chinese New Year (the largest annual population movement in the 45 world), it is expected that the population movement would lead to further expansion 46 of the infection, so the government imposed a lockdown on Wuhan City at 10:00 am 47 on January 23, as well as some other cities later on. However, an estimated 5 million 48 individuals had already left Wuhan for the holiday or travelling, some of which rushed 49 out after the lockdown announcement (5). In addition, the novel coronavirus is 50 infectious during the incubation period and when the symptoms are not obvious, 51 which is likely to make the huge floating population potential sources of infection (6). 52 Therefore, it is reasonable to hypothesize that the population transported from Wuhan 53 may have a significant impact on the potential outbreaks in other parts of China. 54 Recent studies on the novel coronavirus pneumonia focused more on its etiology (7, 55 8), transmission route (9, 10), and epidemiological characteristics (11, 12), there is 56 still a lack of investigating the relationship between the migrating population and the 57 outbreak, which is of great importance for making intervention policies. Thus, we 58 conducted this study with the following objectives: 1) to depict the impacts of the 59 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint As the city closure took place at 10:00 AM Jan 23 rd , 2020, and the incubation period 76 of the infection was estimated to be about 3-7 days (15), we obtained the daily index 77 of population outflow from Wuhan and the proportion of the daily index from Wuhan 78 to other provinces and top 50 cities, from January 1 to 31 in 2020, the information 79 was retrieved through the Spring Festival travel information of China released by 80 Baidu Qianxi. The data is consisted of Baidu Location Based Services (LBS) and 81 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.04.20020339 doi: medRxiv preprint which be display dynamic visual regional population outflow in real-time. Data of 83Baidu Qianxi was freely available to the public (http://qianxi.baidu.com). author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.04.20020339 doi: medRxiv preprintAfter the city closure was taken in force, some population still outflowed from Wuhan. 105We subtracted the outflow index on Jan 23 from the average outflow index from Jan 106 24 to 31, to obtain the index of outflow population within Wuhan reduced by the 107 advance city closure on Jan 23 and 22 (the advance outflow index). And then we 108 calculated the average proportion of the outflow index from Jan 24 to 31 for each 109 province (the average proportion). The number of cases reduced by the advance city 110 closure in each province was estimated by multiplied the advance outflow index by 111 the average proportion and one corresponding unit. The formula can be specified as: 112The reduced index of outflow population: The net loss index of outflow population caused by advance Wuhan city closure for 124 each province: 125The total reduced number of 2019-nCoV cases: 127Similarly, we evaluated the impacts of one-day and two-day delayed city closure. We 129 took the average index of the population outflow between Jan 21 and 23 as the daily 130 index of population outflow before the city closure, and used the same calculation 131 method to estimate the index of population outflow within Wuhan increased by the 132 delayed city closure on Jan 24 and Jan 25 (the delayed outflow index). We multiplied 133 the delayed outflow index by the average proportion and one corresponding unit to 134 estimate the increased number of cases caused by one-day and two-day delayed city 135 closure in each province. Our study provided timely evidence for the formulation of efficient strategies to 253 prevent diseases from spreading out. On the one hand, the result could help assess 254 effectiveness of the prevention and control efforts. For example, the cases in Zhejiang 255 and Guangdong are apparently more than estimated, which indicated a better health 256 emergency response system (i.e. higher detection efficiency) or inadequate isolation. 257Whereas the cases reported in Henan were much lower than expected. Two possible 258 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.04.20020339 doi: medRxiv preprint play an important role in slowing the epidemic spread, especially when an effective 281 vaccine was developed (26, 27). In addition, to explore the impact of date selection, 282we estimated the changes of cases when the measure was implemented on different 283 date. The results varied significantly, 1420 cases could be prevented with the measure 284 implemented two days earlier, and the number of cases will increase by 1462 with the 285 lockdown implemented two days later, suggesting that the effect of the lockdown 286 depending on the choice of date greatly, which could provide a reference for the 287 future outbreaks. Since the political and economic effects were not considered, further 288 studies on secondary impacts of the measure, like socioeconomic impacts, were also 289 warranted. Though we estimated that some cases would possibly be prevented if the 290 policy was implemented earlier, it was actually hard to make such a huge decision 291given the whole picture of the infection was not clear at that stage. The authors 292 believe that the current policy was appropriate at this complex situation. 293There were a few limitations of our study. Firstly, for practical reasons, we used 294 an indicator to reflect the real-time magnitude of population movements, which was 295 acceptable considering our research purpose. Secondly, the influence of some 296 important factors, such as socioeconomic and demographic characteristics, were not 297 considered. Thirdly, it is assumed that the infected travelers in the population are 298 randomly distributed (28), and that there was no significant difference in surveillance 299 capability between cities (18), which might not be the case in reality. In addition, 300 daily data used in this study was reported infection data, rather than the actual number 301 of incident cases. More detailed and further in-depth studies are warranted in the 302 . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.04.20020339 doi: medRxiv preprint future. 303In conclusion, our study indicates that the population outflow from Wuhan might 304 be one important trigger of the 2019-nCoV infection transmission in China, and the 305 policy of city closure is effective to prevent the epidemic and earlier implementation 306 would be more effective. The magnitude of epidemic might be under-estimated and 307 should be paid more attention, such as Henan and Hunan provinces. 308 309\n"
     ]
    }
   ],
   "source": [
    "#print(tagged_data[33])\n",
    "print(parser.toList()[57])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling - Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to preprocess the data - Lemmatization and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    return WordNetLemmatizer().lemmatize(word,pos = \"v\")\n",
    "\n",
    "#part_of_paper could be set to \"title\",\"abstract\",\"body\" or \"whole\".\n",
    "#depends on which part of text do you want to tokenize\n",
    "def preprocess(paper,part_of_paper = \"whole\"):\n",
    "    text = \"\"\n",
    "    if(part_of_paper == \"title\"):\n",
    "        text = paper.title\n",
    "    elif(part_of_paper == \"abstract\"):\n",
    "        text = paper.abstract\n",
    "    elif(part_of_paper == \"body\"):\n",
    "        text = paper.body;\n",
    "    else:\n",
    "        text = paper.whole_text;\n",
    "    \n",
    "    tokens = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if(token not in STOPWORDS and len(token) > 3):\n",
    "            tokens.append(lemmatize(token))\n",
    "    \n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Before preprocessing----------------------\n",
      "['deep', 'learning', 'based', 'detection', 'for', 'covid', 'from', 'chest', 'ct', 'using']\n",
      "----------------------After preprocessing-----------------------\n",
      "['deep', 'learn', 'base', 'detection', 'covid', 'chest', 'weak', 'labelaccurate', 'rapid', 'diagnosis']\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------Before preprocessing----------------------\")\n",
    "print(tokenize(parser.data_dicts[Dataset.BIORXIV][3])[:10])\n",
    "print(\"----------------------After preprocessing-----------------------\")\n",
    "print(preprocess(parser.data_dicts[Dataset.BIORXIV][3])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets make a dictionary of all words that appear in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "papers = parser.data_dicts[Dataset.BIORXIV] #all papers from BIORXIV dataset\n",
    "for index in papers:\n",
    "    paper = papers[index]\n",
    "    documents.append(preprocess(paper))\n",
    "\n",
    "dictionary = Dictionary(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 10 words in our dictonary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 absence\n",
      "1 acetylation\n",
      "2 achieve\n",
      "3 act\n",
      "4 activate\n",
      "5 activation\n",
      "6 actively\n",
      "7 add\n",
      "8 adeno\n",
      "9 adipogenic\n",
      "10 affect\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtriranje top n najčešćih tokena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time for bag-of-words. We will do bag-of-words approach to each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_corpus = [dictionary.doc2bow(document) for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 10), (8, 1), (15, 10), (30, 1), (33, 5), (37, 3), (54, 2), (57, 7), (77, 1), (78, 1), (83, 2), (91, 1), (97, 35), (105, 1), (119, 2), (120, 1), (125, 3), (144, 2), (166, 3), (169, 2), (176, 11), (185, 1), (189, 1), (193, 2), (195, 1), (234, 1), (237, 7), (247, 1), (249, 16), (254, 1), (255, 1), (264, 1), (266, 2), (271, 3), (273, 16), (274, 5), (275, 1), (278, 2), (280, 3), (281, 1), (285, 5), (289, 3), (290, 1), (291, 2), (300, 2), (303, 23), (306, 1), (307, 2), (309, 1), (314, 1), (318, 4), (319, 2), (322, 1), (325, 1), (326, 3), (327, 2), (329, 2), (331, 2), (337, 2), (338, 23), (341, 1), (344, 2), (346, 3), (349, 1), (350, 11), (351, 2), (352, 1), (355, 6), (357, 2), (359, 26), (360, 3), (367, 1), (369, 2), (370, 5), (371, 5), (374, 1), (376, 13), (379, 12), (380, 2), (383, 1), (384, 1), (396, 7), (400, 1), (403, 1), (411, 5), (414, 33), (415, 28), (416, 3), (417, 1), (418, 1), (423, 7), (424, 3), (425, 1), (429, 1), (440, 1), (445, 15), (447, 1), (456, 2), (458, 2), (459, 1), (461, 1), (462, 13), (464, 1), (468, 15), (469, 9), (470, 3), (472, 2), (474, 12), (478, 2), (480, 2), (481, 2), (482, 1), (487, 12), (492, 1), (493, 1), (494, 1), (495, 5), (499, 4), (501, 1), (502, 1), (504, 5), (505, 6), (507, 1), (508, 1), (511, 1), (513, 1), (514, 3), (516, 2), (518, 3), (525, 14), (530, 1), (533, 8), (538, 3), (540, 1), (541, 3), (543, 5), (547, 4), (553, 1), (554, 3), (560, 28), (562, 2), (566, 2), (570, 1), (573, 3), (579, 3), (586, 1), (593, 1), (609, 1), (612, 2), (620, 1), (627, 1), (641, 2), (645, 2), (650, 2), (667, 4), (679, 3), (709, 1), (715, 1), (720, 1), (721, 9), (727, 1), (730, 1), (738, 2), (740, 1), (741, 1), (744, 1), (748, 1), (757, 1), (769, 1), (773, 1), (778, 7), (784, 2), (786, 5), (792, 1), (797, 1), (801, 1), (802, 1), (815, 2), (816, 1), (820, 3), (822, 4), (823, 1), (824, 1), (838, 2), (841, 1), (847, 2), (849, 2), (852, 3), (855, 1), (863, 3), (864, 1), (869, 1), (870, 1), (881, 2), (883, 7), (887, 2), (895, 1), (905, 1), (907, 1), (913, 1), (918, 1), (919, 2), (926, 2), (932, 1), (936, 3), (941, 1), (945, 1), (946, 1), (961, 3), (979, 3), (983, 1), (992, 1), (993, 2), (994, 27), (998, 2), (1005, 1), (1008, 2), (1014, 1), (1019, 1), (1025, 1), (1028, 1), (1036, 1), (1047, 1), (1054, 1), (1058, 2), (1059, 1), (1060, 2), (1071, 1), (1117, 1), (1130, 2), (1154, 2), (1159, 5), (1160, 2), (1184, 3), (1196, 1), (1241, 1), (1245, 3), (1248, 1), (1253, 1), (1254, 1), (1268, 3), (1275, 1), (1287, 1), (1296, 1), (1308, 1), (1336, 7), (1355, 1), (1358, 2), (1366, 1), (1377, 1), (1385, 1), (1390, 1), (1393, 1), (1407, 2), (1427, 2), (1429, 16), (1448, 1), (1450, 1), (1460, 1), (1469, 7), (1473, 1), (1478, 43), (1485, 1), (1486, 2), (1488, 4), (1497, 2), (1499, 1), (1511, 1), (1520, 1), (1529, 2), (1531, 1), (1540, 1), (1550, 1), (1551, 1), (1568, 1), (1579, 3), (1593, 1), (1620, 9), (1622, 6), (1630, 1), (1637, 2), (1642, 2), (1644, 1), (1669, 4), (1670, 4), (1677, 8), (1688, 2), (1693, 11), (1710, 1), (1712, 1), (1716, 3), (1728, 1), (1745, 1), (1779, 2), (1789, 1), (1808, 1), (1809, 1), (1818, 1), (1833, 1), (1857, 1), (1859, 1), (1865, 1), (1880, 1), (1900, 1), (1902, 1), (1920, 2), (1928, 1), (1934, 1), (1939, 3), (1942, 2), (1955, 1), (1962, 1), (1973, 4), (1981, 1), (1988, 1), (2001, 1), (2011, 1), (2013, 1), (2014, 21), (2016, 1), (2021, 2), (2022, 2), (2083, 1), (2089, 1), (2092, 3), (2105, 1), (2107, 1), (2108, 1), (2137, 2), (2221, 1), (2222, 1), (2225, 1), (2237, 1), (2241, 1), (2250, 1), (2277, 1), (2278, 1), (2279, 1), (2280, 1), (2281, 1), (2282, 1), (2283, 2), (2284, 1), (2285, 1), (2286, 5), (2287, 3), (2288, 1), (2289, 1), (2290, 1)]\n"
     ]
    }
   ],
   "source": [
    "#Lets choose the random index of the document\n",
    "\n",
    "bag_of_words_document_34 = bag_of_words_corpus[34]\n",
    "print(bag_of_words_document_34)\n",
    "\n",
    "#Lets explain the output\n",
    "# (6,2) -> 6 is the id of the word in dictionary \n",
    "#       -> 2 is the number of times the word occurred in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More visual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 (\"affect\") appears 10 time.\n",
      "Word 8 (\"animal\") appears 1 time.\n",
      "Word 15 (\"basic\") appears 10 time.\n",
      "Word 30 (\"clear\") appears 1 time.\n",
      "Word 33 (\"combine\") appears 5 time.\n",
      "Word 37 (\"construct\") appears 3 time.\n",
      "Word 54 (\"difficult\") appears 2 time.\n",
      "Word 57 (\"directly\") appears 7 time.\n",
      "Word 77 (\"explain\") appears 1 time.\n",
      "Word 78 (\"expose\") appears 1 time.\n",
      "Word 83 (\"fact\") appears 2 time.\n",
      "Word 91 (\"functional\") appears 1 time.\n",
      "Word 97 (\"growth\") appears 35 time.\n",
      "Word 105 (\"imply\") appears 1 time.\n",
      "Word 119 (\"line\") appears 2 time.\n",
      "Word 120 (\"link\") appears 1 time.\n",
      "Word 125 (\"make\") appears 3 time.\n",
      "Word 144 (\"normal\") appears 2 time.\n",
      "Word 166 (\"predict\") appears 3 time.\n",
      "Word 169 (\"primary\") appears 2 time.\n",
      "Word 176 (\"propagate\") appears 11 time.\n",
      "Word 185 (\"reason\") appears 1 time.\n",
      "Word 189 (\"reduction\") appears 1 time.\n",
      "Word 193 (\"relatively\") appears 2 time.\n",
      "Word 195 (\"relevant\") appears 1 time.\n",
      "Word 234 (\"treat\") appears 1 time.\n",
      "Word 237 (\"underlie\") appears 7 time.\n",
      "Word 247 (\"word\") appears 1 time.\n",
      "Word 249 (\"account\") appears 16 time.\n",
      "Word 254 (\"adjust\") appears 1 time.\n",
      "Word 255 (\"adjustment\") appears 1 time.\n",
      "Word 264 (\"analyse\") appears 1 time.\n",
      "Word 266 (\"approximation\") appears 2 time.\n",
      "Word 271 (\"assess\") appears 3 time.\n",
      "Word 273 (\"assume\") appears 16 time.\n",
      "Word 274 (\"average\") appears 5 time.\n",
      "Word 275 (\"avoid\") appears 1 time.\n",
      "Word 278 (\"believe\") appears 2 time.\n",
      "Word 280 (\"better\") appears 3 time.\n",
      "Word 281 (\"bias\") appears 1 time.\n",
      "Word 285 (\"calculate\") appears 5 time.\n",
      "Word 289 (\"chain\") appears 3 time.\n",
      "Word 290 (\"characterize\") appears 1 time.\n",
      "Word 291 (\"china\") appears 2 time.\n",
      "Word 300 (\"complete\") appears 2 time.\n",
      "Word 303 (\"confidence\") appears 23 time.\n",
      "Word 306 (\"consistent\") appears 1 time.\n",
      "Word 307 (\"contact\") appears 2 time.\n",
      "Word 309 (\"continue\") appears 1 time.\n",
      "Word 314 (\"countries\") appears 1 time.\n",
      "Word 318 (\"cumulative\") appears 4 time.\n",
      "Word 319 (\"curve\") appears 2 time.\n",
      "Word 322 (\"date\") appears 1 time.\n",
      "Word 325 (\"december\") appears 1 time.\n",
      "Word 326 (\"delay\") appears 3 time.\n",
      "Word 327 (\"depend\") appears 2 time.\n",
      "Word 329 (\"despite\") appears 2 time.\n",
      "Word 331 (\"detection\") appears 2 time.\n",
      "Word 337 (\"distribute\") appears 2 time.\n",
      "Word 338 (\"distribution\") appears 23 time.\n",
      "Word 341 (\"earliest\") appears 1 time.\n",
      "Word 344 (\"emerge\") appears 2 time.\n",
      "Word 346 (\"emphasize\") appears 3 time.\n",
      "Word 349 (\"ensure\") appears 1 time.\n",
      "Word 350 (\"epidemic\") appears 11 time.\n",
      "Word 351 (\"epidemiological\") appears 2 time.\n",
      "Word 352 (\"estimation\") appears 1 time.\n",
      "Word 355 (\"example\") appears 6 time.\n",
      "Word 357 (\"expect\") appears 2 time.\n",
      "Word 359 (\"exponential\") appears 26 time.\n",
      "Word 360 (\"exponentially\") appears 3 time.\n",
      "Word 367 (\"february\") appears 1 time.\n",
      "Word 369 (\"finally\") appears 2 time.\n",
      "Word 370 (\"fit\") appears 5 time.\n",
      "Word 371 (\"fix\") appears 5 time.\n",
      "Word 374 (\"fully\") appears 1 time.\n",
      "Word 376 (\"gamma\") appears 13 time.\n",
      "Word 379 (\"grant\") appears 12 time.\n",
      "Word 380 (\"greater\") appears 2 time.\n",
      "Word 383 (\"grow\") appears 1 time.\n",
      "Word 384 (\"help\") appears 1 time.\n",
      "Word 396 (\"incidence\") appears 7 time.\n",
      "Word 400 (\"individuals\") appears 1 time.\n",
      "Word 403 (\"infectious\") appears 1 time.\n",
      "Word 411 (\"instead\") appears 5 time.\n",
      "Word 414 (\"interval\") appears 33 time.\n",
      "Word 415 (\"intervals\") appears 28 time.\n",
      "Word 416 (\"intervention\") appears 3 time.\n",
      "Word 417 (\"intrinsic\") appears 1 time.\n",
      "Word 418 (\"inverse\") appears 1 time.\n",
      "Word 423 (\"january\") appears 7 time.\n",
      "Word 424 (\"joint\") appears 3 time.\n",
      "Word 425 (\"knowledge\") appears 1 time.\n",
      "Word 429 (\"larger\") appears 1 time.\n",
      "Word 440 (\"markov\") appears 1 time.\n",
      "Word 445 (\"medrxiv\") appears 15 time.\n",
      "Word 447 (\"middle\") appears 1 time.\n",
      "Word 456 (\"note\") appears 2 time.\n",
      "Word 458 (\"online\") appears 2 time.\n",
      "Word 459 (\"onset\") appears 1 time.\n",
      "Word 461 (\"originate\") appears 1 time.\n",
      "Word 462 (\"outbreak\") appears 13 time.\n",
      "Word 464 (\"outside\") appears 1 time.\n",
      "Word 468 (\"parameter\") appears 15 time.\n",
      "Word 469 (\"parameters\") appears 9 time.\n",
      "Word 470 (\"particularly\") appears 3 time.\n",
      "Word 472 (\"period\") appears 2 time.\n",
      "Word 474 (\"perpetuity\") appears 12 time.\n",
      "Word 478 (\"population\") appears 2 time.\n",
      "Word 480 (\"possibility\") appears 2 time.\n",
      "Word 481 (\"post\") appears 2 time.\n",
      "Word 482 (\"precise\") appears 1 time.\n",
      "Word 487 (\"probability\") appears 12 time.\n",
      "Word 492 (\"proxy\") appears 1 time.\n",
      "Word 493 (\"public\") appears 1 time.\n",
      "Word 494 (\"publicly\") appears 1 time.\n",
      "Word 495 (\"publish\") appears 5 time.\n",
      "Word 499 (\"rat\") appears 4 time.\n",
      "Word 501 (\"real\") appears 1 time.\n",
      "Word 502 (\"recover\") appears 1 time.\n",
      "Word 504 (\"reflect\") appears 5 time.\n",
      "Word 505 (\"rely\") appears 6 time.\n",
      "Word 507 (\"research\") appears 1 time.\n",
      "Word 508 (\"respect\") appears 1 time.\n",
      "Word 511 (\"risk\") appears 1 time.\n",
      "Word 513 (\"scenario\") appears 1 time.\n",
      "Word 514 (\"scenarios\") appears 3 time.\n",
      "Word 516 (\"secondary\") appears 2 time.\n",
      "Word 518 (\"sensitivity\") appears 3 time.\n",
      "Word 525 (\"shape\") appears 14 time.\n",
      "Word 530 (\"size\") appears 1 time.\n",
      "Word 533 (\"source\") appears 8 time.\n",
      "Word 538 (\"standard\") appears 3 time.\n",
      "Word 540 (\"state\") appears 1 time.\n",
      "Word 541 (\"statistical\") appears 3 time.\n",
      "Word 543 (\"step\") appears 5 time.\n",
      "Word 547 (\"susceptible\") appears 4 time.\n",
      "Word 553 (\"tend\") appears 1 time.\n",
      "Word 554 (\"trace\") appears 3 time.\n",
      "Word 560 (\"uncertainty\") appears 28 time.\n",
      "Word 562 (\"understand\") appears 2 time.\n",
      "Word 566 (\"update\") appears 2 time.\n",
      "Word 570 (\"variable\") appears 1 time.\n",
      "Word 573 (\"vary\") appears 3 time.\n",
      "Word 579 (\"widely\") appears 3 time.\n",
      "Word 586 (\"zero\") appears 1 time.\n",
      "Word 593 (\"accurately\") appears 1 time.\n",
      "Word 609 (\"appear\") appears 1 time.\n",
      "Word 612 (\"availability\") appears 2 time.\n",
      "Word 620 (\"branch\") appears 1 time.\n",
      "Word 627 (\"certain\") appears 1 time.\n",
      "Word 641 (\"coefficient\") appears 2 time.\n",
      "Word 645 (\"combinations\") appears 2 time.\n",
      "Word 650 (\"components\") appears 2 time.\n",
      "Word 667 (\"crucial\") appears 4 time.\n",
      "Word 679 (\"deviations\") appears 3 time.\n",
      "Word 709 (\"examples\") appears 1 time.\n",
      "Word 715 (\"face\") appears 1 time.\n",
      "Word 720 (\"form\") appears 1 time.\n",
      "Word 721 (\"framework\") appears 9 time.\n",
      "Word 727 (\"global\") appears 1 time.\n",
      "Word 730 (\"great\") appears 1 time.\n",
      "Word 738 (\"identical\") appears 2 time.\n",
      "Word 740 (\"illustrate\") appears 1 time.\n",
      "Word 741 (\"independent\") appears 1 time.\n",
      "Word 744 (\"initially\") appears 1 time.\n",
      "Word 748 (\"involve\") appears 1 time.\n",
      "Word 757 (\"linear\") appears 1 time.\n",
      "Word 769 (\"mathematical\") appears 1 time.\n",
      "Word 773 (\"mers\") appears 1 time.\n",
      "Word 778 (\"narrow\") appears 7 time.\n",
      "Word 784 (\"open\") appears 2 time.\n",
      "Word 786 (\"organization\") appears 5 time.\n",
      "Word 792 (\"pose\") appears 1 time.\n",
      "Word 797 (\"predictions\") appears 1 time.\n",
      "Word 801 (\"prevention\") appears 1 time.\n",
      "Word 802 (\"procedure\") appears 1 time.\n",
      "Word 815 (\"recent\") appears 2 time.\n",
      "Word 816 (\"recently\") appears 1 time.\n",
      "Word 820 (\"relationship\") appears 3 time.\n",
      "Word 822 (\"researchers\") appears 4 time.\n",
      "Word 823 (\"resolution\") appears 1 time.\n",
      "Word 824 (\"resources\") appears 1 time.\n",
      "Word 838 (\"set\") appears 2 time.\n",
      "Word 841 (\"site\") appears 1 time.\n",
      "Word 847 (\"square\") appears 2 time.\n",
      "Word 849 (\"strategies\") appears 2 time.\n",
      "Word 852 (\"strong\") appears 3 time.\n",
      "Word 855 (\"structure\") appears 1 time.\n",
      "Word 863 (\"term\") appears 3 time.\n",
      "Word 864 (\"text\") appears 1 time.\n",
      "Word 869 (\"timely\") appears 1 time.\n",
      "Word 870 (\"tool\") appears 1 time.\n",
      "Word 881 (\"wide\") appears 2 time.\n",
      "Word 883 (\"world\") appears 7 time.\n",
      "Word 887 (\"accurate\") appears 2 time.\n",
      "Word 895 (\"applications\") appears 1 time.\n",
      "Word 905 (\"black\") appears 1 time.\n",
      "Word 907 (\"center\") appears 1 time.\n",
      "Word 913 (\"come\") appears 1 time.\n",
      "Word 918 (\"consist\") appears 1 time.\n",
      "Word 919 (\"constant\") appears 2 time.\n",
      "Word 926 (\"diagnostic\") appears 2 time.\n",
      "Word 932 (\"efforts\") appears 1 time.\n",
      "Word 936 (\"explanations\") appears 3 time.\n",
      "Word 941 (\"fast\") appears 1 time.\n",
      "Word 945 (\"final\") appears 1 time.\n",
      "Word 946 (\"frame\") appears 1 time.\n",
      "Word 961 (\"importance\") appears 3 time.\n",
      "Word 979 (\"noise\") appears 3 time.\n",
      "Word 983 (\"original\") appears 1 time.\n",
      "Word 992 (\"play\") appears 1 time.\n",
      "Word 993 (\"plot\") appears 2 time.\n",
      "Word 994 (\"pool\") appears 27 time.\n",
      "Word 998 (\"problem\") appears 2 time.\n",
      "Word 1005 (\"quarantine\") appears 1 time.\n",
      "Word 1008 (\"random\") appears 2 time.\n",
      "Word 1014 (\"release\") appears 1 time.\n",
      "Word 1019 (\"role\") appears 1 time.\n",
      "Word 1025 (\"short\") appears 1 time.\n",
      "Word 1028 (\"simple\") appears 1 time.\n",
      "Word 1036 (\"stag\") appears 1 time.\n",
      "Word 1047 (\"true\") appears 1 time.\n",
      "Word 1054 (\"vertical\") appears 1 time.\n",
      "Word 1058 (\"weak\") appears 2 time.\n",
      "Word 1059 (\"width\") appears 1 time.\n",
      "Word 1060 (\"ability\") appears 2 time.\n",
      "Word 1071 (\"alternative\") appears 1 time.\n",
      "Word 1117 (\"correlations\") appears 1 time.\n",
      "Word 1130 (\"draw\") appears 2 time.\n",
      "Word 1154 (\"family\") appears 2 time.\n",
      "Word 1159 (\"focus\") appears 5 time.\n",
      "Word 1160 (\"fold\") appears 2 time.\n",
      "Word 1184 (\"incorporate\") appears 3 time.\n",
      "Word 1196 (\"lack\") appears 1 time.\n",
      "Word 1241 (\"part\") appears 1 time.\n",
      "Word 1245 (\"particular\") appears 3 time.\n",
      "Word 1248 (\"pathogens\") appears 1 time.\n",
      "Word 1253 (\"phase\") appears 1 time.\n",
      "Word 1254 (\"phenomenon\") appears 1 time.\n",
      "Word 1268 (\"prior\") appears 3 time.\n",
      "Word 1275 (\"quantification\") appears 1 time.\n",
      "Word 1287 (\"reproducibility\") appears 1 time.\n",
      "Word 1296 (\"shorter\") appears 1 time.\n",
      "Word 1308 (\"surround\") appears 1 time.\n",
      "Word 1336 (\"variation\") appears 7 time.\n",
      "Word 1355 (\"apparent\") appears 1 time.\n",
      "Word 1358 (\"baseline\") appears 2 time.\n",
      "Word 1366 (\"degree\") appears 1 time.\n",
      "Word 1377 (\"goal\") appears 1 time.\n",
      "Word 1385 (\"interpret\") appears 1 time.\n",
      "Word 1390 (\"mix\") appears 1 time.\n",
      "Word 1393 (\"observation\") appears 1 time.\n",
      "Word 1407 (\"seven\") appears 2 time.\n",
      "Word 1427 (\"article\") appears 2 time.\n",
      "Word 1429 (\"assumptions\") appears 16 time.\n",
      "Word 1448 (\"contribution\") appears 1 time.\n",
      "Word 1450 (\"cover\") appears 1 time.\n",
      "Word 1460 (\"direction\") appears 1 time.\n",
      "Word 1469 (\"error\") appears 7 time.\n",
      "Word 1473 (\"explore\") appears 1 time.\n",
      "Word 1478 (\"generation\") appears 43 time.\n",
      "Word 1485 (\"humans\") appears 1 time.\n",
      "Word 1486 (\"idea\") appears 2 time.\n",
      "Word 1488 (\"individual\") appears 4 time.\n",
      "Word 1497 (\"like\") appears 2 time.\n",
      "Word 1499 (\"longer\") appears 1 time.\n",
      "Word 1511 (\"negligible\") appears 1 time.\n",
      "Word 1520 (\"overlap\") appears 1 time.\n",
      "Word 1529 (\"proper\") appears 2 time.\n",
      "Word 1531 (\"purpose\") appears 1 time.\n",
      "Word 1540 (\"restrict\") appears 1 time.\n",
      "Word 1550 (\"strongly\") appears 1 time.\n",
      "Word 1551 (\"substitute\") appears 1 time.\n",
      "Word 1568 (\"use\") appears 1 time.\n",
      "Word 1579 (\"bayesian\") appears 3 time.\n",
      "Word 1593 (\"effort\") appears 1 time.\n",
      "Word 1620 (\"posterior\") appears 9 time.\n",
      "Word 1622 (\"read\") appears 6 time.\n",
      "Word 1630 (\"transmit\") appears 1 time.\n",
      "Word 1637 (\"week\") appears 2 time.\n",
      "Word 1642 (\"appropriate\") appears 2 time.\n",
      "Word 1644 (\"attention\") appears 1 time.\n",
      "Word 1669 (\"half\") appears 4 time.\n",
      "Word 1670 (\"infer\") appears 4 time.\n",
      "Word 1677 (\"median\") appears 8 time.\n",
      "Word 1688 (\"progress\") appears 2 time.\n",
      "Word 1693 (\"reproductive\") appears 11 time.\n",
      "Word 1710 (\"basis\") appears 1 time.\n",
      "Word 1712 (\"care\") appears 1 time.\n",
      "Word 1716 (\"course\") appears 3 time.\n",
      "Word 1728 (\"guidelines\") appears 1 time.\n",
      "Word 1745 (\"plan\") appears 1 time.\n",
      "Word 1779 (\"equation\") appears 2 time.\n",
      "Word 1789 (\"largest\") appears 1 time.\n",
      "Word 1808 (\"summary\") appears 1 time.\n",
      "Word 1809 (\"surprisingly\") appears 1 time.\n",
      "Word 1818 (\"assumption\") appears 1 time.\n",
      "Word 1833 (\"hold\") appears 1 time.\n",
      "Word 1857 (\"stronger\") appears 1 time.\n",
      "Word 1859 (\"symptom\") appears 1 time.\n",
      "Word 1865 (\"comparisons\") appears 1 time.\n",
      "Word 1880 (\"awareness\") appears 1 time.\n",
      "Word 1900 (\"official\") appears 1 time.\n",
      "Word 1902 (\"poor\") appears 1 time.\n",
      "Word 1920 (\"approximate\") appears 2 time.\n",
      "Word 1928 (\"workers\") appears 1 time.\n",
      "Word 1934 (\"interrupt\") appears 1 time.\n",
      "Word 1939 (\"sensitive\") appears 3 time.\n",
      "Word 1942 (\"threat\") appears 2 time.\n",
      "Word 1955 (\"effectively\") appears 1 time.\n",
      "Word 1962 (\"move\") appears 1 time.\n",
      "Word 1973 (\"wider\") appears 4 time.\n",
      "Word 1981 (\"likewise\") appears 1 time.\n",
      "Word 1988 (\"shift\") appears 1 time.\n",
      "Word 2001 (\"code\") appears 1 time.\n",
      "Word 2011 (\"deal\") appears 1 time.\n",
      "Word 2013 (\"description\") appears 1 time.\n",
      "Word 2014 (\"distributions\") appears 21 time.\n",
      "Word 2016 (\"gather\") appears 1 time.\n",
      "Word 2021 (\"ignore\") appears 2 time.\n",
      "Word 2022 (\"impose\") appears 2 time.\n",
      "Word 2083 (\"outline\") appears 1 time.\n",
      "Word 2089 (\"recommendations\") appears 1 time.\n",
      "Word 2092 (\"replace\") appears 3 time.\n",
      "Word 2105 (\"dash\") appears 1 time.\n",
      "Word 2107 (\"epidemics\") appears 1 time.\n",
      "Word 2108 (\"errors\") appears 1 time.\n",
      "Word 2137 (\"uniform\") appears 2 time.\n",
      "Word 2221 (\"wear\") appears 1 time.\n",
      "Word 2222 (\"appendix\") appears 1 time.\n",
      "Word 2225 (\"conclusions\") appears 1 time.\n",
      "Word 2237 (\"purple\") appears 1 time.\n",
      "Word 2241 (\"seir\") appears 1 time.\n",
      "Word 2250 (\"apart\") appears 1 time.\n",
      "Word 2277 (\"choice\") appears 1 time.\n",
      "Word 2278 (\"decay\") appears 1 time.\n",
      "Word 2279 (\"exception\") appears 1 time.\n",
      "Word 2280 (\"forecast\") appears 1 time.\n",
      "Word 2281 (\"hope\") appears 1 time.\n",
      "Word 2282 (\"hypothetical\") appears 1 time.\n",
      "Word 2283 (\"informative\") appears 2 time.\n",
      "Word 2284 (\"kind\") appears 1 time.\n",
      "Word 2285 (\"phenomena\") appears 1 time.\n",
      "Word 2286 (\"quantities\") appears 5 time.\n",
      "Word 2287 (\"reasonable\") appears 3 time.\n",
      "Word 2288 (\"scarce\") appears 1 time.\n",
      "Word 2289 (\"simplicity\") appears 1 time.\n",
      "Word 2290 (\"variance\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bag_of_words_document_34)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bag_of_words_document_34[i][0], \n",
    "                                               dictionary[bag_of_words_document_34[i][0]],bag_of_words_document_34[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Besides Bag-of-Words we can also use TF-IDF which is usually more accurate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bag_of_words_corpus)\n",
    "corpus_tfidf = tfidf[bag_of_words_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.09769017238665445),\n",
      " (1, 0.01061565164204266),\n",
      " (2, 0.020660439663961923),\n",
      " (3, 0.020660439663961923),\n",
      " (4, 0.01997696044845842),\n",
      " (5, 0.02998037611460888),\n",
      " (6, 0.011518049469993385),\n",
      " (7, 0.015629019266530028),\n",
      " (8, 0.03628945893049195),\n",
      " (9, 0.03628945893049195),\n",
      " (10, 0.03377374873177601),\n",
      " (11, 0.014761364006274133),\n",
      " (12, 0.03550693994808871),\n",
      " (13, 0.16330256518721378),\n",
      " (14, 0.10132124619532804),\n",
      " (15, 0.015186846739026929),\n",
      " (16, 0.051918478197021986),\n",
      " (17, 0.01656905971474332),\n",
      " (18, 0.02998037611460888),\n",
      " (19, 0.02998037611460888),\n",
      " (20, 0.041320879327923846),\n",
      " (21, 0.041320879327923846),\n",
      " (22, 0.03377374873177601),\n",
      " (23, 0.028702713696157706),\n",
      " (24, 0.1400728013036529),\n",
      " (25, 0.13429285781575248),\n",
      " (26, 0.048267733700932965),\n",
      " (27, 0.018144729465245975),\n",
      " (28, 0.01357353436826171),\n",
      " (29, 0.03377374873177601),\n",
      " (30, 0.01061565164204266),\n",
      " (31, 0.030373693478053857),\n",
      " (32, 0.02998037611460888),\n",
      " (33, 0.008754550081478307),\n",
      " (34, 0.024884716487559244),\n",
      " (35, 0.008754550081478307),\n",
      " (36, 0.011518049469993385),\n",
      " (37, 0.04305407054423656),\n",
      " (38, 0.08866346592537547),\n",
      " (39, 0.060747386956107714),\n",
      " (40, 0.10242124112576537),\n",
      " (41, 0.00851209010676063),\n",
      " (42, 0.0112091546872697),\n",
      " (43, 0.03377374873177601),\n",
      " (44, 0.06754749746355201),\n",
      " (45, 0.018144729465245975),\n",
      " (46, 0.07593423369513465),\n",
      " (47, 0.03171826383350768),\n",
      " (48, 0.05694989859445388),\n",
      " (49, 0.012845930030696909),\n",
      " (50, 0.011518049469993385),\n",
      " (51, 0.03171826383350768),\n",
      " (52, 0.0193325591032584),\n",
      " (53, 0.024884716487559244),\n",
      " (54, 0.01357353436826171),\n",
      " (55, 0.02138804400152672),\n",
      " (56, 0.03171826383350768),\n",
      " (57, 0.01800467854255488),\n",
      " (58, 0.010330219831980961),\n",
      " (59, 0.025959239098510993),\n",
      " (60, 0.01608924456697766),\n",
      " (61, 0.03171826383350768),\n",
      " (62, 0.09515479150052304),\n",
      " (63, 0.10383695639404397),\n",
      " (64, 0.017070206854294227),\n",
      " (65, 0.05694989859445388),\n",
      " (66, 0.018144729465245975),\n",
      " (67, 0.027147068736523415),\n",
      " (68, 0.015629019266530028),\n",
      " (69, 0.02847494929722694),\n",
      " (70, 0.012499011757309957),\n",
      " (71, 0.031846954926127984),\n",
      " (72, 0.044331732962687734),\n",
      " (73, 0.015629019266530028),\n",
      " (74, 0.018723007721805584),\n",
      " (75, 0.013203863404417752),\n",
      " (76, 0.0351893427687192),\n",
      " (77, 0.011518049469993385),\n",
      " (78, 0.06251607706612011),\n",
      " (79, 0.051383720122787635),\n",
      " (80, 0.36903410015685334),\n",
      " (81, 0.018723007721805584),\n",
      " (82, 0.025959239098510993),\n",
      " (83, 0.04186721673713762),\n",
      " (84, 0.012162450815193404),\n",
      " (85, 0.02138804400152672),\n",
      " (86, 0.06343652766701537),\n",
      " (87, 0.025959239098510993),\n",
      " (88, 0.018144729465245975),\n",
      " (89, 0.01608924456697766),\n",
      " (90, 0.01608924456697766),\n",
      " (91, 0.028702713696157706),\n",
      " (92, 0.02181699617708114),\n",
      " (93, 0.014761364006274133),\n",
      " (94, 0.03217848913395532),\n",
      " (95, 0.03171826383350768),\n",
      " (96, 0.018144729465245975),\n",
      " (97, 0.012499011757309957),\n",
      " (98, 0.024884716487559244),\n",
      " (99, 0.012845930030696909),\n",
      " (100, 0.023903754200242672),\n",
      " (101, 0.03377374873177601),\n",
      " (102, 0.03414041370858845),\n",
      " (103, 0.024884716487559244),\n",
      " (104, 0.05429413747304683),\n",
      " (105, 0.0175946713843596),\n",
      " (106, 0.038537790092090726),\n",
      " (107, 0.06900406911687583),\n",
      " (108, 0.023903754200242672),\n",
      " (109, 0.023903754200242672),\n",
      " (110, 0.015186846739026929),\n",
      " (111, 0.017070206854294227),\n",
      " (112, 0.06754749746355201),\n",
      " (113, 0.024998023514619914),\n",
      " (114, 0.023903754200242672),\n",
      " (115, 0.01061565164204266),\n",
      " (116, 0.10132124619532804),\n",
      " (117, 0.03377374873177601),\n",
      " (118, 0.013203863404417752),\n",
      " (119, 0.054542490442702846),\n",
      " (120, 0.013955738912379206),\n",
      " (121, 0.03377374873177601),\n",
      " (122, 0.013955738912379206),\n",
      " (123, 0.018723007721805584),\n",
      " (124, 0.01608924456697766),\n",
      " (125, 0.011835646649362904),\n",
      " (126, 0.04600271274458388),\n",
      " (127, 0.17084969578336165),\n",
      " (128, 0.0193325591032584),\n",
      " (129, 0.0175946713843596),\n",
      " (130, 0.022165866481343867),\n",
      " (131, 0.06977869456189603),\n",
      " (132, 0.071711262600728),\n",
      " (133, 0.02300135637229194),\n",
      " (134, 0.018144729465245975),\n",
      " (135, 0.33747010828064294),\n",
      " (136, 0.00851209010676063),\n",
      " (137, 0.2855516300836209),\n",
      " (138, 0.04976943297511849),\n",
      " (139, 0.06198131899188577),\n",
      " (140, 0.0175946713843596),\n",
      " (141, 0.02138804400152672),\n",
      " (142, 0.09112108043416157),\n",
      " (143, 0.020660439663961923),\n",
      " (144, 0.015186846739026929),\n",
      " (145, 0.04600271274458388),\n",
      " (146, 0.020660439663961923),\n",
      " (147, 0.06343652766701537),\n",
      " (148, 0.13509499492710403),\n",
      " (149, 0.027147068736523415),\n",
      " (150, 0.06754749746355201),\n",
      " (151, 0.0175946713843596),\n",
      " (152, 0.041320879327923846),\n",
      " (153, 0.05429413747304683),\n",
      " (154, 0.11992150445843552),\n",
      " (155, 0.20264249239065607),\n",
      " (156, 0.11500678186145971),\n",
      " (157, 0.06343652766701537),\n",
      " (158, 0.01090849808854057),\n",
      " (159, 0.10652081984426613),\n",
      " (160, 0.05996075222921776),\n",
      " (161, 0.23984300891687105),\n",
      " (162, 0.0664975994440316),\n",
      " (163, 0.06754749746355201),\n",
      " (164, 0.022165866481343867),\n",
      " (165, 0.03171826383350768),\n",
      " (166, 0.009514879998756016),\n",
      " (167, 0.013955738912379206),\n",
      " (168, 0.03600935708510976),\n",
      " (169, 0.12503215413224023),\n",
      " (170, 0.01902975999751203),\n",
      " (171, 0.01656905971474332),\n",
      " (172, 0.03377374873177601),\n",
      " (173, 0.03377374873177601),\n",
      " (174, 0.041320879327923846),\n",
      " (175, 0.09515479150052304),\n",
      " (176, 0.03377374873177601),\n",
      " (177, 0.02138804400152672),\n",
      " (178, 0.025536270320281888),\n",
      " (179, 0.01608924456697766),\n",
      " (180, 0.018144729465245975),\n",
      " (181, 0.027147068736523415),\n",
      " (182, 0.025959239098510993),\n",
      " (183, 0.03171826383350768),\n",
      " (184, 0.05429413747304683),\n",
      " (185, 0.01357353436826171),\n",
      " (186, 0.03377374873177601),\n",
      " (187, 0.025959239098510993),\n",
      " (188, 0.024884716487559244),\n",
      " (189, 0.014761364006274133),\n",
      " (190, 0.02791147782475841),\n",
      " (191, 0.0193325591032584),\n",
      " (192, 0.02847494929722694),\n",
      " (193, 0.012845930030696909),\n",
      " (194, 0.02847494929722694),\n",
      " (195, 0.018144729465245975),\n",
      " (196, 0.02847494929722694),\n",
      " (197, 0.01061565164204266),\n",
      " (198, 0.12871395653582127),\n",
      " (199, 0.10366244522994046),\n",
      " (200, 0.10652081984426613),\n",
      " (201, 0.008274734933712643),\n",
      " (202, 0.02998037611460888),\n",
      " (203, 0.08563391998880415),\n",
      " (204, 0.03377374873177601),\n",
      " (205, 0.012499011757309957),\n",
      " (206, 0.028702713696157706),\n",
      " (207, 0.0175946713843596),\n",
      " (208, 0.0175946713843596),\n",
      " (209, 0.05694989859445388),\n",
      " (210, 0.02791147782475841),\n",
      " (211, 0.014761364006274133),\n",
      " (212, 0.027147068736523415),\n",
      " (213, 0.08144120620957025),\n",
      " (214, 0.025959239098510993),\n",
      " (215, 0.0193325591032584),\n",
      " (216, 0.03313811942948664),\n",
      " (217, 0.06754749746355201),\n",
      " (218, 0.027147068736523415),\n",
      " (219, 0.013203863404417752),\n",
      " (220, 0.02300135637229194),\n",
      " (221, 0.03171826383350768),\n",
      " (222, 0.0193325591032584),\n",
      " (223, 0.051918478197021986),\n",
      " (224, 0.03171826383350768),\n",
      " (225, 0.01997696044845842),\n",
      " (226, 0.03171826383350768),\n",
      " (227, 0.023903754200242672),\n",
      " (228, 0.03995392089691684),\n",
      " (229, 0.05616902316541675),\n",
      " (230, 0.13573534368261708),\n",
      " (231, 0.05616902316541675),\n",
      " (232, 0.02847494929722694),\n",
      " (233, 0.02847494929722694),\n",
      " (234, 0.01090849808854057),\n",
      " (235, 0.03171826383350768),\n",
      " (236, 0.018144729465245975),\n",
      " (237, 0.017070206854294227),\n",
      " (238, 0.015186846739026929),\n",
      " (239, 0.03171826383350768),\n",
      " (240, 0.025959239098510993),\n",
      " (241, 0.022165866481343867),\n",
      " (242, 0.08264175865584769),\n",
      " (243, 0.03377374873177601),\n",
      " (244, 0.08994112834382663),\n",
      " (245, 0.041320879327923846),\n",
      " (246, 0.017070206854294227),\n",
      " (247, 0.023903754200242672),\n",
      " (248, 0.023903754200242672)]\n"
     ]
    }
   ],
   "source": [
    "#Example output\n",
    "# (0,0.32) -> 0 is the id in the dictonary\n",
    "#          -> 0.32 is the frequency of term in dictionary\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally! We are for our first topic modelling algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will use LDA algorithm - or formally more know as Latent Dirichlet Allocation. You can picture it as clustering GMM algorithm for topic modelling. Basically we are running through the text and algorithm will pick some words and depending on how often they are occuring and more, it will give some certainty on whether is that word a topic for that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So here we are actually training this algorithm. We are using our bag of words corpus and we selected 10 topics. Id2Word means that we will look into our dictionary what the words in our topic means ( remove it to see the difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus_tfidf, num_topics=5,id2word = dictionary,passes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay, this output needs some explanation. As you can see we have 5 topics. This means that these five topics (or more precise five distributions ) present our dataset. In other words, this means that this are 5 main topics in our dataset. To clarify, topic represents a distribution of words that represent that topic. For example, Topic 0 is 0,006 percent patients, 0,004 percent covid and so on...  Also, important to mention is that we can see that Topic 0 is vastly different from Topic 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTopic \u001b[0m0:\n",
      "0.006*\"patients\" + 0.004*\"covid\" + 0.004*\"medrxiv\" + 0.003*\"perpetuity\" + 0.003*\"grant\" + 0.002*\"wuhan\" + 0.002*\"cells\" + 0.002*\"read\" + 0.002*\"hospital\" + 0.002*\"january\"\n",
      "-------------------------------------------\n",
      "\u001b[1mTopic \u001b[0m1:\n",
      "0.002*\"heat\" + 0.002*\"antibody\" + 0.002*\"transition\" + 0.002*\"interval\" + 0.001*\"reproductive\" + 0.001*\"medrxiv\" + 0.001*\"outflow\" + 0.001*\"january\" + 0.001*\"covid\" + 0.001*\"variants\"\n",
      "-------------------------------------------\n",
      "\u001b[1mTopic \u001b[0m2:\n",
      "0.003*\"protein\" + 0.002*\"proteins\" + 0.002*\"bind\" + 0.002*\"permission\" + 0.002*\"epitopes\" + 0.002*\"reuse\" + 0.002*\"ncov\" + 0.002*\"codon\" + 0.002*\"reserve\" + 0.002*\"structure\"\n",
      "-------------------------------------------\n",
      "\u001b[1mTopic \u001b[0m3:\n",
      "0.002*\"train\" + 0.002*\"filter\" + 0.002*\"learn\" + 0.002*\"deep\" + 0.001*\"host\" + 0.001*\"strength\" + 0.001*\"speed\" + 0.001*\"network\" + 0.001*\"recovery\" + 0.001*\"covid\"\n",
      "-------------------------------------------\n",
      "\u001b[1mTopic \u001b[0m4:\n",
      "0.002*\"protein\" + 0.001*\"adaptation\" + 0.001*\"publication\" + 0.001*\"residues\" + 0.001*\"individuals\" + 0.001*\"mice\" + 0.001*\"domain\" + 0.001*\"signal\" + 0.001*\"category\" + 0.001*\"selection\"\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    print('\\033[1m'+\"Topic \"+'\\033[0m'+str(index)+\":\")\n",
    "    print(lda.print_topic(index))\n",
    "    print(\"-------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have main topics for this dataset, we want to see to which topic does individual papers belong. To do that, we are going to use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real', 'time', 'estimation', 'risk', 'death', 'novel', 'coronavirus', 'covid', 'infection', 'inference', 'export', 'casesthe', 'export', 'case', 'novel', 'coronavirus', 'infection', 'confirm', 'outside', 'china', 'provide', 'opportunity', 'estimate', 'cumulative', 'incidence', 'confirm', 'case', 'fatality', 'risk', 'ccfr', 'mainland', 'china', 'knowledge', 'ccfr', 'critical', 'characterize', 'severity', 'understand', 'pandemic', 'potential', 'covid', 'early', 'stage', 'epidemic', 'exponential', 'growth', 'rate', 'incidence', 'present', 'study', 'statistically', 'estimate', 'ccfr', 'basic', 'reproduction', 'number', 'average', 'number', 'secondary', 'case', 'generate', 'single', 'primary', 'case', 'naïve', 'population', 'model', 'epidemic', 'growth', 'single', 'index', 'case', 'illness', 'onset', 'december', 'scenario', 'growth', 'rate', 'fit', 'parameters', 'scenario', 'base', 'data', 'export', 'case', 'report', 'january', 'cumulative', 'incidence', 'china', 'january', 'estimate', 'case', 'case', 'respectively', 'latest', 'estimate', 'value', 'ccfr', 'scenario', 'scenario', 'basic', 'reproduction', 'number', 'estimate', 'scenarios', 'respectively', 'base', 'result', 'argue', 'current', 'covid', 'epidemic', 'substantial', 'potential', 'cause', 'pandemic', 'propose', 'approach', 'provide', 'insights', 'early', 'risk', 'assessment', 'publicly', 'available', 'data', 'december', 'cluster', 'pneumonia', 'case', 'unknown', 'etiology', 'emerge', 'wuhan', 'city', 'hubei', 'province', 'china', 'virological', 'investigation', 'suggest', 'causative', 'agent', 'pneumonia', 'novel', 'coronavirus', 'covid', 'january', 'total', 'case', 'include', 'deaths', 'confirm', 'case', 'covid', 'infections', 'report', 'outside', 'china', 'asian', 'countries', 'unite', 'state', 'france', 'australia', 'canada', 'local', 'market', 'sell', 'seafood', 'wildlife', 'wuhan', 'visit', 'case', 'initial', 'cluster', 'indicate', 'common', 'source', 'zoonotic', 'exposure', 'main', 'mode', 'transmission', 'shut', 'market', 'number', 'case', 'continue', 'grow', 'china', 'instance', 'household', 'transmission', 'report', 'speculate', 'sustain', 'human', 'human', 'transmission', 'aid', 'establishment', 'epidemic', 'report', 'case', 'count', 'greatly', 'underestimate', 'actual', 'number', 'infections', 'china', 'early', 'assessment', 'severity', 'infection', 'help', 'quantify', 'pandemic', 'potential', 'covid', 'anticipate', 'likely', 'number', 'deaths', 'epidemic', 'important', 'epidemiological', 'measure', 'severity', 'case', 'fatality', 'risk', 'measure', 'different', 'approach', 'estimate', 'proportion', 'cumulative', 'number', 'deaths', 'cumulative', 'number', 'case', 'point', 'time', 'ratio', 'cumulative', 'number', 'deaths', 'cumulative', 'number', 'infect', 'individuals', 'clinical', 'outcome', 'know', 'decease', 'recover', 'risk', 'death', 'confirm', 'case', 'explicitly', 'account', 'time', 'illness', 'onset', 'death', 'estimate', 'ratio', 'deaths', 'confirm', 'case', 'ccfr', 'adjustment', 'time', 'delay', 'illness', 'onset', 'death', 'method', 'provide', 'insight', 'severity', 'disease', 'naïve', 'base', 'method', 'tend', 'underestimate', 'real', 'time', 'nature', 'growth', 'fatal', 'case', 'example', 'early', 'stage', 'epidemic', 'fail', 'right', 'censor', 'case', 'respect', 'time', 'delay', 'illness', 'onset', 'death', 'lead', 'underestimation', 'death', 'infection', 'occur', 'follow', 'case', 'identification', 'denominator', 'include', 'confirm', 'case', 'refer', 'ccfr', 'overestimate', 'actual', 'infect', 'individuals', 'ascertainment', 'infections', 'population', 'nonetheless', 'ccfr', 'valuable', 'measure', 'upper', 'bind', 'symptomatic', 'scfr', 'symptomatic', 'case', 'particularly', 'circumstances', 'high', 'uncertainty', 'emergence', 'human', 'pathogen', 'basic', 'reproduction', 'number', 'average', 'number', 'secondary', 'case', 'generate', 'single', 'primary', 'case', 'fully', 'susceptible', 'population', 'represent', 'epidemiological', 'measurement', 'help', 'quantify', 'pandemic', 'potential', 'covid', 'define', 'pandemic', 'worldwide', 'spread', 'newly', 'emerge', 'disease', 'number', 'simultaneously', 'infect', 'individuals', 'exceed', 'capacity', 'treatment', 'growth', 'rate', 'estimate', 'cumulative', 'incidence', 'exportation', 'case', 'account', 'time', 'delay', 'illness', 'onset', 'death', 'present', 'study', 'aim', 'estimate', 'ccfr', 'covid', 'real', 'time', 'information', 'export', 'covid', 'case', 'confirm', 'countries', 'deaths', 'covid', 'infection', 'china', 'retrieve', 'announcement', 'date', 'current', 'outbreak', 'december', 'january', 'cutoff', 'time', 'january', 'specifically', 'select', 'reflect', 'governmental', 'public', 'transportation', 'wuhan', 'include', 'flight', 'invoke', 'january', 'data', 'collect', 'government', 'websites', 'media', 'quote', 'government', 'announcements', 'data', 'include', 'case', 'diagnose', 'outside', 'china', 'illness', 'onset', 'january', 'report', 'february', 'date', 'illness', 'onset', 'death', 'decease', 'case', 'china', 'observe', 'incidence', 'date', 'illness', 'onset', 'model', 'exponential', 'growth', 'model', 'rate', 'expect', 'number', 'infect', 'case', 'time', 'cumulative', 'incidence', 'integral', 'time', 'interval', 'zero', 'write', 'cumulative', 'incidence', 'adjust', 'date', 'report', 'factor', 'dependent', 'parameters', 'delay', 'distribution', 'estimation', 'time', 'delay', 'distribution', 'illness', 'onset', 'death', 'account', 'right', 'truncation', 'model', 'lognormal', 'distribution', 'parameters', 'adopt', 'earlier', 'study', 'lognormal', 'distribution', 'parameters', 'cumulative', 'incidence', 'date', 'report', 'adjust', 'time', 'illness', 'onset', 'death', 'report', 'simply', 'multiply', 'factor', 'consequence', 'exponentially', 'grow', 'epidemic', 'factor', 'define', 'follow', 'integral', 'shape', 'inverse', 'scale', 'gamma', 'distribution', 'model', 'distribution', 'time', 'interval', 'illness', 'onset', 'report', 'observe', 'export', 'case', 'cumulative', 'incidence', 'china', 'adjust', 'multiplicative', 'factor', 'number', 'export', 'case', 'sample', 'binomial', 'distribution', 'describe', 'probability', 'find', 'traveler', 'wuhan', 'travelers', 'china', 'subject', 'detection', 'time', 'window', 'virus', 'days', 'give', 'total', 'volume', 'inbound', 'passengers', 'china', 'million', 'passengers', 'year', 'fraction', 'wuhan', 'travelers', 'population', 'wuhan', 'million', 'probability', 'give', 'byfirst', 'fit', 'delay', 'distribution', 'time', 'illness', 'onset', 'report', 'gamma', 'distribution', 'define', 'likelihood', 'follow', 'gamma', 'shape', 'scale', 'maximize', 'mean', 'value', 'parameters', 'follow', 'step', 'second', 'fit', 'observe', 'count', 'export', 'case', 'deaths', 'consider', 'likelihoods', 'respectively', 'process', 'time', 'exportation', 'event', 'death', 'observe', 'total', 'likelihood', 'give', 'likelihoods', 'maximize', 'determine', 'best', 'parameters', 'consider', 'possible', 'scenarios', 'data', 'scenario', 'refer', 'scenario', 'parameter', 'fix', 'date', 'illness', 'onset', 'covid', 'confirm', 'case', 'december', 'provide', 'fix', 'start', 'point', 'exponential', 'growth', 'cumulative', 'incidence', 'december', 'assume', 'start', 'point', 'exponential', 'growth', 'cumulative', 'incidence', 'scenario', 'uncertainty', 'date', 'proxy', 'begin', 'exponential', 'growth', 'inconsistencies', 'report', 'illness', 'onset', 'earliest', 'report', 'case', 'conduct', 'sensitivity', 'analysis', 'start', 'date', 'exponential', 'growth', 'vary', 'december', 'december', 'second', 'scenario', 'refer', 'scenario', 'parameters', 'variable', 'calculation', 'begin', 'date', 'export', 'case', 'observe', 'january', 'scenarios', 'conduct', 'sensitivity', 'analyse', 'estimate', 'growth', 'rate', 'cumulative', 'incidence', 'ccfr', 'value', 'different', 'time', 'value', 'detection', 'window', 'size', 'catchment', 'population', 'wuhan', 'international', 'airport', 'value', 'basic', 'reproduction', 'number', 'covid', 'epidemic', 'calculate', 'aswhere', 'estimate', 'growth', 'rate', 'scenario', 'mean', 'serial', 'interval', 'successive', 'covid', 'infections', 'value', 'serial', 'interval', 'adopt', 'publish', 'study', 'gamma', 'distribution', 'fit', 'know', 'pair', 'infectee', 'infector', 'mean', 'days', 'standard', 'deviation', 'days', 'analysis', 'employ', 'markov', 'chain', 'monte', 'carlo', 'mcmc', 'simulations', 'chain', 'sample', 'sample', 'tune', 'stage', 'leverage', 'turn', 'sampler', 'nut', 'python', 'pymc', 'package', 'input', 'variables', 'binomial', 'distribution', 'model', 'continuous', 'variable', 'approach', 'gamma', 'distribution', 'match', 'moments', 'obtain', 'transformation', 'discrete', 'binomial', 'distribution', 'continuous', 'approximation', 'avoid', 'joint', 'estimation', 'parameters', 'heterogeneity', 'aggregate', 'data', 'similar', 'issue', 'discuss', 'instead', 'implement', 'sequential', 'fit', 'consider', 'likelihood', 'likelihood', 'mean', 'value', 'estimate', 'parameters', 'obtain', 'previous', 'round', 'finally', 'verify', 'obtain', 'calculate', 'pointwise', 'estimate', 'maximum', 'likelihood', 'estimation', 'confidence', 'intervals', 'derive', 'profile', 'likelihood', 'base', 'intervals', 'approach', 'complete', 'agreement', 'result', 'figure', 'mean', 'time', 'illness', 'onset', 'report', 'death', 'respectively', 'employ', 'gamma', 'distribution', 'mean', 'time', 'illness', 'onset', 'report', 'estimate', 'days', 'mean', 'time', 'illness', 'onset', 'death', 'adopt', 'previous', 'study', 'estimate', 'days', 'lognormal', 'distribution', 'location', 'parameter', 'scale', 'parameter', 'international', 'license', 'available', 'author', 'funder', 'grant', 'medrxiv', 'license', 'display', 'preprint', 'perpetuity', 'copyright', 'holder', 'preprint', 'https', 'medrxiv', 'preprint', 'days', 'employ', 'lognormal', 'distribution', 'account', 'right', 'truncation', 'reference', 'estimate', 'mean', 'credible', 'intervals', 'account', 'right', 'truncation', 'show', 'grey', 'value', 'distribution', 'time', 'illness', 'onset', 'death', 'adopt', 'earlier', 'study', 'blue', 'bar', 'empirically', 'observe', 'data', 'collect', 'governmental', 'report', 'january', 'subsequently', 'cumulative', 'incidence', 'estimate', 'export', 'case', 'data', 'fit', 'exponentially', 'grow', 'incidence', 'curve', 'scenarios', 'figure', 'january', 'export', 'case', 'report', 'cumulative', 'incidence', 'china', 'estimate', 'case', 'scenario', 'case', 'scenario', 'table', 'show', 'real', 'time', 'update', 'estimate', 'cumulative', 'incidence', 'exponential', 'growth', 'rat', 'derive', 'growth', 'rate', 'cumulative', 'incidence', 'estimate', 'scenarios', 'respectively', 'estimate', 'ccfr', 'value', 'account', 'time', 'delay', 'illness', 'onset', 'death', 'scenarios', 'respectively', 'total', 'confirm', 'deaths', 'report', 'january', 'ccfr', 'value', 'estimate', 'scenario', 'scenario', 'respectively', 'estimate', 'basic', 'reproduction', 'number', 'covid', 'infection', 'estimate', 'exponential', 'growth', 'account', 'possible', 'variations', 'mean', 'serial', 'interval', 'figure', 'assume', 'mean', 'serial', 'interval', 'days', 'basic', 'reproduction', 'number', 'estimate', 'scenarios', 'respectively', 'mean', 'serial', 'interval', 'vary', 'estimate', 'range', 'scenarios', 'respectively', 'address', 'uncertainty', 'unobserved', 'date', 'illness', 'onset', 'index', 'case', 'scenario', 'ccfr', 'estimate', 'vary', 'start', 'date', 'exponential', 'growth', 'incidence', 'place', 'single', 'index', 'case', 'december', 'figure', 'table', 'assume', 'date', 'illness', 'onset', 'index', 'case', 'december', 'estimate', 'incidence', 'china', 'ccfr', 'january', 'estimate', 'sensitivity', 'analyse', 'vary', 'cutoff', 'date', 'january', 'conduct', 'depend', 'number', 'time', 'point', 'estimate', 'cumulative', 'incidence', 'international', 'license', 'available', 'author', 'funder', 'grant', 'medrxiv', 'license', 'display', 'preprint', 'perpetuity', 'copyright', 'holder', 'preprint', 'https', 'medrxiv', 'preprint', 'ccfr', 'similar', 'value', 'scenario', 'figure', 'slightly', 'decrease', 'scenario', 'cutoff', 'date', 'earlier', 'figure', 'addition', 'consider', 'uncertainty', 'fix', 'parameters', 'detection', 'time', 'window', 'catchment', 'population', 'wuhan', 'airport', 'sensitivity', 'assess', 'vary', 'parameters', 'catchment', 'population', 'increase', 'detection', 'window', 'time', 'decrease', 'estimate', 'ccfr', 'january', 'get', 'smaller', 'increase', 'number', 'incidence', 'china', 'table', 'present', 'study', 'estimate', 'risk', 'death', 'confirm', 'case', 'address', 'ascertainment', 'bias', 'data', 'case', 'diagnose', 'outside', 'mainland', 'china', 'right', 'censor', 'likelihood', 'model', 'count', 'decease', 'case', 'estimate', 'ccfr', 'value', 'date', 'illness', 'onset', 'index', 'case', 'fix', 'priori', 'december', 'scenario', 'time', 'exponential', 'growth', 'epidemic', 'fit', 'data', 'alongside', 'model', 'parameters', 'scenario', 'estimate', 'value', 'scenario', 'adjust', 'time', 'delay', 'illness', 'onset', 'death', 'smaller', 'scenario', 'larger', 'value', 'growth', 'rate', 'incidence', 'estimate', 'ccfr', 'value', 'january', 'larger', 'scenario', 'compare', 'scenario', 'depend', 'available', 'data', 'estimate', 'ccfr', 'delay', 'adjust', 'risk', 'death', 'confirm', 'case', 'vary', 'time', 'fluctuations', 'addition', 'estimate', 'value', 'basic', 'reproduction', 'number', 'range', 'scenario', 'scenario', 'estimate', 'conclude', 'covid', 'substantial', 'potential', 'spread', 'human', 'human', 'transmission', 'guarantee', 'single', 'export', 'untraced', 'case', 'immediately', 'lead', 'major', 'epidemic', 'destination', 'country', 'government', 'responses', 'border', 'control', 'isolation', 'suspect', 'case', 'intensive', 'surveillance', 'serve', 'reduce', 'opportunities', 'transmission', 'occur', 'ccfr', 'estimate', 'indicate', 'severity', 'covid', 'high', 'diseases', 'cause', 'coronaviruses', 'include', 'severe', 'acute', 'respiratory', 'syndrome', 'sars', 'estimate', 'hong', 'kong', 'middle', 'east', 'respiratory', 'syndrome', 'estimate', 'south', 'korea', 'nonetheless', 'consider', 'overall', 'magnitude', 'ongoing', 'epidemic', 'risk', 'death', 'mean', 'insignificant', 'addition', 'quantify', 'overall', 'risk', 'death', 'future', 'research', 'identify', 'group', 'risk', 'death', 'elderly', 'people', 'underlie', 'comorbidities', 'consider', 'infect', 'individuals', 'ascertain', 'report', 'infection', 'fatality', 'risk', 'risk', 'death', 'infect', 'individuals', 'order', 'estimate', 'range', 'covid', 'consistent', 'preliminary', 'estimate', 'post', 'public', 'domains', 'comparable', 'sars', 'range', 'outbreak', 'singapore', 'estimate', 'scenario', 'yield', 'greater', 'value', 'increasingly', 'improve', 'ascertainment', 'early', 'january', 'virus', 'identify', 'sequence', 'january', 'subsequently', 'primer', 'widely', 'distribute', 'allow', 'rapid', 'laboratory', 'identification', 'case', 'contribute', 'time', 'dependent', 'increase', 'number', 'confirm', 'case', 'china', 'consequently', 'scenario', 'fully', 'dependent', 'growth', 'rate', 'export', 'case', 'overestimate', 'intrinsic', 'growth', 'rate', 'case', 'consider', 'estimate', 'value', 'possibility', 'presymptomatic', 'transmission', 'ongoing', 'epidemic', 'critical', 'question', 'substantial', 'impact', 'public', 'health', 'response', 'epidemic', 'contact', 'trace', 'prioritize', 'overall', 'predictability', 'epidemic', 'containment', 'stage', 'technical', 'emphasize', 'propose', 'approach', 'especially', 'useful', 'early', 'stage', 'epidemic', 'local', 'surveillance', 'affect', 'substantial', 'ascertainment', 'bias', 'export', 'death', 'data', 'available', 'better', 'ascertain', 'nonetheless', 'caution', 'implement', 'similar', 'estimations', 'covid', 'epidemic', 'flight', 'wuhan', 'airport', 'ground', 'january', 'intervention', 'abruptly', 'change', 'human', 'migration', 'network', 'despite', 'decrease', 'outbound', 'flow', 'travelers', 'wuhan', 'substantial', 'risk', 'epidemic', 'wave', 'originate', 'cities', 'main', 'limitations', 'present', 'study', 'result', 'present', 'estimate', 'ccfr', 'address', 'fatality', 'confirm', 'case', 'precise', 'estimate', 'include', 'infect', 'individuals', 'confirm', 'case', 'estimate', 'additional', 'piece', 'data', 'data', 'outpatient', 'clinic', 'visit', 'note', 'denominator', 'numerator', 'value', 'subject', 'better', 'estimation', 'excess', 'mortality', 'estimate', 'second', 'study', 'rely', 'limit', 'empirical', 'data', 'extract', 'publicly', 'available', 'data', 'source', 'future', 'study', 'greater', 'sample', 'size', 'precision', 'need', 'nonetheless', 'believe', 'study', 'improve', 'situational', 'assessment', 'ongoing', 'epidemic', 'assume', 'date', 'illness', 'onset', 'index', 'case', 'scenario', 'base', 'initial', 'report', 'earliest', 'onset', 'date', 'case', 'continue', 'exponential', 'growth', 'rate', 'author', 'extrapolation', 'conduct', 'sensitivity', 'analysis', 'ensure', 'result', 'statistical', 'estimate', 'greatly', 'vary', 'main', 'result', 'fourth', 'uncertainty', 'detection', 'window', 'time', 'epidemiological', 'investigations', 'actively', 'implement', 'outside', 'china', 'believe', 'incubation', 'period', 'infectious', 'period', 'plausible', 'estimation', 'detection', 'time', 'window', 'fifth', 'heterogeneous', 'aspects', 'death', 'risk', 'group', 'need', 'address', 'future', 'study', 'conclusion', 'present', 'study', 'estimate', 'ccfr', 'order', 'endorse', 'notion', 'covid', 'infection', 'ongoing', 'epidemic', 'possess', 'potential', 'pandemic', 'propose', 'approach', 'help', 'direct', 'risk', 'assessment', 'settings', 'publicly', 'available', 'datasets', 'follow', 'available', 'online', 'mdpi', 'figure', 'sensitivity', 'analysis', 'vary', 'start', 'point', 'exponential', 'growth', 'cumulative', 'incidence', 'december', 'figure', 'sensitivity', 'analysis', 'vary', 'cutoff', 'date', 'january', 'estimation', 'scenario', 'figure', 'sensitivity', 'analysis', 'vary', 'cutoff', 'date', 'january', 'estimation', 'scenario', 'table', 'sensitivity', 'analysis', 'vary', 'start', 'point', 'exponential', 'growth', 'cumulative', 'incidence', 'december', 'estimation', 'scenario', 'table', 'sensitivity', 'analysis', 'vary', 'catchment', 'population', 'size', 'wuhan', 'airport', 'table', 'international', 'license', 'available', 'author', 'funder', 'grant', 'medrxiv', 'license', 'display', 'preprint', 'perpetuity', 'copyright', 'holder', 'preprint', 'https', 'medrxiv', 'preprint', 'sensitivity', 'analyse', 'vary', 'data', 'cutoff', 'date', 'january', 'january', 'present', 'study', 'rely', 'real', 'time', 'data', 'estimate', 'growth', 'rate', 'cumulative', 'incidence', 'ccfr', 'different', 'number', 'data', 'cutoff', 'date', 'show', 'figure', 'scenario', 'scenario', 'figure', 'sensitivity', 'analysis', 'scenario', 'vary', 'data', 'cutoff', 'date', 'january', 'estimate', 'value', 'exponential', 'growth', 'rate', 'cumulative', 'incidence', 'confirm', 'case', 'fatality', 'risk', 'depend', 'data', 'cutoff', 'date', 'point', 'estimation', 'line', 'shade', 'area', 'indicate', 'estimate', 'confidence', 'interval', 'international', 'license', 'available', 'author', 'funder', 'grant', 'medrxiv', 'license', 'display', 'preprint', 'perpetuity', 'peer', 'review', 'copyright', 'holder', 'preprint', 'figure', 'sensitivity', 'analysis', 'scenario', 'vary', 'data', 'cutoff', 'date', 'january', 'estimate', 'value', 'exponential', 'growth', 'rate', 'cumulative', 'incidence', 'confirm', 'case', 'fatality', 'risk', 'depend', 'data', 'cutoff', 'date', 'point', 'estimation', 'line', 'shade', 'area', 'indicate', 'estimate', 'confidence', 'interval', 'international', 'license', 'available', 'author', 'funder', 'grant', 'medrxiv', 'license', 'display', 'preprint', 'perpetuity', 'peer', 'review', 'copyright', 'holder', 'preprint', 'https', 'medrxiv', 'preprintin', 'present', 'study', 'detection', 'time', 'window', 'fix', 'days', 'base', 'assume', 'value', 'incubation', 'infectious', 'periods', 'catchment', 'population', 'wuhan', 'airport', 'fix', 'million', 'sensitivity', 'analysis', 'table', 'show', 'estimate', 'growth', 'rate', 'cumulative', 'incidence', 'ccfr', 'vary', 'detection', 'time', 'window', 'table', 'show', 'estimate', 'different', 'catchment', 'population', 'size', 'fix', 'detection', 'time', 'window', 'scenarios', 'respectively']\n"
     ]
    }
   ],
   "source": [
    "#Lets test with this document with index 1! As we can see this paper cleary is talking about Covid and Coronavirus,\n",
    "#so we except thta the topic should be about that :) \n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So here we test LDA with a document with index 1 ( to be more precise, with TF-IDF representation of document with index 1) we can se distribution between topics. This papers topic is 92 percent Topic 0, and all the rest are around 1.7%. Meaning that the main topic of this paper is something about patients, covid and wuhan. This will give us a clue, on which papers are actually talking about this pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9284133),\n",
       " (1, 0.017921286),\n",
       " (2, 0.017866932),\n",
       " (3, 0.017814443),\n",
       " (4, 0.017984083)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus_tfidf[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
