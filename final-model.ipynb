{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to COVID-19 Information Retrieval Engine \n",
    "                                                                                \n",
    "### Project is part of Text analysis and Retrieval, FER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Preprocessing of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This part can be replaced by different method of parsing, just be sure to finish with a same data structure as we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import our own Parser class that will parse dataset based on the source.\n",
    "from Parser import *\n",
    "\n",
    "parser = Parser([Dataset.TEST])\n",
    "parser.parse(indexByFile = False)\n",
    "papers = parser.data_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to combine all papers in one dictonary... We do this because we do want to test on all papers, but just by tweaking this little bit, you can determine which dataset you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = {}\n",
    "\n",
    "current_id = 0\n",
    "for dataset in papers: #<--- change this in order to change which dataset you want in dictonary\n",
    "    for paper_id in papers[dataset]:\n",
    "        all_papers[current_id] = papers[dataset][paper_id]\n",
    "        current_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b) Importing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is known about Covid-19 transmission?', 'What is known about Covid-19 incubation?', 'What is known about Covid-19 environmental stability?']\n"
     ]
    }
   ],
   "source": [
    "from TaskQuery import *\n",
    "\n",
    "#We are importing all the queries here.\n",
    "queries = TaskQuery.questions()\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Here enter the Query for which you want the end result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is known about Covid-19 incubation?\n"
     ]
    }
   ],
   "source": [
    "QUERY = queries[1]\n",
    "print(QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) BioNER filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are going to filter the dataset. We are going to take a query and extract keywords from it. Then, we will extract keywords from each paper and compare query keywords from paper keywords. Those papers that do not match with query keywords will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "#download this https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz and put\n",
    "#in the data folder\n",
    "bioNER = spacy.load(\"./data/en_core_sci_sm\")\n",
    "covid_keywords = [\"covid19\",\"sars-cov-2\",\"covid-19\",\"coronavirus\",\"sars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Covid-19, incubation)\n"
     ]
    }
   ],
   "source": [
    "#Getting keywords for Query\n",
    "query_NER = bioNER(QUERY)\n",
    "print(query_NER.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.6521125659566156, 70), (1.9290890882359208, 14), (1.926678550207962, 44), (1.8618975741239894, 50), (1.8057273768613975, 47), (0.12185451231755348, 25), (0.07731321784617717, 72), (0.04284795472086442, 85), (0.03071658615136876, 29), (0.030021406953569058, 20), (0.024999999999999998, 67), (0.0091991341991342, 57), (0.007692307692307693, 9), (0.00646551724137931, 1), (0.0046816479400749065, 84), (0.004048582995951417, 6), (0.0034965034965034965, 2), (0.003048780487804878, 56), (0.002621231979030144, 94), (0.002250562640660165, 11), (0.0020304568527918783, 39), (0.00130718954248366, 65), (0.0012804097311139564, 22), (0.0010672358591248667, 86), (0.0010416666666666667, 60), (0.0009900990099009901, 90), (0.0009082652134423251, 51), (0.0008312551953449709, 40), (0.0007867820613690008, 32), (0.0007407407407407407, 53), (0.0006646726487205051, 4), (0.0006596306068601583, 18), (0.0005494505494505495, 68), (0.0005420054200542005, 58), (0.00043243243243243243, 59), (0.0002924831822170225, 37), (0.0, 93), (0.0, 92), (0.0, 91), (0.0, 89), (0.0, 88), (0.0, 87), (0.0, 83), (0.0, 82), (0.0, 81), (0.0, 80), (0.0, 79), (0.0, 78), (0.0, 77), (0.0, 76), (0.0, 75), (0.0, 74), (0.0, 73), (0.0, 71), (0.0, 69), (0.0, 66), (0.0, 64), (0.0, 63), (0.0, 62), (0.0, 61), (0.0, 55), (0.0, 54), (0.0, 52), (0.0, 49), (0.0, 48), (0.0, 46), (0.0, 45), (0.0, 43), (0.0, 42), (0.0, 41), (0.0, 38), (0.0, 36), (0.0, 35), (0.0, 34), (0.0, 33), (0.0, 31), (0.0, 30), (0.0, 28), (0.0, 27), (0.0, 26), (0.0, 24), (0.0, 23), (0.0, 21), (0.0, 19), (0.0, 17), (0.0, 16), (0.0, 15), (0.0, 13), (0.0, 12), (0.0, 10), (0.0, 8), (0.0, 7), (0.0, 5), (0.0, 3), (0.0, 0)]\n"
     ]
    }
   ],
   "source": [
    "#now we are going through all the documents and parsing them to get the important BIO terms like - coronavirus etc.\n",
    "#then we are going to count the number of times some word from query has appered in doc, and divide that counter with \n",
    "#number of words in document. That will give us a probability score that we can use.\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filtered_results = {}\n",
    "alpha = 1 #parameter by which we are determining how much are we valuing COVID19 terms in body\n",
    "beta = 1.8 #parametar by which we are determining how much are we valuing search terms in title\n",
    "gamma = 1.2 #paramtar by which we are determining how much are we valuing search terms in abstract\n",
    "\n",
    "NER_scores = []\n",
    "if(len(set(query_NER.ents)) != 0):\n",
    "    for i in range(len(all_papers)):\n",
    "        paper = all_papers[i]\n",
    "        document = bioNER(paper.body)\n",
    "        title = bioNER(paper.title)\n",
    "        abstract = bioNER(paper.abstract)\n",
    "        \n",
    "        query_counter = 0\n",
    "        for query in set(query_NER.ents):\n",
    "            counter = 0\n",
    "            for keyword in set(document.ents):\n",
    "                if(lemmatizer.lemmatize(keyword.text.lower()) == lemmatizer.lemmatize(query.text.lower())):\n",
    "                    counter += 1\n",
    "                if(keyword.text.lower() in covid_keywords):\n",
    "                    counter *= alpha\n",
    "            counter = counter/len(set(document.ents))\n",
    "            query_counter += counter\n",
    "            \n",
    "        \n",
    "        title_counter = 0\n",
    "        for query in set(query_NER.ents):\n",
    "            for keyword in set(title.ents):\n",
    "                if(lemmatizer.lemmatize(keyword.text.lower()) == lemmatizer.lemmatize(query.text.lower())):\n",
    "                    title_counter += 1 \n",
    "        title_counter *= beta\n",
    "        \n",
    "        abstract_counter = 0\n",
    "        for query in set(query_NER.ents):\n",
    "            counter = 0\n",
    "            for keyword in set(abstract.ents):\n",
    "                if(lemmatizer.lemmatize(keyword.text.lower()) == lemmatizer.lemmatize(query.text.lower())):\n",
    "                    counter += 1\n",
    "                if(keyword.text.lower() in covid_keywords):\n",
    "                    counter *= gamma\n",
    "            if(len(set(abstract.ents)) != 0):\n",
    "                counter = counter/len(set(abstract.ents))\n",
    "            else:\n",
    "                counter = 0\n",
    "            abstract_counter += counter\n",
    "        \n",
    "        \n",
    "        query_counter = (query_counter+title_counter+abstract_counter)/len(set(query.ents))\n",
    "        NER_scores.append((query_counter,i))\n",
    "    NER_scores.sort(reverse = True)\n",
    "    print(NER_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have filtered and removed all the papers that have score zero.\n",
    "for score in NER_scores:\n",
    "    if(score[0] != 0.0):\n",
    "        filtered_results[all_papers[score[1]]] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "#User manual\n",
    "#----------------------------------\n",
    "#Install --> pip3 install gensim (apart from gensim, you will need numpy)\n",
    "#Download word2vec file -->  https://code.google.com/archive/p/word2vec/\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we initialize word2vec with already pretrained vectors\n",
    "word2vec = word2vec.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'known', 'about', 'Covid-19', 'incubation', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "query = nltk.word_tokenize(QUERY)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_that_are_not_in_word2vec = [\"covid19\",\"sars-cov-2\",\"covid-19\"]\n",
    "\n",
    "query_vector = np.zeros(300)\n",
    "for word in query:\n",
    "    if(word in word2vec.vocab):\n",
    "        query_vector += word2vec[word]\n",
    "    if(word.lower() in keywords_that_are_not_in_word2vec):\n",
    "        query_vector += word2vec[\"coronavirus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.7745787633280168, 70), (1.1651454139411335, 44), (0.9572473053210789, 14), (0.8919523639696857, 50), (0.777466692794606, 47), (0.09882556503606972, 25), (0.01955334546586607, 72), (0.018348460813301815, 85), (0.012520233950979263, 29), (0.010303128458724269, 20), (0.004333155666635088, 57), (0.004299592925933213, 9), (0.003738136730696603, 67), (0.0032602240579680205, 1), (0.0021496746601150727, 84), (0.0018462848262183202, 2), (0.001759438590582564, 6), (nan, 94), (0.0014688405951929077, 11), (0.0012089852186225373, 56), (0.0009545882658005207, 39), (0.0007264972096762861, 22), (0.0007009531331236247, 65), (0.000624956868539344, 60), (0.0005656798916535547, 86), (0.0005506408658853582, 90), (0.0004825135162031871, 40), (0.00048139173758721566, 51), (nan, 53), (0.00037243974984641637, 18), (0.0003665547910325716, 32), (0.0003558975120049752, 4), (0.00034362642169754183, 58), (0.00021625522895253865, 68), (0.00019026997850283298, 59), (0.00015771814518768338, 37)]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "W2V_scores = []\n",
    "for paper in filtered_results:\n",
    "    paper_vector = np.zeros(300)\n",
    "    title_vector = np.zeros(300)\n",
    "    tokens = bioNER(paper.whole_text)\n",
    "    title_tokens = bioNER(paper.title)\n",
    "    \n",
    "    for token in set(tokens.ents):\n",
    "        if(token.text in word2vec.vocab):\n",
    "            if(token.text in query):\n",
    "                paper_vector += word2vec[token.text]*alpha\n",
    "            else:\n",
    "                paper_vector += word2vec[token.text]\n",
    "        \n",
    "    for token in set(title_tokens.ents):\n",
    "        if(token.text in word2vec.vocab):\n",
    "            if(token.text in query):\n",
    "                title_vector += word2vec[token.text]*beta\n",
    "            else:\n",
    "                title_vector += word2vec[token.text]\n",
    "        \n",
    "    cos_sim_paper = np.inner(query_vector,paper_vector)/(norm(query_vector)*norm(paper_vector))\n",
    "    cos_sim_title = np.inner(query_vector,title_vector)/(norm(query_vector)*norm(title_vector))\n",
    "    \n",
    "    cos_sim = (cos_sim_paper+cos_sim_title*beta)/2\n",
    "    \n",
    "    score = filtered_results[paper]\n",
    "    w2v_score = (score[0]*cos_sim,score[1])\n",
    "    \n",
    "    filtered_results[paper] = w2v_score\n",
    "    W2V_scores.append(w2v_score)\n",
    "W2V_scores.sort(reverse = True)\n",
    "print(W2V_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "Estimating the Relative Probability of Direct Transmission between Infectious Disease Patients\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "Estimating infectious disease parameters such as the serial interval (time between symptom onset in primary and secondary cases) and reproductive number (average number of secondary cases produced by a primary case) are important to understand infectious disease dynamics. Many estimation methods require linking cases by direct transmission, a difficult task for most diseases.Using a subset of cases with detailed genetic or contact investigation data to develop a training set of probable transmission events, we build a model to estimate the relative transmission probability for all case-pairs from demographic, spatial and clinical data. Our method is based on naive Bayes, a machine learning classification algorithm which uses the observed frequencies in the training dataset to estimate the probability that a pair is linked given a set of covariates.In simulations we find that the probabilities estimated using genetic distance between cases to define training transmission events are able to distinguish between truly linked and unlinked pairs with high accuracy (area under the receiver operating curve value of 95%). Additionally only a subset of the cases, 10-50% depending on sample size, need to have detailed genetic data for our method to perform well. We show how these probabilities can be used to estimate the average effective reproductive number and apply our method to a tuberculosis outbreak in Hamburg, Germany.All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint 3 Our method is a novel way to infer transmission dynamics in any dataset when only a subset of cases has rich contact investigation and/or genetic data.\n",
      "\n",
      "\u001b[1mBody\u001b[0m\n",
      "Infectious disease parameters such as the serial interval (time between symptom onset from primary to secondary case) and the reproductive number (average number of secondary cases produced by a primary case over the infection course) are instrumental in managing outbreaks (1) . For diseases in which disease progression shortly follows infection, these parameters have been studied extensively (1) (2) (3) (4) (5) (6) . For others, such as tuberculosis (TB), the serial interval and reproductive number estimates are few and inconsistent (7) (8) (9) .Methods to estimate both the serial interval and reproductive number often rely on determining which cases are linked by direct transmission. Pathogen whole genome sequence (WGS) data is a powerful tool to link cases and several methods have been developed to analyze these data (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) . However, WGS data is still relatively expensive and requires bioinformatics expertise, making universal use in high disease burden settings unfeasible and many datasets are likely to have WGS data on only a proportion of cases. Another way to link cases is contact investigations, which are often part of the infectious disease outbreak response (22) (23) (24) (25) (26) (27) (28) .However, these often do not perfectly identify due to nonspecific transmission mechanisms, disease characteristics, and the willingness and ability of cases to share information about contacts (25, (28) (29) (30) (31) (32) . In addition, contact investigations are time consuming and require significant human resources, again meaning that this data source is unlikely to be available for all cases.Here, we present a novel method to predict the relative probability of direct transmission between infectious disease patients using pathogen WGS data and/or contact investigations when All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint these data are only available on a proportion of cases, paired with other risk factor data. These probabilities can be used to understand outbreak transmission dynamics and estimate the reproductive number without a reliable serial interval estimate. We apply our method to a TB outbreak in Hamburg, Germany.Our method requires individual-level case data, e.g. geographic location, clinical information, demographics, and sampling date. At least a subset of the cases needs additional information, e.g. detailed contact investigation and/or pathogen genome WGS data, to form the training set to generate the model. We transform this dataset of individuals into a dataset of ordered case-pairs ( , ), where case was observed before case . We convert the individual-level covariates (X 1 , X 2 , … , X p ) into pair-level covariates ( 1 , 2 , … , ) by computing \"distances\" which capture how well the individuals match on covariates values. For example, if the individual-level covariate 1 was town of residence, the pair-level covariate 1 could indicate if the individuals live in the same town, neighboring towns, or more distant towns. This process is described further in the supplementary material.To estimate the probability that cases and are linked by direct transmission, ( → ), we use a classification technique called naive Bayes. This method uses Bayes rule to estimate the probability of an outcome given a set of covariates from the observed frequencies in a training dataset. Our outcome, equals 1 if case infected case and 0 otherwise. We know the 6 probable value of for case-pairs in the training set based on pathogen WGS or contact investigation data and want to predict the probability that = 1 for the remaining pairs.We first use the training set to calculate ( = | = ), the probability that the pair-level covariate equals for each covariate ∈ {1, 2, … , } for a pair with link status ∈ {1, 0}, using: can result in zero probabilities, we add 1 to each cell (33) . Then we use the training set to calculate ( = ), the prior probability of link status for ∈ {1, 0} using:We use Bayes rule to calculate the predicted probability that case infected case , ( → ) for all pairs in the prediction set as:( → ) = P(L ij = 1|Z 1ij = z 1 , … , Z pij = z p ) = P(Z 1 = z 1 , … , Z p = z p |L = 1)P(L = 1) P(Z 1 = z 1 , … , Z p = z p )We calculate the conditional probability of all covariate values given link status P(Z 1 = z 1 , … , Z p = z p |L = 1), as the product of the conditional probabilities of each covariate, P(Z k =All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint z k |L = 1) for ∈ {1, 2, … , }, assuming that covariates are conditionally independent (i.e. that the covariate values are independent given link status).Finally, the estimated probabilities are scaled to represent the relative likelihood that case has been infected by case rather than any other sampled case using:We call this scaled probability, ( → ) , the \"relative transmission probability\". Note: the ordered nature of the pair dataset implies that if case was observed before case , then ( → ) = 0.Naive Bayes uses a training set, with a known outcome, to inform a model to estimate probabilities in a separate prediction set. For many infectious diseases we never know the true infector. In our training set, the outcome therefore represents probable rather than certain transmission events, inferred from a subset of cases with pathogen WGS and/or detailed contact investigation data. Because of this uncertainty, we want to estimate the transmission probability of the training case-pairs as well as those which lack WGS or contact data and are only included in the prediction set. Therefore, we use an iterative estimation procedure where each pair has a turn in the prediction set. The algorithm used to create the training dataset and iterative estimation procedure is shown in Figure 1 and described further in the supplementary material.All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprintTo estimate the reproductive number, we use the Wallinga and Teunis approach (4) that calculates the relative probability that each case was infected by all other cases using the serial interval distribution. The effective reproductive number ( ) for each case is then calculated by summing the scaled probabilities for all possible infectees assuming that all cases are sampled and the outbreak is completed with:We use this equation, but with probabilities derived from our naive Bayes approach instead of probabilities based on the serial interval.By averaging the individual reproductive numbers for all cases observed each month, we obtain monthly effective reproductive number ( ) estimates and average those values for the stable portion of the outbreak to give an average reproductive number estimate over the study period ( ̅ ). We calculate confidence intervals for and ̅ using parametric bootstrapping (see supplementary material).We assess our method by applying it to 1000 simulated outbreaks of at least 500 cases, composed of multiple transmission chains using TB transmission dynamics (11, 34) . We simulate each transmission chain with an ̅ of 1.2 and a gamma distributed serial interval (shape=1.05, All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprintWe compare our method performance when training the model using probable transmission events defined by SNP distances, with training using truly linked and unlinked case-pairs. We also compare the method performance with that of probabilities derived from the time between infection dates and the serial interval distribution motivated by the Wallinga and Teunis method (4). For each simulation scenario (Table 1) , we calculate the area under the receiver operating curve (AUC) and assess how the probability of the true infector rank compared to the probabilities of all possible infectors. We also estimate and ̅ . To determine what proportion of cases needs to be in the training set to achieve good performance, we use a sensitivity analysis with various outbreak sizes and training proportions. The simulation structure is explained further in the supplementary material. Table 1 HereWe apply our method to a small TB outbreak in Hamburg and Schleswig-Holstein, Germany, analyzed in Roetzer et al. (10) . The outbreak includes 86 individuals from the largest strain cluster in a long-term surveillance study conducted by the health departments in these cities. The dataset includes pathogen WGS data for all individuals as well as clinical, demographic, and social risk factor data. Furthermore, a subset of these individuals was involved in contact investigations performed by the local health authorities.We define probable links in the training set in two ways: 1) SNP distances and 2) contact investigation. When training with SNP distance, case-pairs with <2 SNPs are considered linked, All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint those with >12 SNPs are considered unlinked. Pairs with 2-12 SNPs are excluded from the training set as indeterminate (11, 34) . When using contact investigation data, pairs that had confirmed contact with each other are considered linked, pairs without confirmed contact with each other are considered unlinked, and cases who did not undergo contact investigation are excluded. For comparison, we also calculate the relative transmission probabilities randomly and using the same serial intervals as the simulation study.The sample sizes of the 1000 simulated outbreaks were 500-1178 (median: 545) and each outbreak had 2-39 (median: 14) individual transmission chains with 2-846 (median: 9) cases each. Supplementary Figure S1 shows the relative transmission probability distributions for one outbreak comparing truly linked and unlinked case-pairs. In that outbreak, our method estimated relative transmission probabilities of <0.005 for most unlinked pairs (92% when training with the truth and 89% training using SNP distance). With both ways of defining the training set, our method assigned more than 75% of truly linked case-pairs higher probabilities than the serial interval method (Supplementary Figure S2) .Over 1000 simulations, the average AUC was 97% (standard deviation [SD] 0.6) compared to 95% (SD 1.2) when the model was trained using true links and SNP distances respectively Table S1 ). When the model was trained with links determined by SNP distances, the estimated probability of the true infector ranked in the top 25% of all possible All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint source case probabilities 93% (SD 2.4) of the time (compared to 95% (SD 1.6) when training with true links). Our method outperformed probabilities estimated using serial intervals ( Figure   2 , Supplementary Table S1). Figure 3 and Supplementary Table S2 show the ̅ estimates for each of the different scenarios compared to the 1.2 value used to simulate the outbreaks. Both our method and the correct serial interval estimated ̅ accurately. However, when incorrect serial intervals were used, the ̅ estimates were either too high or too low.In our sensitivity analysis, the performance improved and the metrics' variability decreased as the proportion of cases in the training set increased (Supplementary Figure S3 ). If the sample size was at least 500, only 10% of all cases were needed to train the model to obtain good performance. For a sample size of 200-500, training the model with 20% of cases resulted in good performance. For smaller outbreaks, the performance was best with at least 50% of the cases in the training set ( Figure 4 ). The ̅ estimates grew increasingly accurate as the training dataset proportion increased and when 30% of cases were included, the estimates were close to the true value (Supplementary Figure S4) .Case counts over the course of the outbreak and clinical and demographic characteristics are shown in Figure 5 and Table 2 . The 86 cases resulted in 3633 possible ordered case-pairs where the possible infector was observed before the infectee. These pairs were separated by 0-20 SNPs (median=4). Of the 86 individuals, 31 (36%) were part of contact investigations and 51 casepairs had a confirmed contact. All individual-level covariates were transformed into pair-level covariates for analysis (Table 3) .All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint Tables 2 and 3 here Figure 6 shows heatmaps of all potential infectors for each infectee using our method compared to random probabilities ( Figure 6A ) and a serial interval distribution ( Figure 6B ). Using our method, defining links with either with SNP distance ( Figure 6C ) or confirmed contact ( Figure   6D ), there is more variation in the relative transmission probability across possible infectors than the serial interval or random scenarios. Some infectees have infectors with a higher probability than all others in the row, suggesting this is the likely true infector. However, even for rows without a clear single infector, many of the possible infectors have very low probabilities and can be eliminated as the true infector.All methods except random probabilities show spikes in at the second peak in case counts, but to different degrees (Figure 7) Figure 8 ).We have developed a method to estimate the relative transmission probability between pairs of infectious disease cases using clinical, demographic, geographic, and genetic characteristics that accurately distinguishes between linked and unlinked case-pairs in simulation studies. Using a SNP distance proxy for transmission to train the model, the classification accuracy was 95%, and 93% of the time the true infector had a probability in the top 25% of all possible infectors. Our All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint method outperformed the serial interval method in all metrics and accurately estimated ̅ . This is important because the serial interval is difficult to estimate and highly variable (7, 8, 35) , highlighting the value of estimation methods that are independent of serial interval estimates.Applying these methods to the Hamburg dataset, we found that both methods allowed for the elimination of many transmission links ( Figure 6 ). The two ways of model training produced slightly different ̅ estimates, which is expected because neither of the different probable transmission events used to train the model perfectly capture the truth. Using contact investigation for training is more discriminating than SNP distance because we know the cases have actually interacted. However, this may miss links with unknown or unreported contacts.Using SNP distances for training will result in fewer missed links, but could connect cases that never had contact with one another. We hypothesize that the true reproductive number for M. tuberculosis in this context lies in between these two estimates (0.84-0.97).Most established methods for exploring transmission focus on either identifying recent transmission clusters (11, 16, 34, (36) (37) (38) (39) , recreating possible transmission chains (12, 14, 15, (17) (18) (19) (20) (21) (40) (41) (42) , or identifying the true infector (43) (44) (45) (46) (47) . When estimating transmission parameters, simply knowing clusters is not informative enough and identifying the true infector is often impossible in reality. The strength of our method is that it directly estimates the relative transmission probability for all case-pairs instead of seeking to find the true infector or a set of possible transmission trees. This gives our method broad applicability as it can identify potential true infectors (pairs with very high probabilities) or transmission clusters (groups of pairs with All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint high probabilities). These probabilities can then be used to estimate transmission parameters incorporating the uncertainty around the true infector.Other methods to estimate the transmission probabilities either only use genetic data (13) or require prior knowledge of the relationship between the covariates and transmission (48) . Our method, however, uses many different information sources about the individual cases without assuming any relationship between these factors and transmission. Additionally, our method uses naive Bayes, a simple but powerful machine learning tool that has many diverse applications (33, (49) (50) (51) (52) . Although traditionally a naive Bayes model is trained with a set of true events, our method performs almost as well when SNP distance is used as a transmission proxy.Having both a training and prediction set also means that not all cases require highly discriminatory information such as contact investigation or pathogen WGS data to estimate relative transmission probabilities. This is relevant because existing datasets often have rich demographic, clinical, and spatial data but lack detailed contact investigation or pathogen WGS data due to significant time and resources needed to obtain these data. Provided a subset of cases, 10-50% depending on the sample size, has this information, our method can infer transmission patterns among the remaining cases as well.Our method does make assumptions that might not be appropriate. Firstly, naive Bayes assumes independence of the covariates when conditioning on the outcome, which may not be realistic.However, numerous papers have shown that naive Bayes still performs well even when this assumption is violated (51, (53) (54) (55) . Furthermore, many naive Bayes extensions have been All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint developed that relax this assumption (33, 56, 57) , which could be easily integrated into our method.The Wallinga and Teunis (4) approach for estimating ̅̅̅ , which we applied, assumes that every case was infected by someone that has been sampled. These authors and others found that simulations incorporating random incomplete reporting did not substantially decrease the accuracy of their ̅ estimates, so this is unlikely to be an issue here (4, 58) . Our probability estimates themselves do not assume all cases in an outbreak are sampled because we estimate the relative probability that one case was infected by another over any other sampled case. However, our method could be affected by biased sampling, e.g. because only certain types of cases are observed or have the information needed to define probable links in the training set. Future work could more fully examine the effect of biased reporting and biased training sets.Finally, as with other infectious disease analytical approaches, our method assumes that cases were infected in the same order that they were observed (39, 59) . Although not a strong assumption for diseases with clear symptoms and a short latent period, this may not be appropriate for diseases such as TB, with a highly variable and potentially long latent period, and often substantial delays in care-seeking care and diagnosis (60, 61) . Although this assumption is a known problem in infectious disease research, it is frequently made (43, 44) because relaxing it complicates models substantially.We have developed a method to estimate the relative transmission probabilities between pairs of cases, which is flexible, using any information sources available without making assumptions All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint about the relationship between these covariates and transmission. The power of our method is that only a subset of cases requires pathogen WGS or contact investigation data, making this method applicable to many outbreak and surveillance datasets. These probabilities can be used to better understand the transmission dynamics of an outbreak by identifying or ruling out possible transmission events and estimating transmission parameters. In a disease where determining transmission events can be extremely difficult, using transmission probabilities between all possible cases provides a unique and powerful analysis tool.All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint method and a training set with links based on SNP distance; probabilities derived from narrow: gamma(0.54, 1.9), medium: gamma(1.05, 2.0), and wide: gamma(1.33, 3.0) serial interval distributions; and random probabilities. The vertical bars represent 95% bootstrap confidence intervals. The dotted horizontal line represents a ̅ value of 1.All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint Figure 1 All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint Figure 2 All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint Figure 5 All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint Figure 8 All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/612945 doi: bioRxiv preprint\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(all_papers[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.) Doc2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "for paper in filtered_results:\n",
    "    score = filtered_results[paper]\n",
    "    \n",
    "    tokens = [token.text for token in bioNER(paper.title.lower()).ents]\n",
    "    doc = TaggedDocument(words = tokens, tags = [str(score[1])])\n",
    "    tagged_data.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the training part. Here we are making our own word embeddings. That means we are basically going to make our own word2vec. In other words, for every word from our dataset our model will make a vector in 20 dimensional space. Furthermore, every vectors will be similar if the words they are representing are similar. E.g. vectors for word coronavirus and covid19 will be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "iteration 100\n",
      "iteration 101\n",
      "iteration 102\n",
      "iteration 103\n",
      "iteration 104\n",
      "iteration 105\n",
      "iteration 106\n",
      "iteration 107\n",
      "iteration 108\n",
      "iteration 109\n",
      "iteration 110\n",
      "iteration 111\n",
      "iteration 112\n",
      "iteration 113\n",
      "iteration 114\n",
      "iteration 115\n",
      "iteration 116\n",
      "iteration 117\n",
      "iteration 118\n",
      "iteration 119\n",
      "iteration 120\n",
      "iteration 121\n",
      "iteration 122\n",
      "iteration 123\n",
      "iteration 124\n",
      "iteration 125\n",
      "iteration 126\n",
      "iteration 127\n",
      "iteration 128\n",
      "iteration 129\n",
      "iteration 130\n",
      "iteration 131\n",
      "iteration 132\n",
      "iteration 133\n",
      "iteration 134\n",
      "iteration 135\n",
      "iteration 136\n",
      "iteration 137\n",
      "iteration 138\n",
      "iteration 139\n",
      "iteration 140\n",
      "iteration 141\n",
      "iteration 142\n",
      "iteration 143\n",
      "iteration 144\n",
      "iteration 145\n",
      "iteration 146\n",
      "iteration 147\n",
      "iteration 148\n",
      "iteration 149\n",
      "iteration 150\n",
      "iteration 151\n",
      "iteration 152\n",
      "iteration 153\n",
      "iteration 154\n",
      "iteration 155\n",
      "iteration 156\n",
      "iteration 157\n",
      "iteration 158\n",
      "iteration 159\n",
      "iteration 160\n",
      "iteration 161\n",
      "iteration 162\n",
      "iteration 163\n",
      "iteration 164\n",
      "iteration 165\n",
      "iteration 166\n",
      "iteration 167\n",
      "iteration 168\n",
      "iteration 169\n",
      "iteration 170\n",
      "iteration 171\n",
      "iteration 172\n",
      "iteration 173\n",
      "iteration 174\n",
      "iteration 175\n",
      "iteration 176\n",
      "iteration 177\n",
      "iteration 178\n",
      "iteration 179\n",
      "iteration 180\n",
      "iteration 181\n",
      "iteration 182\n",
      "iteration 183\n",
      "iteration 184\n",
      "iteration 185\n",
      "iteration 186\n",
      "iteration 187\n",
      "iteration 188\n",
      "iteration 189\n",
      "iteration 190\n",
      "iteration 191\n",
      "iteration 192\n",
      "iteration 193\n",
      "iteration 194\n",
      "iteration 195\n",
      "iteration 196\n",
      "iteration 197\n",
      "iteration 198\n",
      "iteration 199\n",
      "iteration 200\n",
      "iteration 201\n",
      "iteration 202\n",
      "iteration 203\n",
      "iteration 204\n",
      "iteration 205\n",
      "iteration 206\n",
      "iteration 207\n",
      "iteration 208\n",
      "iteration 209\n",
      "iteration 210\n",
      "iteration 211\n",
      "iteration 212\n",
      "iteration 213\n",
      "iteration 214\n",
      "iteration 215\n",
      "iteration 216\n",
      "iteration 217\n",
      "iteration 218\n",
      "iteration 219\n",
      "iteration 220\n",
      "iteration 221\n",
      "iteration 222\n",
      "iteration 223\n",
      "iteration 224\n",
      "iteration 225\n",
      "iteration 226\n",
      "iteration 227\n",
      "iteration 228\n",
      "iteration 229\n",
      "iteration 230\n",
      "iteration 231\n",
      "iteration 232\n",
      "iteration 233\n",
      "iteration 234\n",
      "iteration 235\n",
      "iteration 236\n",
      "iteration 237\n",
      "iteration 238\n",
      "iteration 239\n",
      "iteration 240\n",
      "iteration 241\n",
      "iteration 242\n",
      "iteration 243\n",
      "iteration 244\n",
      "iteration 245\n",
      "iteration 246\n",
      "iteration 247\n",
      "iteration 248\n",
      "iteration 249\n",
      "iteration 250\n",
      "iteration 251\n",
      "iteration 252\n",
      "iteration 253\n",
      "iteration 254\n",
      "iteration 255\n",
      "iteration 256\n",
      "iteration 257\n",
      "iteration 258\n",
      "iteration 259\n",
      "iteration 260\n",
      "iteration 261\n",
      "iteration 262\n",
      "iteration 263\n",
      "iteration 264\n",
      "iteration 265\n",
      "iteration 266\n",
      "iteration 267\n",
      "iteration 268\n",
      "iteration 269\n",
      "iteration 270\n",
      "iteration 271\n",
      "iteration 272\n",
      "iteration 273\n",
      "iteration 274\n",
      "iteration 275\n",
      "iteration 276\n",
      "iteration 277\n",
      "iteration 278\n",
      "iteration 279\n",
      "iteration 280\n",
      "iteration 281\n",
      "iteration 282\n",
      "iteration 283\n",
      "iteration 284\n",
      "iteration 285\n",
      "iteration 286\n",
      "iteration 287\n",
      "iteration 288\n",
      "iteration 289\n",
      "iteration 290\n",
      "iteration 291\n",
      "iteration 292\n",
      "iteration 293\n",
      "iteration 294\n",
      "iteration 295\n",
      "iteration 296\n",
      "iteration 297\n",
      "iteration 298\n",
      "iteration 299\n",
      "iteration 300\n",
      "iteration 301\n",
      "iteration 302\n",
      "iteration 303\n",
      "iteration 304\n",
      "iteration 305\n",
      "iteration 306\n",
      "iteration 307\n",
      "iteration 308\n",
      "iteration 309\n",
      "iteration 310\n",
      "iteration 311\n",
      "iteration 312\n",
      "iteration 313\n",
      "iteration 314\n",
      "iteration 315\n",
      "iteration 316\n",
      "iteration 317\n",
      "iteration 318\n",
      "iteration 319\n",
      "iteration 320\n",
      "iteration 321\n",
      "iteration 322\n",
      "iteration 323\n",
      "iteration 324\n",
      "iteration 325\n",
      "iteration 326\n",
      "iteration 327\n",
      "iteration 328\n",
      "iteration 329\n",
      "iteration 330\n",
      "iteration 331\n",
      "iteration 332\n",
      "iteration 333\n",
      "iteration 334\n",
      "iteration 335\n",
      "iteration 336\n",
      "iteration 337\n",
      "iteration 338\n",
      "iteration 339\n",
      "iteration 340\n",
      "iteration 341\n",
      "iteration 342\n",
      "iteration 343\n",
      "iteration 344\n",
      "iteration 345\n",
      "iteration 346\n",
      "iteration 347\n",
      "iteration 348\n",
      "iteration 349\n",
      "iteration 350\n",
      "iteration 351\n",
      "iteration 352\n",
      "iteration 353\n",
      "iteration 354\n",
      "iteration 355\n",
      "iteration 356\n",
      "iteration 357\n",
      "iteration 358\n",
      "iteration 359\n",
      "iteration 360\n",
      "iteration 361\n",
      "iteration 362\n",
      "iteration 363\n",
      "iteration 364\n",
      "iteration 365\n",
      "iteration 366\n",
      "iteration 367\n",
      "iteration 368\n",
      "iteration 369\n",
      "iteration 370\n",
      "iteration 371\n",
      "iteration 372\n",
      "iteration 373\n",
      "iteration 374\n",
      "iteration 375\n",
      "iteration 376\n",
      "iteration 377\n",
      "iteration 378\n",
      "iteration 379\n",
      "iteration 380\n",
      "iteration 381\n",
      "iteration 382\n",
      "iteration 383\n",
      "iteration 384\n",
      "iteration 385\n",
      "iteration 386\n",
      "iteration 387\n",
      "iteration 388\n",
      "iteration 389\n",
      "iteration 390\n",
      "iteration 391\n",
      "iteration 392\n",
      "iteration 393\n",
      "iteration 394\n",
      "iteration 395\n",
      "iteration 396\n",
      "iteration 397\n",
      "iteration 398\n",
      "iteration 399\n",
      "iteration 400\n",
      "iteration 401\n",
      "iteration 402\n",
      "iteration 403\n",
      "iteration 404\n",
      "iteration 405\n",
      "iteration 406\n",
      "iteration 407\n",
      "iteration 408\n",
      "iteration 409\n",
      "iteration 410\n",
      "iteration 411\n",
      "iteration 412\n",
      "iteration 413\n",
      "iteration 414\n",
      "iteration 415\n",
      "iteration 416\n",
      "iteration 417\n",
      "iteration 418\n",
      "iteration 419\n",
      "iteration 420\n",
      "iteration 421\n",
      "iteration 422\n",
      "iteration 423\n",
      "iteration 424\n",
      "iteration 425\n",
      "iteration 426\n",
      "iteration 427\n",
      "iteration 428\n",
      "iteration 429\n",
      "iteration 430\n",
      "iteration 431\n",
      "iteration 432\n",
      "iteration 433\n",
      "iteration 434\n",
      "iteration 435\n",
      "iteration 436\n",
      "iteration 437\n",
      "iteration 438\n",
      "iteration 439\n",
      "iteration 440\n",
      "iteration 441\n",
      "iteration 442\n",
      "iteration 443\n",
      "iteration 444\n",
      "iteration 445\n",
      "iteration 446\n",
      "iteration 447\n",
      "iteration 448\n",
      "iteration 449\n",
      "iteration 450\n",
      "iteration 451\n",
      "iteration 452\n",
      "iteration 453\n",
      "iteration 454\n",
      "iteration 455\n",
      "iteration 456\n",
      "iteration 457\n",
      "iteration 458\n",
      "iteration 459\n",
      "iteration 460\n",
      "iteration 461\n",
      "iteration 462\n",
      "iteration 463\n",
      "iteration 464\n",
      "iteration 465\n",
      "iteration 466\n",
      "iteration 467\n",
      "iteration 468\n",
      "iteration 469\n",
      "iteration 470\n",
      "iteration 471\n",
      "iteration 472\n",
      "iteration 473\n",
      "iteration 474\n",
      "iteration 475\n",
      "iteration 476\n",
      "iteration 477\n",
      "iteration 478\n",
      "iteration 479\n",
      "iteration 480\n",
      "iteration 481\n",
      "iteration 482\n",
      "iteration 483\n",
      "iteration 484\n",
      "iteration 485\n",
      "iteration 486\n",
      "iteration 487\n",
      "iteration 488\n",
      "iteration 489\n",
      "iteration 490\n",
      "iteration 491\n",
      "iteration 492\n",
      "iteration 493\n",
      "iteration 494\n",
      "iteration 495\n",
      "iteration 496\n",
      "iteration 497\n",
      "iteration 498\n",
      "iteration 499\n",
      "iteration 500\n",
      "iteration 501\n",
      "iteration 502\n",
      "iteration 503\n",
      "iteration 504\n",
      "iteration 505\n",
      "iteration 506\n",
      "iteration 507\n",
      "iteration 508\n",
      "iteration 509\n",
      "iteration 510\n",
      "iteration 511\n",
      "iteration 512\n",
      "iteration 513\n",
      "iteration 514\n",
      "iteration 515\n",
      "iteration 516\n",
      "iteration 517\n",
      "iteration 518\n",
      "iteration 519\n",
      "iteration 520\n",
      "iteration 521\n",
      "iteration 522\n",
      "iteration 523\n",
      "iteration 524\n",
      "iteration 525\n",
      "iteration 526\n",
      "iteration 527\n",
      "iteration 528\n",
      "iteration 529\n",
      "iteration 530\n",
      "iteration 531\n",
      "iteration 532\n",
      "iteration 533\n",
      "iteration 534\n",
      "iteration 535\n",
      "iteration 536\n",
      "iteration 537\n",
      "iteration 538\n",
      "iteration 539\n",
      "iteration 540\n",
      "iteration 541\n",
      "iteration 542\n",
      "iteration 543\n",
      "iteration 544\n",
      "iteration 545\n",
      "iteration 546\n",
      "iteration 547\n",
      "iteration 548\n",
      "iteration 549\n",
      "iteration 550\n",
      "iteration 551\n",
      "iteration 552\n",
      "iteration 553\n",
      "iteration 554\n",
      "iteration 555\n",
      "iteration 556\n",
      "iteration 557\n",
      "iteration 558\n",
      "iteration 559\n",
      "iteration 560\n",
      "iteration 561\n",
      "iteration 562\n",
      "iteration 563\n",
      "iteration 564\n",
      "iteration 565\n",
      "iteration 566\n",
      "iteration 567\n",
      "iteration 568\n",
      "iteration 569\n",
      "iteration 570\n",
      "iteration 571\n",
      "iteration 572\n",
      "iteration 573\n",
      "iteration 574\n",
      "iteration 575\n",
      "iteration 576\n",
      "iteration 577\n",
      "iteration 578\n",
      "iteration 579\n",
      "iteration 580\n",
      "iteration 581\n",
      "iteration 582\n",
      "iteration 583\n",
      "iteration 584\n",
      "iteration 585\n",
      "iteration 586\n",
      "iteration 587\n",
      "iteration 588\n",
      "iteration 589\n",
      "iteration 590\n",
      "iteration 591\n",
      "iteration 592\n",
      "iteration 593\n",
      "iteration 594\n",
      "iteration 595\n",
      "iteration 596\n",
      "iteration 597\n",
      "iteration 598\n",
      "iteration 599\n",
      "iteration 600\n",
      "iteration 601\n",
      "iteration 602\n",
      "iteration 603\n",
      "iteration 604\n",
      "iteration 605\n",
      "iteration 606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 607\n",
      "iteration 608\n",
      "iteration 609\n",
      "iteration 610\n",
      "iteration 611\n",
      "iteration 612\n",
      "iteration 613\n",
      "iteration 614\n",
      "iteration 615\n",
      "iteration 616\n",
      "iteration 617\n",
      "iteration 618\n",
      "iteration 619\n",
      "iteration 620\n",
      "iteration 621\n",
      "iteration 622\n",
      "iteration 623\n",
      "iteration 624\n",
      "iteration 625\n",
      "iteration 626\n",
      "iteration 627\n",
      "iteration 628\n",
      "iteration 629\n",
      "iteration 630\n",
      "iteration 631\n",
      "iteration 632\n",
      "iteration 633\n",
      "iteration 634\n",
      "iteration 635\n",
      "iteration 636\n",
      "iteration 637\n",
      "iteration 638\n",
      "iteration 639\n",
      "iteration 640\n",
      "iteration 641\n",
      "iteration 642\n",
      "iteration 643\n",
      "iteration 644\n",
      "iteration 645\n",
      "iteration 646\n",
      "iteration 647\n",
      "iteration 648\n",
      "iteration 649\n",
      "iteration 650\n",
      "iteration 651\n",
      "iteration 652\n",
      "iteration 653\n",
      "iteration 654\n",
      "iteration 655\n",
      "iteration 656\n",
      "iteration 657\n",
      "iteration 658\n",
      "iteration 659\n",
      "iteration 660\n",
      "iteration 661\n",
      "iteration 662\n",
      "iteration 663\n",
      "iteration 664\n",
      "iteration 665\n",
      "iteration 666\n",
      "iteration 667\n",
      "iteration 668\n",
      "iteration 669\n",
      "iteration 670\n",
      "iteration 671\n",
      "iteration 672\n",
      "iteration 673\n",
      "iteration 674\n",
      "iteration 675\n",
      "iteration 676\n",
      "iteration 677\n",
      "iteration 678\n",
      "iteration 679\n",
      "iteration 680\n",
      "iteration 681\n",
      "iteration 682\n",
      "iteration 683\n",
      "iteration 684\n",
      "iteration 685\n",
      "iteration 686\n",
      "iteration 687\n",
      "iteration 688\n",
      "iteration 689\n",
      "iteration 690\n",
      "iteration 691\n",
      "iteration 692\n",
      "iteration 693\n",
      "iteration 694\n",
      "iteration 695\n",
      "iteration 696\n",
      "iteration 697\n",
      "iteration 698\n",
      "iteration 699\n",
      "iteration 700\n",
      "iteration 701\n",
      "iteration 702\n",
      "iteration 703\n",
      "iteration 704\n",
      "iteration 705\n",
      "iteration 706\n",
      "iteration 707\n",
      "iteration 708\n",
      "iteration 709\n",
      "iteration 710\n",
      "iteration 711\n",
      "iteration 712\n",
      "iteration 713\n",
      "iteration 714\n",
      "iteration 715\n",
      "iteration 716\n",
      "iteration 717\n",
      "iteration 718\n",
      "iteration 719\n",
      "iteration 720\n",
      "iteration 721\n",
      "iteration 722\n",
      "iteration 723\n",
      "iteration 724\n",
      "iteration 725\n",
      "iteration 726\n",
      "iteration 727\n",
      "iteration 728\n",
      "iteration 729\n",
      "iteration 730\n",
      "iteration 731\n",
      "iteration 732\n",
      "iteration 733\n",
      "iteration 734\n",
      "iteration 735\n",
      "iteration 736\n",
      "iteration 737\n",
      "iteration 738\n",
      "iteration 739\n",
      "iteration 740\n",
      "iteration 741\n",
      "iteration 742\n",
      "iteration 743\n",
      "iteration 744\n",
      "iteration 745\n",
      "iteration 746\n",
      "iteration 747\n",
      "iteration 748\n",
      "iteration 749\n",
      "iteration 750\n",
      "iteration 751\n",
      "iteration 752\n",
      "iteration 753\n",
      "iteration 754\n",
      "iteration 755\n",
      "iteration 756\n",
      "iteration 757\n",
      "iteration 758\n",
      "iteration 759\n",
      "iteration 760\n",
      "iteration 761\n",
      "iteration 762\n",
      "iteration 763\n",
      "iteration 764\n",
      "iteration 765\n",
      "iteration 766\n",
      "iteration 767\n",
      "iteration 768\n",
      "iteration 769\n",
      "iteration 770\n",
      "iteration 771\n",
      "iteration 772\n",
      "iteration 773\n",
      "iteration 774\n",
      "iteration 775\n",
      "iteration 776\n",
      "iteration 777\n",
      "iteration 778\n",
      "iteration 779\n",
      "iteration 780\n",
      "iteration 781\n",
      "iteration 782\n",
      "iteration 783\n",
      "iteration 784\n",
      "iteration 785\n",
      "iteration 786\n",
      "iteration 787\n",
      "iteration 788\n",
      "iteration 789\n",
      "iteration 790\n",
      "iteration 791\n",
      "iteration 792\n",
      "iteration 793\n",
      "iteration 794\n",
      "iteration 795\n",
      "iteration 796\n",
      "iteration 797\n",
      "iteration 798\n",
      "iteration 799\n",
      "iteration 800\n",
      "iteration 801\n",
      "iteration 802\n",
      "iteration 803\n",
      "iteration 804\n",
      "iteration 805\n",
      "iteration 806\n",
      "iteration 807\n",
      "iteration 808\n",
      "iteration 809\n",
      "iteration 810\n",
      "iteration 811\n",
      "iteration 812\n",
      "iteration 813\n",
      "iteration 814\n",
      "iteration 815\n",
      "iteration 816\n",
      "iteration 817\n",
      "iteration 818\n",
      "iteration 819\n",
      "iteration 820\n",
      "iteration 821\n",
      "iteration 822\n",
      "iteration 823\n",
      "iteration 824\n",
      "iteration 825\n",
      "iteration 826\n",
      "iteration 827\n",
      "iteration 828\n",
      "iteration 829\n",
      "iteration 830\n",
      "iteration 831\n",
      "iteration 832\n",
      "iteration 833\n",
      "iteration 834\n",
      "iteration 835\n",
      "iteration 836\n",
      "iteration 837\n",
      "iteration 838\n",
      "iteration 839\n",
      "iteration 840\n",
      "iteration 841\n",
      "iteration 842\n",
      "iteration 843\n",
      "iteration 844\n",
      "iteration 845\n",
      "iteration 846\n",
      "iteration 847\n",
      "iteration 848\n",
      "iteration 849\n",
      "iteration 850\n",
      "iteration 851\n",
      "iteration 852\n",
      "iteration 853\n",
      "iteration 854\n",
      "iteration 855\n",
      "iteration 856\n",
      "iteration 857\n",
      "iteration 858\n",
      "iteration 859\n",
      "iteration 860\n",
      "iteration 861\n",
      "iteration 862\n",
      "iteration 863\n",
      "iteration 864\n",
      "iteration 865\n",
      "iteration 866\n",
      "iteration 867\n",
      "iteration 868\n",
      "iteration 869\n",
      "iteration 870\n",
      "iteration 871\n",
      "iteration 872\n",
      "iteration 873\n",
      "iteration 874\n",
      "iteration 875\n",
      "iteration 876\n",
      "iteration 877\n",
      "iteration 878\n",
      "iteration 879\n",
      "iteration 880\n",
      "iteration 881\n",
      "iteration 882\n",
      "iteration 883\n",
      "iteration 884\n",
      "iteration 885\n",
      "iteration 886\n",
      "iteration 887\n",
      "iteration 888\n",
      "iteration 889\n",
      "iteration 890\n",
      "iteration 891\n",
      "iteration 892\n",
      "iteration 893\n",
      "iteration 894\n",
      "iteration 895\n",
      "iteration 896\n",
      "iteration 897\n",
      "iteration 898\n",
      "iteration 899\n",
      "iteration 900\n",
      "iteration 901\n",
      "iteration 902\n",
      "iteration 903\n",
      "iteration 904\n",
      "iteration 905\n",
      "iteration 906\n",
      "iteration 907\n",
      "iteration 908\n",
      "iteration 909\n",
      "iteration 910\n",
      "iteration 911\n",
      "iteration 912\n",
      "iteration 913\n",
      "iteration 914\n",
      "iteration 915\n",
      "iteration 916\n",
      "iteration 917\n",
      "iteration 918\n",
      "iteration 919\n",
      "iteration 920\n",
      "iteration 921\n",
      "iteration 922\n",
      "iteration 923\n",
      "iteration 924\n",
      "iteration 925\n",
      "iteration 926\n",
      "iteration 927\n",
      "iteration 928\n",
      "iteration 929\n",
      "iteration 930\n",
      "iteration 931\n",
      "iteration 932\n",
      "iteration 933\n",
      "iteration 934\n",
      "iteration 935\n",
      "iteration 936\n",
      "iteration 937\n",
      "iteration 938\n",
      "iteration 939\n",
      "iteration 940\n",
      "iteration 941\n",
      "iteration 942\n",
      "iteration 943\n",
      "iteration 944\n",
      "iteration 945\n",
      "iteration 946\n",
      "iteration 947\n",
      "iteration 948\n",
      "iteration 949\n",
      "iteration 950\n",
      "iteration 951\n",
      "iteration 952\n",
      "iteration 953\n",
      "iteration 954\n",
      "iteration 955\n",
      "iteration 956\n",
      "iteration 957\n",
      "iteration 958\n",
      "iteration 959\n",
      "iteration 960\n",
      "iteration 961\n",
      "iteration 962\n",
      "iteration 963\n",
      "iteration 964\n",
      "iteration 965\n",
      "iteration 966\n",
      "iteration 967\n",
      "iteration 968\n",
      "iteration 969\n",
      "iteration 970\n",
      "iteration 971\n",
      "iteration 972\n",
      "iteration 973\n",
      "iteration 974\n",
      "iteration 975\n",
      "iteration 976\n",
      "iteration 977\n",
      "iteration 978\n",
      "iteration 979\n",
      "iteration 980\n",
      "iteration 981\n",
      "iteration 982\n",
      "iteration 983\n",
      "iteration 984\n",
      "iteration 985\n",
      "iteration 986\n",
      "iteration 987\n",
      "iteration 988\n",
      "iteration 989\n",
      "iteration 990\n",
      "iteration 991\n",
      "iteration 992\n",
      "iteration 993\n",
      "iteration 994\n",
      "iteration 995\n",
      "iteration 996\n",
      "iteration 997\n",
      "iteration 998\n",
      "iteration 999\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "vec_size = 20 #word2vec has 300, but I left 100 here\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are testing our word embeddings with some query. Our query will be \"Coronavirus transmission\" and we are hoping to  find all the documents that are talking about coronavirus transmission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is known about Covid-19 incubation?\n"
     ]
    }
   ],
   "source": [
    "test_data = word_tokenize(QUERY.lower()) #change this query to test different things \n",
    "v1 = model.infer_vector(test_data)\n",
    "print(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('53', 0.9585644006729126), ('2', 0.9557319283485413), ('57', 0.9515500068664551), ('44', 0.9492290019989014), ('47', 0.9472017288208008), ('56', 0.9471619725227356), ('11', 0.9470564723014832), ('58', 0.9351019859313965), ('4', 0.9318826794624329), ('84', 0.9258043766021729), ('90', 0.9254163503646851), ('86', 0.9197592735290527), ('25', 0.9177792072296143), ('39', 0.9117348194122314), ('20', 0.911331057548523), ('29', 0.8868950605392456), ('59', 0.8069602847099304), ('68', 0.7970524430274963), ('67', 0.76641446352005), ('1', 0.6952894926071167), ('94', 0.694424569606781), ('65', 0.692328929901123), ('6', 0.6846479177474976), ('70', 0.6808638572692871), ('50', 0.6744297742843628), ('72', 0.6282376050949097), ('40', 0.6197509765625), ('9', 0.6145830750465393), ('51', 0.6060972213745117), ('85', 0.5939964056015015), ('18', 0.5676460266113281), ('60', 0.5560325980186462), ('32', 0.5553334951400757), ('14', 0.5230740904808044), ('37', 0.5217598676681519), ('22', 0.5160170197486877)]\n"
     ]
    }
   ],
   "source": [
    "similar_doc = model.docvecs.most_similar([v1],topn = len(tagged_data))\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.2082465418276749, 70), (1.105989818458939, 44), (0.7364177955156412, 47), (0.6015592315044789, 50), (0.5007112635960242, 14), (0.09070004873282275, 25), (0.012284146927069112, 72), (0.01110413364791927, 29), (0.010898919771421281, 85), (0.00938956095434747, 20), (0.004123214304340037, 57), (0.002864962057021431, 67), (0.0026424570418683816, 9), (0.0022667995310501, 1), (0.0019901782086053227, 84), (0.0017645533572422866, 2), (0.0012045959674469443, 6), (nan, 94), (0.001391074992456606, 11), (0.001145104824421353, 56), (0.0008703313601326729, 39), (0.0005202893261972667, 86), (0.0005095720604692783, 90), (0.00048529013256631853, 65), (0.00037488492499289464, 22), (0.00034749639126352904, 60), (nan, 53), (0.00033165472710120966, 4), (0.0003213257493478709, 58), (0.00029903822287153086, 40), (0.0002917701945442595, 51), (0.00021141394415243525, 18), (0.00020356015326445805, 32), (0.0001723667585540915, 68), (0.00015354031602439843, 59), (8.229099856199205e-05, 37)]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for doc in similar_doc:\n",
    "    results[int(doc[0])] = doc[1]\n",
    "\n",
    "d2v_results = []\n",
    "for paper in filtered_results:\n",
    "    score = filtered_results[paper]\n",
    "    result = results[score[1]]\n",
    "    freshed_score = (result*score[0],score[1])\n",
    "    \n",
    "    filtered_results[paper] = freshed_score\n",
    "    d2v_results.append(freshed_score)\n",
    "d2v_results.sort(reverse = True)\n",
    "print(d2v_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "Title: First 12 patients with coronavirus disease\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "Introduction: More than 93,000 cases of coronavirus disease have been reported worldwide. We describe the epidemiology, clinical course, and virologic characteristics of the first 12 U.S. patients with COVID-19.We collected demographic, exposure, and clinical information from 12 patients confirmed by CDC during January 20-February 5, 2020 to have COVID-19. Respiratory, stool, serum, and urine specimens were submitted for SARS-CoV-2 rRT-PCR testing, virus culture, and whole genome sequencing.Results: Among the 12 patients, median age was 53 years (range: 21-68); 8 were male, 10 had traveled to China, and two were contacts of patients in this series. Commonly reported signs and symptoms at illness onset were fever (n=7) and cough (n=8). Seven patients were hospitalized with radiographic evidence of pneumonia and demonstrated clinical or laboratory signs of worsening during the second week of illness. Three were treated with the investigational antiviral remdesivir. All patients had SARS-CoV-2 RNA detected in respiratory specimens, typically for 2-3 weeks after illness onset, with lowest rRT-PCR Ct values often detected in the first week. SARS-CoV-2 RNA was detected after reported symptom resolution in seven patients. SARS-CoV-2 was cultured from respiratory specimens, and SARS-CoV-2 RNA was detected in stool from 7/10 patients.In 12 patients with mild to moderately severe illness, SARS-CoV-2 RNA and viable virus were detected early, and prolonged RNA detection suggests the window for diagnosis is long. Hospitalized patients showed signs of worsening in the second week after illness onset.for use under a CC0 license.\n",
      "\n",
      "\u001b[1mBody\u001b[0m\n",
      "In December 2019, an outbreak of the novel disease COVID-19 caused by the newly identified severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was reported in Wuhan City, Hubei province, China. As of March 4, 2020, more than 93,000 COVID-19 cases have been reported in 73 countries, 1 and 80 confirmed and presumptive patients with COVID-19 have been identified in the United States; 2 one has been previously described in detail. 3 SARS-CoV-2 RNA detection, virus culture, and the relationship to the clinical course of COVID-19 are not fully understood. We report the epidemiology, clinical course, clinical management, and virologic characteristics of the first 12 patients with COVID-19 diagnosed in the United States.Local health departments in consultation with clinicians identified patients under investigation (PUI) for COVID-19 beginning January 17, 2020. PUI testing criteria changed during this period but included the presence of fever and/or lower respiratory symptoms (e.g., cough or shortness of breath) and at least one epidemiologic risk factor in the two weeks before symptom onset. During January 17-31, epidemiologic risk factors were history of travel from Wuhan City, close contact with an ill PUI, or close contact with a patient with laboratory-confirmed COVID-19. 4, 5 Beginning February 1, epidemiologic risk factors changed to close contact with a patient with confirmed COVID-19 or history of travel from mainland China. 6 During both time periods, close contact was defined as being within 6 feet for a prolonged period of time 7 or contact with respiratory secretions. 8 Specimens from PUIs were tested for SARS-CoV-2 at the Centers for Disease Control and Prevention (CDC).Upper respiratory tract specimens (nasopharyngeal [NP] , oropharyngeal [OP] ) and available lower respiratory tract specimens (sputum) were collected and tested for SARS-CoV-2 RNA by real-time reverse-transcription polymerase chain reaction (rRT-PCR). 3 A case of COVID-19 was defined as identification of laboratory-confirmed SARS-CoV-2 in ≥1 specimen from a patient. We included patients with COVID-19 who were confirmed by CDC during January 20-February 5, 2020.Patients with COVID-19 were interviewed by public health officials to collect information on demographics, exposures, travel history, and symptoms, including signs or symptoms before presentation. For all twelve patients, available medical records were reviewed. For hospitalized patients, clinicians systematically abstracted clinical data from the medical record.Illness day 1 was defined as the first day of reported COVID-19 signs and symptoms; collection date of the first SARS-CoV-2-positive specimen was used for one patient with no clear symptom onset date. When symptoms at onset or onset dates in the medical record differed from those reported from the public health interview, the latter were used. Results for virologic tests were reported relative to illness day 1. Duration of potential exposure to SARS-CoV-2 was defined as dates of travel to China or dates of first to last exposure to a U.S. patient with COVID-19. Fever was defined as subjective fever or temperature ≥100.4 °F.We requested collection of NP swabs, OP swabs, sputum (if available), serum, urine, and stool from each patient initially for every 2-3 days for the first 17 days of illness for SARS-CoV-2 virologic testing. 9for use under a CC0 license. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint Further specimens were collected for testing if the patient continued to test positive for SARS-CoV-2 beyond day 17.CDC's Human Research Protection Office determined this work was exempt from human subjects' research regulations as it involved identification, control, or prevention of disease in response to an immediate public health threat. Forms used in this response were approved under OMB, number 0920-1011. Data were analyzed and visualized using Excel, SAS 9.4, R 3.6.2, and Python 3.7.3. 10-13 Laboratory Methods Specimens were evaluated using SARS-CoV-2 RNA detection, virus culture, whole genome sequencing, and phylogenetic analysis. Virus culture was attempted from early SARS-CoV-2-positive respiratory specimens (NP swabs, OP swabs, and sputum) from 9 patients. Further virus culture is ongoing. Detailed methods are included in the Appendix.Twelve patients with confirmed COVID-19 were identified in six states. Five patients received only outpatient care and were isolated at home (Patients 1-5), and seven were hospitalized (Patients 6-12) ( Figure 1 ). Median patient age was 53 years (range: 21-68); eight patients were male ( Table 1) . Four of five patients with ≥1 underlying medical conditions were hospitalized (Tables 1 and 2) .Dates of illness onset ranged from January 14 through 29. Ten patients traveled to mainland China in the two weeks before illness onset, including nine to Wuhan City. Two patients' only reported exposure was close contact with a previously identified U.S. patient with COVID-19. Among all patients, the duration of potential exposure ranged from 5 days to over 1 month; the time between last date of possible exposure and illness onset ranged from 0-5 days.The most commonly reported signs or symptoms at illness onset were cough (n=8) and subjective or measured fever (n=7) ( Table 1) . Two patients reported neither fever nor cough at onset, though they did develop them subsequently: one reported diarrhea as the initial symptom (one day before fever and cough), and the other reported sore throat.Over the course of illness, patients reported cough (n=12), subjective or measured fever (n=9), diarrhea (n=3), and vomiting (n=2). Three patients who did not report fever were never hospitalized and remained on home isolation. Of these, one patient reported only cough and rhinorrhea; one patient reported only cough which began before travel to China and did not change from the initial onset until resolution; and one patient reported cough, chills, fatigue, headache, and nausea.The clinical course for each hospitalized patient is described in the Appendix. The median duration of fever was 9 days (range: 2-11). Peak body temperature during hospitalization occurred at a median of illness day 9 (range: 4-10) ( Figure 2 ). All hospitalized patients had oxygen saturation <94% on room air at some point during their illness, with the oxygen saturation nadir (range: 86-93%) occurring at a median of illness day 12 (range: 4-23) (Figure 2 ). Five patients reported difficulty breathing, and four received supplemental oxygen (Table 2; Figure 1 ). Patient 9 required high-flow nasal cannula oxygen supplementation and intensive care monitoring.for use under a CC0 license. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint Two patients received a short course (≤3 days) of corticosteroids. Three, including one who also received corticosteroids, received the investigational antiviral remdesivir (Gilead Sciences, Foster City, California) under expanded access (compassionate use) for a duration of 4-10 days. Following remdesivir initiation, all had transient gastrointestinal symptoms, including nausea, vomiting, gastroparesis, or rectal bleeding. No other post-remdesivir symptoms were observed. Patient 9 reported loose stool and rectal bleeding and had traveled in Mexico before illness onset; stool later tested positive for Giardia and Clostridiodes difficile. Remdesivir was discontinued after improvement in each patient's respiratory symptoms.Blood cultures were negative in 6/6 hospitalized patients tested, including those obtained from four patients treated empirically for bacterial pneumonia. Molecular testing for influenza A and B on respiratory specimens was negative, and multi-pathogen respiratory PCR panels were negative for all targets in all hospitalized patients (Table 2) .Six of seven hospitalized patients had leukopenia (<4000 cells/µl), and the white blood cell count nadir occurred at a median of illness day 9 (range: 4-15) ( Figure 1 ). Procalcitonin levels were <0.15 ng/ml in five of six patients who had levels checked. Aminotransferase levels were elevated in all hospitalized patients: AST levels peaked (median peak value 129 U/L, range 46-190 U/L) at a median of illness day 13 (range 7-19) and ALT levels peaked (median peak value 136 U/L, range 66-389 U/L) at a median of illness day 14 (range: 6-23). Three of seven hospitalized patients had mild elevations in alkaline phosphatase levels >115 U/L (maximum value 163 U/L). Elevated lactate dehydrogenase levels >600 U/L, coinciding with clinical deterioration, were observed in two patients tested. No major elevations in serum total bilirubin (7 patients tested) or prolongations in prothrombin time (4 patients tested) were identified. Among the three remdesivir recipients, aminotransferase elevation developed in Patient 6 one day after starting remdesivir and in Patient 8 four days after starting remdesivir. Patient 9 had aminotransferase elevation at illness days 6-7 before starting remdesivir; aminotransferase levels started to decrease but increased again five days after starting remdesivir.Unilateral or bilateral pulmonary opacities were seen on chest imaging at some point for all seven hospitalized patients (Table 2) . Four hospitalized patients did not have any abnormalities identified on initial chest radiograph (illness day range: 4-9). Patient 7 had an abnormal chest computed tomography scan on the day of the normal chest radiograph (day 7).Initial SARS-CoV-2 testing All 12 patients had respiratory specimens collected between illness days 1-9 (median, day 4), and all tested positive in ≥2 specimen types ( Figure 3 ). Among initial diagnostic specimens, SARS-CoV-2 RNA was detected in OP (11/11 patients), NP (10/12 patients), and sputum (4/4 patients). Viable SARS-CoV-2 virus was cultured from 5/6 initial NP specimens, 4/7 initial OP specimens, 3/3 initial sputum specimens, and 1/1 additional sputum specimen collected 3 days after the initial specimens ( Figure 3 ). Virus was cultured from two patients who were not hospitalized ( Figure 3 ).As of February 22, 398 specimens were collected and tested from the 12 patients throughout the course of illness. All 12 patients had SARS-CoV-2 RNA detected in at least one NP swab, 11/12 in an OP swab, 6/6 in sputum, 1/12 in serum, 7/10 in stool, and 0/10 in urine ( Figure 3 ). Among 98 pairs of simultaneous NP and OP specimens, 58 (59%) had concordant results. Among 27 discordant pairs with one positive for use under a CC0 license.This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint specimen, the NP specimen was positive in 70%; the remaining 13 discordant pairs had one negative and one inconclusive specimen. Two patients provided sputum specimens when NP and/or OP specimens tested negative, and sputum continued to be positive in both patients. In Patient 7, viral RNA was detected in sputum 17 days after the last positive OP specimen and ≥2 weeks after reported symptom resolution. In seven patients who had SARS-CoV-2 RNA detected in stool, most detections occurred when viral RNA was still detectable in the respiratory tract. Among three patients who reported diarrhea, all had viral RNA detected in stool.Mean Ct values in positive specimens were 17.0-39.0 for NP, 22.1-39.7 for OP, and 24.1-39.4 for stool. Ct values were lower in the first week of illness than the second in most patients ( Figure S1 ); in some patients, low Ct values continued into the 2 nd and 3 rd week of illness. There was no apparent relationship between Ct values in the upper respiratory tract and disease progression. SARS-CoV-2 rRT-PCR results turned positive in serum of Patient 9 in the second week of illness at the time of rapid clinical deterioration.Serial testing to determine duration of RNA detection and viral shedding is ongoing. As of February 22, SARS-CoV-2 RNA has been detected at a maximum of day 26 in NP specimens, day 26 in OP, day 29 in sputum, and day 25 in stool ( Figure 3 ). The duration of viral RNA detection did not differ by hospitalization status or supplemental oxygen requirement.As of February 22, all patients reported symptom resolution (Figure 1 ). Eleven patients reported cough, (often intermittent) as the last symptom. Median symptom duration was 14 days (range: 6-20). SARS-CoV-2 RNA was detected after reported symptom resolution in 7/11 patients, including in NP (n=6), OP (n=2), sputum (n=1), and stool (n=3) specimens. As of February 22, one patient remained hospitalized, and five patients remained on home isolation. Home isolation was discontinued for 6 patients per CDC criteria; 14 the last respiratory specimens with a positive or inconclusive test result were collected from these patients on days 12-29.Complete genome sequences were generated from respiratory specimens from all 12 patients. The sequences had >99% nucleotide identity to 85 reference sequences of SARS-CoV-2 genomes; phylogenetic tree analysis identified a few distinct subgroups ( Figure S2 ) which were not divergent from each other, indicating that the outbreak may still be in an early stage.We describe the first 12 patients with confirmed COVID-19 in the United States, including clinical course of the first 7 hospitalized patients. Nine patients had traveled to Wuhan City, the epicenter of the outbreak, one had traveled to China but not to Hubei Province, and two had close contact with a patient with confirmed COVID-19 in the United States, representing domestic human-to-human transmission. Illness ranged from mild to moderately severe, and hospitalized patients showed signs of clinical worsening in the second week. All patients recovered or are improving, and three patients tolerated treatment with the investigational antiviral remdesivir. SARS-CoV-2 RNA was detected in upper and lower respiratory specimens, stool, and serum. The highest viral RNA levels were detected in upper respiratory tract specimens, typically during the first week of illness. SARS-CoV-2 was cultured from the initial respiratory specimens of mild and moderately ill patients. Viral RNA was still detected after reported symptom resolution for seven patients. SARS-CoV-2 genome sequencing and phylogenetic for use under a CC0 license.This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint analysis from these 12 patients' respiratory tract specimens support a single recent zoonotic transmission event in Wuhan City and subsequent human-to-human transmission.Overall, these patients had milder disease than those in initial reports from China describing higher rates of complications and death. [15] [16] [17] [18] Initial case identification in China focused on hospitalized patients with pneumonia, but recent reports have described a milder clinical course, consistent with our findings. [19] [20] [21] [22] [23] Among hospitalized patients in this report, the second week of illness was characterized by clinical or laboratory signs of worsening such as hypoxemia, increase in fever, or elevation of aminotransferases. Although some patients received empiric antibiotic treatment for possible secondary bacterial pneumonia, no definitive evidence of bacterial co-infection was found. Worsening in the second week of illness is consistent with previous reports 15, 17 and highlights the importance of close monitoring beyond the first week of illness, even in patients with mild illness or no initial radiographic abnormalities.Patient 9, the most severely ill among this series, experienced sudden clinical deterioration late in the second week of illness. This was the only patient with SARS-CoV-2 RNA detected in serum, and detection in serum was temporally related to clinical deterioration. Similar observations have been described previously. 24, 25 Increased proinflammatory cytokines have been observed in patients with COVID-19, 17 and it is possible that cytokine dysregulation and endothelial dysfunction contribute to both clinical worsening and SARS-CoV-2 RNA detection in serum.Characterizing SARS-CoV-2 shedding is important to understand transmission and guide prevention strategies. We detected viral RNA and cultured virus from upper respiratory specimens, even from patients with lower respiratory tract illness. In general, Ct values in upper respiratory tract specimens were lowest during the first week of illness (suggesting high RNA counts), consistent with previous reports. 25-27 SARS-CoV-2 RNA was detected in respiratory tract specimens for 2-3 weeks in most patients and for up to 29 days as of February 22. In two patients with a productive cough, viral RNA was detected in sputum after RNA was no longer detectable in NP or OP specimens. SARS-CoV-2 RNA levels and duration of RNA detection did not appear to vary by illness severity, and several patients had viral RNA detected in respiratory specimens after reported symptom resolution.We detected SARS-CoV-2 RNA in stool of multiple patients; testing continues to assess if this represents viable virus. SARS-CoV-2 was cultured from one patient's stool in China 28 but the implications for transmission are unclear. We detected SARS-CoV-2 RNA in the serum of one hospitalized patient but did not detect RNA in urine. More data are needed to better understand how duration of RNA detection, RNA levels, and viable virus are related to symptom progression, illness severity, and transmission.Three hospitalized patients received the investigational antiviral remdesivir under expanded access (compassionate use) at the time of clinical worsening based upon a decision by each patient's clinician. Remdesivir inhibits viral replication through premature termination of RNA transcription. 29, 30 In vitro studies have demonstrated that remdesivir inhibits SARS-CoV-2 replication in non-human cells. 31 Because remdesivir use was not given as part of a randomized controlled trial, we are unable to assess its effectiveness or safety. Randomized controlled trials of remdesivir are underway. [32] [33] [34] Two hospitalized patients received corticosteroids. WHO interim guidance for clinical management of severe acute respiratory infection with suspected COVID-19 advises against use of corticosteroids unless indicated for another reason. 35 Several limitations should be considered when interpreting our findings. Our sample of patients is small. Information collected from patient interviews may have been subject to response bias. The threshold for admitting patients and for monitoring in the hospital was likely lower than for other respiratory infections because of uncertainty about the clinical course of COVID-19. Dates of illness resolution may be for use under a CC0 license. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint imprecise due to non-specific lingering symptoms or symptoms from chronic or unrelated conditions. Clinical laboratory tests and radiographic studies were ordered as a part of routine patient care and were not collected systematically. SARS-CoV-2 RNA detection does not necessarily reflect the presence of infectious virus, and rRT-PCR Ct values may have varied due to specimen collection or handling. Specimen collection is ongoing to inform both clinical management and infection prevention and control practices, and findings will be updated as more information becomes available.Characterization of the first 12 patients with COVID-19 identified in the United States, including 7 hospitalized patients, provides key insight into the epidemiology, clinical characteristics, and natural history of SARS-CoV-2 infection. These patients experienced mild to moderately severe illness. Clinicians should anticipate that some patients may worsen in the second week of illness. Early and prolonged detection of SARS-CoV-2 RNA suggest the window for diagnosis of COVID-19 is long. Although duration of infectiousness is unclear, our early data show viable virus can be cultured readily from upper respiratory tract specimens soon after illness onset. Further investigations are needed to understand clinical course, immunologic response, SARS-CoV-2 RNA detection, virus culture, and transmission, to inform clinical management and public health strategies to prevent disease spread.The findings and conclusions in this report are those of the author(s) and do not necessarily represent the official position of the Centers for Disease Control and Prevention.for use under a CC0 license. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint c Initial symptoms were obtained through patient interview, and may have differed from symptoms at the time of identification d One patient reported a cough with initial onset in mid-December before the patient traveled to China. The patient reported no change in the cough from the initial onset until resolution 2 weeks after SARS-CoV-2 was first detected.for use under a CC0 license.This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available author/funder, who has granted medRxiv a license to display the preprint in perpetuity.The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.03.09.20032896 doi: medRxiv preprint Legend: Specimen types tested include nasopharyngeal (NP) swab, oropharyngeal (OP) swab, sputum, serum, stool, and urine. Day of illness is the number of days from the date of symptom onset (day 1) until the date of specimen collection. Virus culture was attempted on selected respiratory specimens collected early in the course of illness. rRT-PCR results were reported as positive (all three targets positive), negative (all three targets negative), or inconclusive (only one or two positive targets). Red squares surrounding black-filled circles indicate rRT-PCR-positive specimens from which virus culture was successful. Gray squares surrounding black-filled circles indicate rRT-PCR-positive specimens from which virus culture was unsuccessful. Black-filled circles indicate rRT-PCR-positive specimens. Black-outlined circles indicate rRT-PCRnegative specimens. Gray-filled circles indicate specimens with inconclusive rRT-PCR results. The gray vertical bar indicates the date home isolation was discontinued. An asterisk indicates patients who required supplemental oxygen.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(all_papers[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.) bm25? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
