{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools as it\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \n",
    "    def __init__(self, paper_id, abstract, body_text):\n",
    "        self.paper_id = paper_id\n",
    "        self.abstract = abstract\n",
    "        self.body_text = body_text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, path):\n",
    "        with open(path, 'r') as fd:\n",
    "            data = json.load(fd)\n",
    "        \n",
    "        paper_id = data['paper_id']\n",
    "        abstract = '\\n'.join([record['text'] for record in data['abstract']])\n",
    "        body_text = '\\n'.join([record['text'] for record in data['body_text']])\n",
    "        return cls(paper_id, abstract, body_text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]} ... {self.body_text[:200]} ...'\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        paper_html = f'<b>Paper ID:</b> {self.paper_id}'\n",
    "        abstract_html = ['<p>' + record + '</p>' for record in self.abstract.split('\\n')]\n",
    "        abstract_html = '<h3>' + 'Abstract' + '</h3>' + ''.join(abstract_html)\n",
    "        body_text_html = ['<p>' + record + '</p>' for record in self.body_text.split('\\n')]\n",
    "        body_text_html = '<h3>' + 'Body text' + '</h3>' + ''.join(body_text_html)  \n",
    "        return paper_html + abstract_html + body_text_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionLoader:\n",
    "    \n",
    "    def __init__(self, dirs, spec=''):\n",
    "        spec = self._parse_spec(spec, dirs)   \n",
    "\n",
    "        docfiles = []\n",
    "        for i, dirname in enumerate(dirs):\n",
    "            dirfiles = glob.glob(f'{dirname}/**/*.json', recursive=True)\n",
    "            limit = spec[i] or len(dirfiles)            \n",
    "            docfiles.extend(dirfiles[:limit])\n",
    "        \n",
    "        self.docfiles = docfiles\n",
    "            \n",
    "    @staticmethod\n",
    "    def _parse_spec(spec, dirs):\n",
    "        if not spec: return [None] * len(dirs)\n",
    "            \n",
    "        spec_to_int = [int(s) if s.isdigit() else None \n",
    "                       for s in spec.split(':')]\n",
    "        \n",
    "        if len(dirs) != len(spec_to_int):\n",
    "            raise ValueError('length of dirs does not match length of spec')\n",
    "        \n",
    "        return spec_to_int\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fname in self.docfiles:\n",
    "            yield Document.from_json(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic usage \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00d16927588fb04d4be0e6b269fc02f0d3c2aa7b: Infectious bronchitis (IB) causes significant economic losses in the global poultry industry. Control of infectious bronchitis is hindered by the genetic diversity of the causative agent, infectious b ... Infectious bronchitis (IB), which is caused by infectious bronchitis virus (IBV), is one of the most important diseases of poultry, causing severe economic losses worldwide. 8 Clinical signs of diseas ...\n",
      "number of documents: 6\n"
     ]
    }
   ],
   "source": [
    "# specify list of directories\n",
    "# note: if topmost directory does not contain json files, \n",
    "# recursive search is performed\n",
    "dirs = ('./dataset/noncomm_use100', \n",
    "        './dataset/comm_use100',\n",
    "        './dataset/biorxiv_medrxiv100')\n",
    "\n",
    "# pass above list and spec string\n",
    "# each entry, delimited by :, in spec string represents the number of json files \n",
    "# that will be read from corresponding directory\n",
    "collection_loader = CollectionLoader(dirs, spec='2:1:3')\n",
    "collection = list(collection_loader)\n",
    "\n",
    "# sanity check\n",
    "print(collection[3])\n",
    "print('number of documents:', len(collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rich output (only available in Jupyter)\n",
    "# collection[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def __init__(self, model, before_tokenizer=None, after_tokenizer=None):\n",
    "        self.model = model\n",
    "        self.before_tokenizer = before_tokenizer or []\n",
    "        self.after_tokenizer = after_tokenizer or []\n",
    "\n",
    "        self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        nlp = self._create_tokenizer()\n",
    "\n",
    "        for component in self.after_tokenizer:\n",
    "            if isinstance(component, str):\n",
    "                # spacy component\n",
    "                if component in self._pretrained:\n",
    "                    obj = self._pretrained[component]\n",
    "                else:\n",
    "                    obj = nlp.create_pipe(component)\n",
    "                \n",
    "                nlp.add_pipe(obj)\n",
    "            else:\n",
    "                # user-defined component\n",
    "                name, obj = component\n",
    "                nlp.add_pipe(obj, name=name)\n",
    "\n",
    "        # we dont't need cache anymore \n",
    "        del self._pretrained\n",
    "        \n",
    "        self.nlp = nlp\n",
    "\n",
    "\n",
    "    def _create_tokenizer(self):\n",
    "        # hacky way of creating spacy pipeline without components\n",
    "        \n",
    "        nlp = spacy.load(self.model)\n",
    "\n",
    "        # we have to cache the pretrained components in case we need them later\n",
    "        self._pretrained = {}\n",
    "        \n",
    "        for pipe in nlp.pipe_names:\n",
    "            name, obj = nlp.remove_pipe(pipe)\n",
    "            \n",
    "            if name in self.after_tokenizer:\n",
    "                self._pretrained[name] = obj\n",
    "        \n",
    "        return nlp\n",
    "\n",
    "    def _apply_before_tokenizer(self, text):\n",
    "        for func in self.before_tokenizer:\n",
    "            text = func(text)\n",
    "        return text\n",
    "\n",
    "    def __call__(self, texts, n_process=1):\n",
    "        pre_tokenizer = (self._apply_before_tokenizer(text) for text in texts)\n",
    "       \n",
    "        # nlp.pipe returns the generator, so yield from it\n",
    "        yield from self.nlp.pipe(pre_tokenizer, n_process=n_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ALPHANUM_REG = re.compile(r\"[^A-Za-z']\")\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def single_space(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    return re.sub(NON_ALPHANUM_REG, ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build a class for the convenient access to the tokens of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocViewer:\n",
    "\n",
    "    def __init__(self, doc):\n",
    "        self.doc = doc\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # if the key is a normal attribute, get its value\n",
    "        # otherwise ask for forgiveness\n",
    "        \n",
    "        try:\n",
    "            return [getattr(token, key) for token in self.doc]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        values = []\n",
    "        for token in self.doc:\n",
    "            extension_holder = getattr(token, '_')\n",
    "            values.append(getattr(extension_holder, key))\n",
    "            \n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no stemmer in spacy, so let's provide one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from spacy.tokens import Token\n",
    "\n",
    "class Stemmer:\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self._stemmer = SnowballStemmer(language)\n",
    "        Token.set_extension('stem', default=None, force=True)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        for token in doc:\n",
    "            token._.set('stem', self._stemmer.stem(token.text))  \n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(model='en_core_web_sm', before_tokenizer= [lowercase, remove_non_alpha, single_space], \n",
    "                                            after_tokenizer= [('stemmer', Stemmer()), 'tagger'])\n",
    "\n",
    "processed = list(pipeline([doc.abstract for doc in collection]))\n",
    "viewer = DocViewer(processed[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print textual representation\n",
    "# print(viewer['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stemms\n",
    "# print(viewer['stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print lemmas (in spacy, lemmatization is performed by default)\n",
    "# print(viewer['lemma_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word is a stopword\n",
    "# print(viewer['is_stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print POS tags\n",
    "# print(viewer['tag_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_field(documents, field='abstract'):\n",
    "    return [getattr(doc, field) for doc in documents]\n",
    "\n",
    "def remove_empty_doc(corpus):\n",
    "    return [doc for doc in corpus if doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS = list(CollectionLoader(dirs, spec='40:40:40'))\n",
    "RAW_CORPUS = remove_empty_doc(extract_doc_field(DOCUMENTS))\n",
    "PIPELINE = Pipeline(model='en_core_web_sm', before_tokenizer=[remove_non_alpha, single_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(corpus, pipeline, out_field='text'):\n",
    "    processed_corpus = list(pipeline(corpus))\n",
    "    return [DocViewer(doc)[out_field] for doc in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_CORPUS = process(RAW_CORPUS, PIPELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_doc_ids, relevant_doc_ids, k):\n",
    "    rel_cnt = 0\n",
    "    for doc_id in retrieved_doc_ids[:k]:\n",
    "        if doc_in in relevant_doc_ids:\n",
    "            rel_cnt += 1\n",
    "    return np.array(rel_cnt / k)\n",
    "\n",
    "def r_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    return precision_at_k(retrieved_doc_ids, relevant_doc_ids, len(relevant_doc_ids))\n",
    "\n",
    "def average_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    precisions = []\n",
    "    rel_cnt = 0\n",
    "    for i, doc_id in enumerate(retrieved_doc_ids):\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            rel_cnt += 1\n",
    "            precisions.append(rel_cnt / (i + 1))\n",
    "    return np.array(precisions).mean()\n",
    "\n",
    "def mean_average_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    assert len(retrieved_doc_ids) == len(relevant_doc_ids)\n",
    "    \n",
    "    average_precisions = [average_precision(ret_ids, rel_ids) in \n",
    "                          zip(retrieved_doc_ids, relevant_doc_ids)]\n",
    "    return np.array(average_precisions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(text, pipeline=PIPELINE, out_field='text'):\n",
    "    return DocViewer(list(pipeline([text]))[0])[out_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Role', 'of', 'the', 'environment', 'in', 'transmission']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('Role of the environment in transmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    NAME2ALG = {\n",
    "        'BM25Okapi': rank_bm25.BM25Okapi,\n",
    "        'BM25L': rank_bm25.BM25L,\n",
    "        'BM25Plus': rank_bm25.BM25Plus,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, alg_type, documents, params=None):\n",
    "        if alg_type not in BM25.NAME2ALG.keys():\n",
    "            raise ValueError(f'{alg_type} is not supported')\n",
    "            \n",
    "        params = params or {}\n",
    "        self.alg = BM25.NAME2ALG[alg_type](documents, **params)\n",
    "        self.documents = documents\n",
    "    \n",
    "    def get_scores(self, query):\n",
    "        return self.alg.get_scores(query)\n",
    "    \n",
    "    def top_n(self, query, n=5):\n",
    "        scores = self.get_scores(query)\n",
    "        top_n_scores = np.argsort(scores)[::-1][:n]\n",
    "        return [(i, scores[i]) for i in top_n_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query('covid transmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.51634976, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 3.26065278, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.05718893, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 3.8591491 , 0.        ,\n",
       "       2.25098326, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 3.34468806, 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.80644798, 0.        , 0.        , 4.23000377,\n",
       "       0.        , 2.56119156, 0.        , 1.77900065, 0.        ,\n",
       "       0.        , 3.53640782, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 3.45807275,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25okapi = BM25('BM25Okapi', PROCESSED_CORPUS)\n",
    "bm25okapi.get_scores(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.33813654,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  7.33776457,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.72401203,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , 12.56779579,  0.        ,\n",
       "        2.86780626,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , 11.21932476,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  2.54235745,  0.        ,  0.        ,  5.29203556,\n",
       "        0.        ,  3.10439927,  0.        ,  2.52276609,  0.        ,\n",
       "        0.        ,  7.80823987,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  7.67305313,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25l = BM25('BM25L', PROCESSED_CORPUS)\n",
    "bm25l.get_scores(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        8.48899843,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035, 10.37202935,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        9.07285162,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035, 11.01812527,  6.85205035,\n",
       "        9.2820588 ,  6.85205035,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035, 10.46274812,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  8.80216875,  6.85205035,  6.85205035, 11.50486429,\n",
       "        6.85205035,  9.61693858,  6.85205035,  8.77253848,  6.85205035,\n",
       "        6.85205035, 10.66971574,  6.85205035,  6.85205035,  6.85205035,\n",
       "        6.85205035,  6.85205035,  6.85205035,  6.85205035, 10.58515052,\n",
       "        6.85205035])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25plus = BM25('BM25Plus', PROCESSED_CORPUS)\n",
    "bm25plus.get_scores(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 4.230003768125611\n",
      "0a27cb2cd52229472fcfc3e49d3a3cb7179867e4: Strict interventions were successful to control the novel coronavirus (COVID-19) outbreak in China. As transmission intensifies in other countries, the interplay between age, contact patterns, social  ... also estimate age differences in susceptibility to infection and clinical disease based on contact tracing information gathered by the Hunan Provincial Center for Disease Control and Prevention (CDC), ...\n"
     ]
    }
   ],
   "source": [
    "doc_id, score = bm25okapi.top_n(q, 1)[0]\n",
    "print('score:', score)\n",
    "print(DOCUMENTS[doc_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
