{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import glob\n",
    "import itertools as it\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_doc_ids, relevant_doc_ids, k):\n",
    "    rel_cnt = 0\n",
    "    for doc_id in retrieved_doc_ids[:k]:\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            rel_cnt += 1\n",
    "    return np.array(rel_cnt / k)\n",
    "\n",
    "def r_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    return precision_at_k(retrieved_doc_ids, relevant_doc_ids, len(relevant_doc_ids))\n",
    "\n",
    "def average_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    precisions = []\n",
    "    rel_cnt = 0\n",
    "    for i, doc_id in enumerate(retrieved_doc_ids):\n",
    "        if doc_id in relevant_doc_ids:\n",
    "            rel_cnt += 1\n",
    "            precisions.append(rel_cnt / (i + 1))\n",
    "    return np.array(precisions).mean()\n",
    "\n",
    "def mean_average_precision(retrieved_doc_ids, relevant_doc_ids):\n",
    "    assert len(retrieved_doc_ids) == len(relevant_doc_ids)\n",
    "    \n",
    "    average_precisions = [average_precision(ret_ids, rel_ids) for ret_ids, rel_ids in \n",
    "                          zip(retrieved_doc_ids, relevant_doc_ids)]\n",
    "    return np.array(average_precisions).mean()\n",
    "\n",
    "precision_at_5 = functools.partial(precision_at_k, k=5)\n",
    "precision_at_10 = functools.partial(precision_at_k, k=10)\n",
    "precision_at_15 = functools.partial(precision_at_k, k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def load_dataset(dirname):\n",
    "    dataset_dict = {}\n",
    "    files = glob.glob(f'{dirname}/**/*.json', recursive=True)\n",
    "    for file in files:\n",
    "        doc = Document.from_json(file)\n",
    "        dataset_dict[doc.paper_id] = doc\n",
    "    return dataset_dict\n",
    "\n",
    "def append_score(d, results):\n",
    "    for id, score in results:\n",
    "        d[id].append(score)\n",
    "\n",
    "def order_ids_by_score(ids, scores):\n",
    "    score_order = scores.argsort()[::-1]\n",
    "    return [ids[sc] for sc in score_order]\n",
    "\n",
    "def eval_model(retrieved_ids, relevant_ids):\n",
    "    return np.array([\n",
    "        precision_at_5(retrieved_ids, relevant_ids),\n",
    "        precision_at_10(retrieved_ids, relevant_ids),\n",
    "        precision_at_15(retrieved_ids, relevant_ids),\n",
    "        r_precision(retrieved_ids, relevant_ids),\n",
    "        average_precision(retrieved_ids, relevant_ids),\n",
    "    ])\n",
    "\n",
    "def print_relevant(ranked_ids, relevant_ids):\n",
    "    for i, id in enumerate(ranked_ids):\n",
    "        pref = ''\n",
    "        if id in relevant_ids:\n",
    "            pref = '--> '\n",
    "        print(f'{i+1}. {pref}{id}')\n",
    "\n",
    "def print_dataframe(df):\n",
    "    df_print = df.copy()\n",
    "    df_print['Paper ID'] = df_print['Paper ID'].str.pad(60)\n",
    "    return df_print.head()\n",
    "    \n",
    "    \n",
    "def plot_metrics(models, metrics_matrix):\n",
    "    assert len(models) == len(metrics_matrix)\n",
    "    \n",
    "    header = f\"<tr>{''.join(f'<th>{metric}</th>' for metric in ['Model', 'P@5', 'P@10', 'P@15', 'R-prec', 'AveP'])}</tr>\"\n",
    "    rows = []\n",
    "    for model, metrics in zip(models, metrics_matrix):\n",
    "        metrics = [f'{metric:.2f}'for metric in metrics]\n",
    "        row = f\"<tr>{''.join(f'<td>{m}</td>' for m in [model, *metrics])}</tr>\"\n",
    "        rows.append(row)\n",
    "        \n",
    "    html = f\"<table style='width:100%'>{header}{''.join(rows)}</table>\"\n",
    "    display(HTML(html))\n",
    "    \n",
    "def plot_map(models, map_scores):\n",
    "    assert len(models) == len(map_scores)\n",
    "    \n",
    "    header = f\"<tr>{''.join(f'<th>{metric}</th>' for metric in ['Model', 'MAP'])}</tr>\"\n",
    "    rows = []\n",
    "    for model, map_score in zip(models, map_scores):\n",
    "        row = f\"<tr><td>{model}</td><td>{map_score:.2f}</td></tr>\"\n",
    "        rows.append(row)\n",
    "    \n",
    "    html = f\"<table style='width:40%'>{header}{''.join(rows)}</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \n",
    "    def __init__(self, paper_id, title, abstract, body_text):\n",
    "        self.paper_id = paper_id\n",
    "        self.title = title\n",
    "        self.abstract = abstract\n",
    "        self.body_text = body_text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, path):\n",
    "        with open(path, 'r') as fd:\n",
    "            data = json.load(fd)\n",
    "        \n",
    "        paper_id = data['paper_id']\n",
    "        title = data['metadata']['title']\n",
    "        abstract = '\\n'.join([record['text'] for record in data['abstract']])\n",
    "        body_text = '\\n'.join([record['text'] for record in data['body_text']])\n",
    "        return cls(paper_id, title, abstract, body_text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.title[:30]} ... {self.abstract[:200]} ... {self.body_text[:200]} ...'\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        paper_html = f'<b>Paper ID:</b> {self.paper_id}'\n",
    "        title_html = f'<h3>Title</h3> {self.title}'\n",
    "        abstract_html = ['<p>' + record + '</p>' for record in self.abstract.split('\\n')]\n",
    "        abstract_html = '<h3>' + 'Abstract' + '</h3>' + ''.join(abstract_html)\n",
    "        body_text_html = ['<p>' + record + '</p>' for record in self.body_text.split('\\n')]\n",
    "        body_text_html = '<h3>' + 'Body text' + '</h3>' + ''.join(body_text_html)  \n",
    "        return paper_html + title_html + abstract_html + body_text_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import results2tuple, annotations2tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_PATH = 'annotations-v3.txt'\n",
    "annotations = annotations2tuple(ANNOTATIONS_PATH)\n",
    "assert len(annotations) == 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transmission evaluation - What is known about Covid-19 transmission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_annotations = {annot[0]: [annot[1][0]] for annot in annotations}\n",
    "\n",
    "append_score(transmission_annotations, results2tuple('results/lm-transmission-scores.txt'))\n",
    "append_score(transmission_annotations, results2tuple('results/bm25-transmission-scores.txt'))\n",
    "append_score(transmission_annotations, results2tuple('results/transmission-v2.txt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper ID</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>LM</th>\n",
       "      <th>BM25</th>\n",
       "      <th>Our Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15e842b8bf1a55c1df1c369a5f...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459039</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13f6bc615450f0470c2a265e0a...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.360454</td>\n",
       "      <td>1.959558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce660d55c0f3908969f3542d3c...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.663033</td>\n",
       "      <td>2.413859</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aae7699e079d2964df8078931c...</td>\n",
       "      <td>I</td>\n",
       "      <td>1.151933</td>\n",
       "      <td>3.219056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f72975ffd96b7ab1528afbea8f...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.537950</td>\n",
       "      <td>2.258155</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Paper ID Relevance        LM  \\\n",
       "0                      15e842b8bf1a55c1df1c369a5f...         I  0.000000   \n",
       "1                      13f6bc615450f0470c2a265e0a...         I  0.360454   \n",
       "2                      ce660d55c0f3908969f3542d3c...         I  0.663033   \n",
       "3                      aae7699e079d2964df8078931c...         I  1.151933   \n",
       "4                      f72975ffd96b7ab1528afbea8f...         I  0.537950   \n",
       "\n",
       "       BM25  Our Model  \n",
       "0  0.459039        0.0  \n",
       "1  1.959558        0.0  \n",
       "2  2.413859        0.0  \n",
       "3  3.219056        0.0  \n",
       "4  2.258155        0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmission_flatten = [(k, *v) for k, v in transmission_annotations.items()]\n",
    "transmission_df = pd.DataFrame(transmission_flatten, columns=['Paper ID', 'Relevance', 'LM', 'BM25', 'Our Model'])\n",
    "print_dataframe(transmission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='width:100%'><tr><th>Model</th><th>P@5</th><th>P@10</th><th>P@15</th><th>R-prec</th><th>AveP</th></tr><tr><td>LM</td><td>0.80</td><td>0.60</td><td>0.47</td><td>0.67</td><td>0.69</td></tr><tr><td>BM25</td><td>0.80</td><td>0.70</td><td>0.47</td><td>0.78</td><td>0.70</td></tr><tr><td>Our Model</td><td>0.80</td><td>0.70</td><td>0.60</td><td>0.67</td><td>0.84</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = transmission_df['Paper ID'].tolist()\n",
    "relevant_ids_tran = transmission_df[transmission_df['Relevance'] == 'R']['Paper ID'].tolist()\n",
    "\n",
    "ids_lm_tran = order_ids_by_score(ids, transmission_df['LM'].to_numpy())\n",
    "ids_bm25_tran = order_ids_by_score(ids, transmission_df['BM25'].to_numpy())\n",
    "ids_model_tran = order_ids_by_score(ids, transmission_df['Our Model'].to_numpy())\n",
    "\n",
    "res_lm_tran = eval_model(ids_lm_tran, relevant_ids_tran)\n",
    "res_bm25_tran = eval_model(ids_bm25_tran, relevant_ids_tran)\n",
    "res_model_tran = eval_model(ids_model_tran, relevant_ids_tran)\n",
    "\n",
    "plot_metrics(['LM', 'BM25', 'Our Model'], [res_lm_tran, res_bm25_tran, res_model_tran])\n",
    "# relevant_ids_tran, ids_lm_tran[:15], ids_bm25_tran[:15], ids_model_tran[:15], ids_bm25_tran[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_model_tran, relevant_ids_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_bm25_tran, relevant_ids_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_lm_tran, relevant_ids_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incubation evaluation - What is known about Covid-19 incubation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "incubation_annotations = {annot[0]: [annot[1][1]] for annot in annotations}\n",
    "\n",
    "append_score(incubation_annotations, results2tuple('results/lm-incubation-scores.txt'))\n",
    "append_score(incubation_annotations, results2tuple('results/bm25-incubation-scores.txt'))\n",
    "append_score(incubation_annotations, results2tuple('results/incubation-v2.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper ID</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>LM</th>\n",
       "      <th>BM25</th>\n",
       "      <th>Our Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15e842b8bf1a55c1df1c369a5f...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459039</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13f6bc615450f0470c2a265e0a...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.360454</td>\n",
       "      <td>1.959558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce660d55c0f3908969f3542d3c...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.663033</td>\n",
       "      <td>2.413859</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aae7699e079d2964df8078931c...</td>\n",
       "      <td>I</td>\n",
       "      <td>1.151933</td>\n",
       "      <td>3.219056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f72975ffd96b7ab1528afbea8f...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.537950</td>\n",
       "      <td>2.258155</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Paper ID Relevance        LM  \\\n",
       "0                      15e842b8bf1a55c1df1c369a5f...         I  0.000000   \n",
       "1                      13f6bc615450f0470c2a265e0a...         I  0.360454   \n",
       "2                      ce660d55c0f3908969f3542d3c...         I  0.663033   \n",
       "3                      aae7699e079d2964df8078931c...         I  1.151933   \n",
       "4                      f72975ffd96b7ab1528afbea8f...         I  0.537950   \n",
       "\n",
       "       BM25  Our Model  \n",
       "0  0.459039        0.0  \n",
       "1  1.959558        0.0  \n",
       "2  2.413859        0.0  \n",
       "3  3.219056        0.0  \n",
       "4  2.258155        0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incubation_flatten = [(k, *v) for k, v in incubation_annotations.items()]\n",
    "incubation_df = pd.DataFrame(incubation_flatten, columns=['Paper ID', 'Relevance', 'LM', 'BM25', 'Our Model'])\n",
    "print_dataframe(incubation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='width:100%'><tr><th>Model</th><th>P@5</th><th>P@10</th><th>P@15</th><th>R-prec</th><th>AveP</th></tr><tr><td>LM</td><td>0.60</td><td>0.50</td><td>0.33</td><td>0.60</td><td>0.64</td></tr><tr><td>BM25</td><td>0.40</td><td>0.50</td><td>0.33</td><td>0.40</td><td>0.55</td></tr><tr><td>Our Model</td><td>0.80</td><td>0.50</td><td>0.33</td><td>0.80</td><td>0.97</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = incubation_df['Paper ID'].tolist()\n",
    "relevant_ids_incub = incubation_df[incubation_df['Relevance'] == 'R']['Paper ID'].tolist()\n",
    "\n",
    "ids_lm_incub = order_ids_by_score(ids, incubation_df['LM'].to_numpy())\n",
    "ids_bm25_incub = order_ids_by_score(ids, incubation_df['BM25'].to_numpy())\n",
    "ids_model_incub = order_ids_by_score(ids, incubation_df['Our Model'].to_numpy())\n",
    "\n",
    "res_lm_incub = eval_model(ids_lm_incub, relevant_ids_incub)\n",
    "res_bm25_incub = eval_model(ids_bm25_incub, relevant_ids_incub)\n",
    "res_model_incub = eval_model(ids_model_incub, relevant_ids_incub)\n",
    "\n",
    "plot_metrics(['LM', 'BM25', 'Our Model'], [res_lm_incub, res_bm25_incub, res_model_incub])\n",
    "# relevant_ids_incub, ids_lm_incub[:5], ids_bm25_incub[:5], ids_model_incub[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_model_incub, relevant_ids_incub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_bm25_incub, relevant_ids_incub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_relevant(ids_lm_incub, relevant_ids_incub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='width:40%'><tr><th>Model</th><th>MAP</th></tr><tr><td>LM</td><td>0.66</td></tr><tr><td>BM25</td><td>0.62</td></tr><tr><td>Our Model</td><td>0.91</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_lm = mean_average_precision([ids_lm_tran, ids_lm_incub], [relevant_ids_tran, relevant_ids_incub])\n",
    "map_bm25 = mean_average_precision([ids_bm25_tran, ids_bm25_incub], [relevant_ids_tran, relevant_ids_incub])\n",
    "map_model = mean_average_precision([ids_model_tran, ids_model_incub], [relevant_ids_tran, relevant_ids_incub])\n",
    "\n",
    "plot_map(['LM', 'BM25', 'Our Model'], [map_lm, map_bm25, map_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c6b625a4ff46b98cde9847e2065e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Enter Paper ID')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1d1cbfa2604ce7be702d39dc80246b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "text = widgets.Text(placeholder='Enter Paper ID')\n",
    "out = widgets.Output()\n",
    "\n",
    "ids_dict = {\n",
    "    'lm tran': ids_lm_tran,\n",
    "    'bm25 tran': ids_bm25_tran,\n",
    "    'model tran': ids_model_tran,\n",
    "    'lm incub': ids_lm_incub,\n",
    "    'bm25 incub': ids_bm25_incub,\n",
    "    'model incub': ids_model_incub,\n",
    "}\n",
    "dataset_dict = load_dataset('dataset')\n",
    "\n",
    "def handle_submit(sender):\n",
    "    out.clear_output(wait=True)\n",
    "    with out:\n",
    "        paper_id = text.value\n",
    "        if any(text.value.startswith(k) for k in ids_dict):\n",
    "            key, ind = text.value.split(',')\n",
    "            paper_id = ids_dict[key.strip()][int(ind.strip())]\n",
    "        display(dataset_dict[paper_id])\n",
    "        \n",
    "display(text)\n",
    "text.on_submit(handle_submit)\n",
    "\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
